\section{Complexity of input partitions}
\label{ch:partitions}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-intro.png}
    \label{fig:ch04-intro}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-intro-different.png}
    \label{fig:ch04-intro-different}
  \end{subfigure}
  \caption{The landscape of the first layer of two different \rdtlifsnn}
\end{figure}

% MAYBE: remark about comparative strength of dt.lifsnn and rsnn, compare images

% DONE: what kind of shapes a d.t. â€¦
% DONE: what is meant with graph
% DONE: etwas text zwischen aussagen
In the following we will analyze what shape the graph of the realized function \(R(Î¦)\) of a \rdtlifsnn \(Î¦\) has. Of particular interest to us is how many different values it can obtain.
Since the number of different values of the following layers only depends on the spike trains of the first hidden layer, we can get an upper bound on that number by analyzing how many different spike trains the first hidden layer can produce. We will therefore only study the output landscape of the first hidden layer.
We will further assume \(W^{[1]}=\idM{n_1}\). Let \(Î¦\) be a \rdtlifsnn with \(W^{[1]}\) arbitrary and \(Î¦'\) be a copy of \(Î¦\), but with trivial \(W^{[1]}=\idM{n_1}\). We then have \(R(Î¦)(x)=R(Î¦')(W^{[1]}x)\) since we are using direct encoding. So \(W^{[1]}\) just corresponds to a pre-transformation on the input vector and can therefore only decrease the number of different output values. Hence, the key to understanding the output landscape of a general \rdtlifsnn is to understand the landscape of one with \(W^{[1]}=\idM{n_1}\).

We will moreover simplify the notation in this section by writing \(W,b,V,Î±,Î²,Ï‘,i,u,s,n\) for \(W^{[1]},b^{[1]},V^{[1]},Î±^{[1]},Î²^{[1]},Ï‘^{[1]},i^{[1]},u^{[1]},s^{[1]},n_1\) respectively, since we will only be working on the first layer anyways. We further write \(xâ‰”s^{[0]}\). Since we are using direct encoding, we have \(âˆ€_{tâˆˆ[T]}s^{[0]}(t)=x\) and hence obtain the following simplified defining equations:
\begin{align}
  i(t) & = Î±i(t-1)+Wx+Vs(t-1), \\
  p(t) & = Î²u(t-1)+i(t)+b, \\
  s(t) & = H(p(t)-Ï‘\oneV{n}), \\
  u(t) & = p(t)-Ï‘s(t).
\end{align}
Since the definitions recursively depend on \(x\), we will sometimes also write \(i(t;x)\), \(p(t;x)\), \(s(t;x)\) and \(u(t;x)\) to make the dependency explicit. It will also be helpful to write e.g.\ \(i(Â·;x)\) for \((i(t;x))_{tâˆˆ[T]}\).

By using this simplified notation, we obtain the following equations from~\cref{def:non-recursive-defs} for the input vector \(xâˆˆâ„^{n_0}\) and the first hidden layer spike train \(Ïƒ\):
\begin{align*}
    i(t;x;Ïƒ) &= Î±^ti(0)+\sum_{k=1}^tÎ±^{t-k}\left(Wx+VÏƒ(k-1)\right), \\\label{eq:1}
    p(t;x;Ïƒ) &= Î²^tu(0)+\sum_{k=1}^tÎ²^{t-k}\left(i(k;x;Ïƒ)+b\right)-Ï‘\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ(k), \\
    s(t;x;Ïƒ) &= H(p(t;x;Ïƒ)-Ï‘\oneV{n_1}), \\
    u(t;x;Ïƒ) &= Î²^tu(0)+\sum_{k=1}^tÎ²^{t-k}\left(i(k;x;Ïƒ)+b-Ï‘Ïƒ(k)\right).
\end{align*}

\begin{comment}

\begin{remark}\label{rem:regions-non-recursive-defs}
  It is furthermore quite obvious that \(i\) grows linear in \(x\), given fixed \(Ïƒ\). The same holds therefore for \(u\) and \(p\).

  Further, the interdependence of the different components in \(i/u/p/s\) is gone using a constant \(Ïƒ\). We can therefore split e.g. \(p\) into functions \(p_{i;Ïƒ}\) such that \(p(t,x)=p_{1;Ïƒ}(t;x_1)Ã—â€¦Ã—p_{n_1;Ïƒ}(t;x_{n_1})\).

  The functions \(i_{i;Ïƒ}\),\(u_{i;Ïƒ}\),\(p_{i;Ïƒ}\) are not only linear in \(x\), but also grow monotonically in \(x\) since \(Î±,Î²â‰¥0\). If \(tâ‰¥1\), then \(i_{i;Ïƒ}\),\(u_{i;Ïƒ}\),\(p_{i;Ïƒ}\) grow even strictly monotonically in \(x\).

  So we in particular have that \(p\) to is growing monotonically (regarding by-component ordering) on fixed \(Ïƒ\).
  Since \(H\) is a monotonically growing function, \(s\) is also growing monotonically with fixed \(Ïƒ\).
\end{remark}

\end{comment}

Let us now introduce some preliminary definitions.

\begin{definition}
  The set of constant regions of a \rdtlifsnn \(Î¦\) is defined as the partition
  \[ C_Î¦â‰”\{R(Î¦)^{-1}(\{y\})\mid yâˆˆ\operatorname{im}(R(Î¦))\} \]
  of \(â„^{n_0}\). A constant region with spike train \(s'âˆˆ\{0,1\}^{n_1Ã—T}\), of the first layer of a \rdtlifsnn \(Î¦\) is defined as
  \[ C_{s'}â‰”\{xâˆˆâ„^{n_0}\midâˆ€_{tâˆˆ[T]}s(t;x)=s'(t)\} \]
  We further notate the set of such non-empty regions by \( C_{Î¦,1}â‰”\{C_{s'}\mid s'âˆˆ\{0,1\}^{n_1Ã—T},C_{s'}â‰ âˆ…\} \).
\end{definition}

Note that we have argued in the beginning of this chapter that in fact \(\abs{C_{Î¦}}â‰¤\abs{C_{Î¦,1}}\).

% CANCELLED: remark about the connection between C_Î¦ and C_{Î¦,1}
While quite technical the following lemma is the key for writing rigorous proofs about the landscape of \rdtlifsnns.

\begin{lemma}\label{lem:invert-p}
  Consider the function \(g:\dot{\bigcup}_{tâˆˆ[T]}\{Ïƒ\mid Ïƒâˆˆ\{0,1\}^{n_1Ã—(t-1)}\}â†’â„\) defined by
  \[ g(t;Ïƒ)â‰”-\frac{\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki(0)+b+\sum_{l=1}^kÎ±^{k-l}VÏƒ(l-1)\right) +Î²^tu(0)-Ï‘\left(1+\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ(k)\right)}{\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}}. \]
  Then \(g\) satisfies \(âˆ€_{iâˆˆ[n_1]}s_i(t;x;Ïƒ)=1â‡”âŸ¨w_i,xâŸ©â‰¥g_i(t;Ïƒ)\).
  Here \(w_i\) denotes the \(i\)-th row vector of \(W\). %MAYBE: remove?
\end{lemma}

\begin{proof}
We compute for \(iâˆˆ[n_1]\) using~\cref{lem:non-recursive-defs},
\begin{align*}
 & H(p_i(t;x;Ïƒ)-Ï‘) \\
 &= H\left(\sum_{k=1}^tÎ²^{t-k}\left(i_i(k;x;Ïƒ)+b_i\right)+\underset{(*)}{\underbrace{Î²^tu_i(0)-Ï‘\left(1+\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ_i(k)\right)}}\right)  \\
 &= H\left(\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki_i(0)+\sum_{l=1}^kÎ±^{k-l}\left(âŸ¨w_i,xâŸ©+(VÏƒ(l-1))_i\right)+b_i\right) +(*)\right) \\
 &= H\left(\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}âŸ¨w_i,xâŸ©+\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki_i(0)+b_i+\sum_{l=1}^kÎ±^{k-l}(VÏƒ(l-1))_i\right) +(*)\right) \\
 &= H\left(âŸ¨w_i,xâŸ©+\frac{\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki_i(0)+b_i+\sum_{l=1}^kÎ±^{k-l}(VÏƒ(l-1))_i\right) +(*)}{\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}}\right) \\
 &= H\left(âŸ¨w_i,xâŸ©-g_i(t;Ïƒ)\right).
\end{align*}
\end{proof}

\begin{remark}\label{rem:g-prefix}
  Like with \(i(t;x;Ïƒ)\), \(p(t;x;Ïƒ)\), etc. we will allow supplying \(g\) with an extension of the required spike train. In that case the value of \(g\) should be understood as the value at the prefix of that spike train.
\end{remark}

\begin{proposition}\label{prop:const-regions-cuboids}
  The constant regions of the first layer of a \rdtlifsnn \(Î¦\) (with \(W=I_{n_1}\)) are half-open cuboids. In particular, let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be a spike train and \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†\) be the corresponding constant region. Then
  \[ x^{s'}_i =\sup_{\substack{tâˆˆ[T] \\ s'_i(t)=1}}g_i(t;s'),\qquad y^{s'}_i =\inf_{\substack{tâˆˆ[T] \\ s'_i(t)=0}}g_i(t;s'). \]
\end{proposition}

\begin{remark}\label{rem:const-regions-cuboids}
  Note that by \cref{prop:const-regions-cuboids}, we have \(x^{s'}_i=-âˆ\) exactly if \(âˆ€_{tâˆˆ[T]}s'_i(t)=0\); and \(y^{s'}_i=âˆ\) exactly if \(âˆ€_{tâˆˆ[T]}s'_i(t)=1\). So in particular we cannot have \(s'\) with a component \(iâˆˆ[n_1]\), such that both \(x^{s'}_i=-âˆ\) and \(y^{s'}_i=âˆ\).
\end{remark}

%MAYBE: Ïƒ vs s'

\begin{lemma}\label{lem:const-regions-cuts}
  For every \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) we have
  \[ C_{s'}=\left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=0}}Ï€_i^{-1}([-âˆ,g_i(t;s')))\right) \bigcap \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=1}}Ï€_i^{-1}([g_i(t;s'),âˆ))\right). \]
\end{lemma}

\begin{proof}[\proofofref{lem:const-regions-cuts}]
  Let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be given. We then get
  \begin{align*}
    C_{s'}&=\bigcap_{iâˆˆ[n_1],tâˆˆ[T]}\{x\mid s'_i(t) =s_i(t;x)\} \\
     &=\bigcap_{iâˆˆ[n_1],tâˆˆ[T]}\{x\mid s'_i(t) =s_i(t;x;s')\} \\
     &= \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=0}}\{x\mid s_i(t;x;s')=0\}\right) \bigcap \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=1}}\{x\mid s_i(t;x;s')=1\}\right) \\
     &= \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=0}}Ï€_i^{-1}([-âˆ,g_i(t;s')))\right) \bigcap \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=1}}Ï€_i^{-1}([g_i(t;s'),âˆ))\right),
  \end{align*}
  where we use~\cref{lem:non-recursive-defs} for the second equality and~\cref{lem:invert-p} for the last one.
\end{proof}

\begin{lemma}\label{lem:intersection-and-product}
  \(\bigcap_{jâˆˆJ}\prod_{iâˆˆ[n]}M_{i,j}=\prod_{iâˆˆ[n]}\bigcap_{jâˆˆJ}M_{i,j}\) for an index set \(J\) and sets \((M_{i,j})_{iâˆˆ[n],jâˆˆJ}\).
\end{lemma}

\begin{proof}[\proofofref{lem:intersection-and-product}]
  For every \(x=(x_i)_{iâˆˆ[n]}âˆˆMâ‰”\prod_{iâˆˆ[n]}\bigcup_{jâˆˆJ}M_{i,j}\) holds
  \begin{equation*}
   xâˆˆ\bigcap_{jâˆˆJ}\prod_{iâˆˆ[n]}M_{i,j}â‡”\left(âˆ€_{jâˆˆJ}xâˆˆ\prod_{iâˆˆ[n]}M_{i,j}\right) â‡”âˆ€_{jâˆˆJ}âˆ€_{iâˆˆ[n]}x_iâˆˆM_{i,j}.
  \end{equation*}
  On the other hand, we have
  \begin{equation*}
    xâˆˆ\prod_{iâˆˆ[n]}\bigcap_{jâˆˆI}M_{i,j}â‡”\left(âˆ€_{iâˆˆ[n]}x_i\bigcap_{jâˆˆI}M_{i,j}\right)â‡”âˆ€_{iâˆˆ[n]}âˆ€_{jâˆˆJ}x_iâˆˆM_{i,j}.
  \end{equation*}
  We thus conclude with
  \[ \bigcap_{jâˆˆJ}\prod_{iâˆˆ[n]}M_{i,j}=Mâˆ©\bigcap_{jâˆˆJ}\prod_{iâˆˆ[n]}M_{i,j}=Mâˆ©\prod_{iâˆˆ[n]}\bigcap_{jâˆˆI}M_{i,j}=\prod_{iâˆˆ[n]}\bigcap_{jâˆˆI}M_{i,j}. \]
\end{proof}

\begin{lemma}\label{lem:intersection-cuboid}
  The intersection \(C_1âˆ©C_2\) of half-open cuboids \(C_1,C_2âŠ‚â„^n\) is a half-open cuboid. In particular, if \(C_iâ‰”\prod_{jâˆˆ[n]}[c_{i,j},d_{i,j})\), then
  \[ C_1âˆ©C_2=\prod_{jâˆˆ[n]}([\sup(c_{1,j},c_{2,j}),\inf(d_{1,j},d_{2,j}))). \]
\end{lemma}

\begin{proof}[\proofofref{lem:intersection-cuboid}]
  %DONE: explicitely show case n=1?
  Let us first regard \(n=1\). For \(c_1,c_2,d_1,d_2âˆˆâ„âˆª\{Â±âˆ\}\) we get
  \begin{align*}
       &x âˆˆ [c_1,d_1)\cap [c_2,d_2) \\
   â‡”{} &c_1,c_2â‰¤x<d_1,d_2 \\
   â‡”{} &xâˆˆ[\sup(c_{1,j},c_{2,j}),\inf(d_{1,j},d_{2,j}))
  \end{align*}
  for \(xâˆˆâ„\) and therefore
  \[ C_1âˆ©C_2=\prod_{jâˆˆ[n]}\left([c_{1,j},d_{1,j})âˆ©[c_{2,j},d_{2,j})\right)=\prod_{jâˆˆ[n]}[\sup(c_{1,j},c_{2,j}),\inf(d_{1,j},d_{2,j})) \]
  by~\cref{lem:intersection-and-product}.
\end{proof}
%DONE: remark, we do allow empty cuboids?

%DONE: define cuboid/half-open?

\begin{comment}
\begin{lemma}\label{lem:pre-image-half-open-int}
  Let \([c,d)\) be a half-open interval and \(Ï†:â„â†’â„\) be an affine linear function. Then \(Ï†^{-1}([c,d))\) is a half-open interval.
\end{lemma}

\begin{proof}
  If \(Ï†\) is constant, then \(Ï†^{-1}([c,d))\) is either \(â„=[-âˆ,âˆ)\) or \(âˆ…=[0,0)\).
  If \(Ï†\) is not constant, then \(Ï†=ax+b\) with \(a>0\) and \(Ï†^{-1}([c,d))=[\frac{c-b}{a},\frac{d-b}{a})\), if \(c,dâˆˆâ„\); if \(c=-âˆ\) or \(d=âˆ\) the lower limit or upper limit of \(Ï†^{-1}([c,d))\) is \(-âˆ\) or \(âˆ\) respectively.
\end{proof}
\end{comment}

We are now ready to proof~\cref{prop:const-regions-cuboids}.

\begin{proof}[\proofofref{prop:const-regions-cuboids}]
  Let \(C_{s'}âˆˆC_{Î¦,1}\) be a constant region. We then get
  \begin{align*}
   C_{s'}&=\left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=0}}Ï€_i^{-1}([-âˆ,g_i(t;s')))\right) \bigcap \left(\bigcap_{\substack{iâˆˆ[n_1],tâˆˆ[T] \\ s'_i(t)=1}}Ï€_i^{-1}([g_i(t;s'),âˆ))\right) \\
         &=\prod_{iâˆˆ[n]}[\sup_{tâˆˆ[T],\ s'_i(t)=1}g_i(t;s'), \inf_{tâˆˆ[T],\ s'_i(t)=0}g_i(t;s'))
  \end{align*}
  by~\cref{lem:const-regions-cuts} and~\cref{lem:intersection-cuboid}, since
  \[ Ï€_i^{-1}([c,d))=[-âˆ,âˆ)Ã—â€¦Ã—[c,d)Ã—â€¦Ã—[-âˆ,âˆ). \]
\end{proof}

We will order spike trains using lexicographical ordering.

\begin{definition}
  Let \(s',s''âˆˆ\{0,1\}^{n_1Ã—T}\), we then define \(s'â‰¤_ls''\) to be true exactly in the case that either \(s'=s''\) or that there exists a \(tâˆˆ[T]\) such that \(s'(t)<s''(t)\) and \(âˆ€_{t'<t}s'(t')=s''(t')\).
  We further define \(s'<_ls''\) to be true if and only if \(s'â‰¤_ls''\) but not \(s'=s''\).
\end{definition}

\begin{lemma}\label{lem:spike-trains-ineq}%DONE: introduce s(Â·;y)
  Let \(x,yâˆˆâ„^{n_0}\). We then have \(xâ‰¤yâ‡’s(Â·;x)â‰¤_ls(Â·;y)\).

  Is on the other hand \(s(Â·;x)<_ls(Â·;y)\) and \(iâˆˆ[n_1]\) such that \(s_i(t;x)â‰ s_i(t;y)\) holds at the minimal time \(t\) at which \(s(Â·;x)\) and \(s(Â·;y)\) are different, then \(x_i<y_i\).
\end{lemma}

% DONE: introduce a<b means aâ‰¤b and âˆƒ_i(a_i<b_i)
% CANCELLED: generalize
% CANCELLED: proof using Prop 4.1?
\begin{proof}
  First notice that for a fixed \(Ïƒ\) the function \(i_i(t;x;Ïƒ)\) is growing monotonically in \(x_i\) for all \(iâˆˆ[n]\), since \(Î±â‰¥0\). We similarly get that \(p_i(t;x;Ïƒ)\) and \(s_i(t;x;Ïƒ)\) are growing monotonically in \(x_i\), since \(H=Ï‡_{[0,âˆ)}\) is monotonically growing.

  Let now \(xâ‰¤y\) and suppose \(s(Â·;x)â‰ s(Â·;y)\). We then have a minimal \(tâˆˆ[T]\) such \(s(t;x)â‰ s(t;y)\). Now due to \(âˆ€_{t'<t}s(t';x)=s(t';y)\) and \(xâ‰¤y\) we have \(âˆ€_{iâˆˆ[n_1]}s_i(t;x)â‰¤s_i(t;y)\) using the previous remark with \(Ïƒ(t)â‰”s(t;x)\). We hence have \(s(t;x)<_ls(t;y)\).

  Let on the other hand \(s(Â·;x)<_ls(Â·;y)\). Then there is a smallest time \(t\) such that \(âˆƒ_{iâˆˆ[n_1]}s_i(t;x)â‰ s_i(t;y)\). By definition of the ordering on spike trains we get \(s_i(t;x)<_ls_i(t;y)\). Now \(s_i(t;x)\) is growing monotonically in \(x_i\) with \(Ïƒ(t)â‰”s(t;x)\) by choice of \(t\). We therefore have \(x_i<y_i\).
\end{proof}

We will further prove some theorems about specific classes of spike trains.

\begin{definition}
  We call a spike train \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) constant in component \(iâˆˆ[n_1]\), if \(âˆ€_{t,t'âˆˆ[T]}s'_i(t)=s'_i(t')\).
  A spike train \(s'\) is therefore constant in every component, if
  \[ âˆ€_{iâˆˆ[n_1]}âˆ€_{t,t'âˆˆ[T]}s'_i(t)=s'_i(t'). \]
  We further call \(s'\) non-constant in every component, if
  \[âˆ€_{iâˆˆ[n_1]}âˆƒ_{t,t'âˆˆ[T]}s'_i(t)â‰ s'_i(t').\]
\end{definition}

We also need some geometric definitions.

\begin{definition}
  A point \(pâˆˆ(â„âˆª\{Â±âˆ\})^n\) as a vertex of a non-empty region \(âŸ¦x,yâ¦†\), \(âŸ¦x,yâŸ§\) or \(â¦…x,yâ¦†\), if \(âˆ€_{iâˆˆ[n]}p_iâˆˆ\{x_i,y_i\}\). We call the vertex \(p\) finite, if \(pâˆˆâ„^{n}\).
\end{definition}

We will continue by characterizing regions with constant spike trains in none/some/all components.

\begin{lemma}\label{lem:inf-corner-regions}
  Let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be a spike train that is constant in every component. Then \(C_{s'}\) is non-empty.
\end{lemma}

\begin{proof}
  Suppose \(s'\) is constant in every component. Let further \(iâˆˆ[n_1]\). We now either have \(âˆ€_{tâˆˆ[T]}s'_i(t)=0\) or \(âˆ€_{tâˆˆ[T]}s'_i(t)=1\). In the first case, we get \(x^{s'}_i=-âˆ\) and \(y^{s'}_iâˆˆâ„\); in the second case \(x^{s'}_iâˆˆâ„\) and \(y^{s'}_i=âˆ\) by construction.
  In both cases, \(C_{s'}\) is clearly non-empty.
\end{proof}

\begin{lemma}\label{lem:constant-components-finite-vertices}
  Let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be a spike train such that \(C_{s'}â‰ âˆ…\) and \(s'\) is constant in \(k\) components. Then \(C_{s'}\) has \(2^{n_1-k}\) finite vertices.
\end{lemma}

\begin{remark}
  In particular, every region \(C_{s'}âˆˆC_{Î¦,1}\) has at least one finite vertex.
\end{remark}

\begin{proof}
  First note that if \(s'\) is constant in component \(iâˆˆ[n_1]\), then either \(x^{s'}_i=-âˆ\) and \(y^{s'}_iâˆˆâ„\); or \(x^{s'}_iâˆˆâ„\) and \(y^{s'}_i=âˆ\). In either case \(\abs{â„âˆ©\{x^{s'}_i,y^{s'}_i\}}=1\). Is on the other hand \(s'\) non-constant in \(i\), then \(x^{s'}_i,y^{s'}_iâˆˆâ„\). We therefore get
  \[ \abs{â„^{n_1}âˆ©\prod_{jâˆˆ[n_1]}\{x^{s'}_j,y^{s'}_j\}}=\abs{\prod_{jâˆˆ[n_1]}â„âˆ©\{x^{s'}_j,y^{s'}_j\}}=2^{n_1-k} \]
  using~\cref{lem:intersection-and-product} for the set of finite vertices, \(â„^{n_1}âˆ©\prod_{jâˆˆ[n_1]}\{x^{s'}_j,y^{s'}_j\}\).
\end{proof}

\begin{lemma}\label{lem:non-finite-regions}
  Let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be a spike train such that \(C_{s'}â‰ âˆ…\). Then the following conditions are equivalent:
  \begin{enumerate}[noitemsep, topsep=3pt, label=(\alph*)]
  \item \(s'\) is constant in at least one component,
  \item \(C_{s'}\) has \(â‰¤2^{n_1-1}\) finite vertices,
  \item \(C_{s'}\) has a non-finite vertex,
  \item \(C_{s'}\) is unbounded.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let first \(x^{s'},y^{s'}\) be defined as in~\cref{prop:const-regions-cuboids}, such that we have \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†\).
  \begin{itemize}
  \item \((a)â‡”(b)\): Follows directly by~\cref{lem:constant-components-finite-vertices}.
  \item \((b)â‡’(c)\): 
  A non-empty cube \(âŸ¦x^{s'},y^{s'}â¦†\) has
  \[ \abs{\prod_{jâˆˆ[n_1]}\{x^{s'}_j,y^{s'}_j\}}=2^{n_1} \]
  vertices, so assuming \(C_{s'}\) has \(â‰¤2^{n_1-1}\) finite vertices, it must have at least one non-finite one.
  \item \((c)â‡’(b)\): If \(C_{s'}\) has a non-finite vertex, then the number of finite vertices is smaller than \(2^{n_1}\). On the other hand the number of finite vertices must be a power of \(2\) by~\cref{lem:constant-components-finite-vertices}, so the number must be smaller or equal \(2^{n_1-1}\).
  \item \((c)â‡”(d)\): If all vertices are finite, then in particular \(x^{s'},y^{s'}âˆˆâ„^{n_1}\) and \(âŸ¦x^{s'},y^{s'}â¦†\) is clearly bounded. 
  Is one vertex non-finite, then either \(x^{s'}\) or \(y^{s'}\) has a component that is \(Â±âˆ\). Suppose we have \(x^{s'}_i=-âˆ\). Since \(C_{s'}â‰ âˆ…\) we have a \(zâˆˆC_{s'}\). By definition of \(âŸ¦x^{s'},y^{s'}â¦†\), we have now \(z'âˆˆâŸ¦x^{s'},y^{s'}â¦†\) with \(âˆ€_{iâ‰ j}z'_j=z_j\) and \(z'_iâˆˆ[-âˆ,z_i]\) arbitrary. So \(C_{s'}\) is not bounded. The proof for \(y^{s'}_i=âˆ\) is analogous.
  \end{itemize}
\end{proof}

\begin{comment}
\begin{proof}
  By~\cref{rem:regions-non-recursive-defs} \(p_{i;Ïƒ}(t;x)\) is a non-constant linear function for \(tâ‰¥1\) and fixed \(Ïƒ\). We have therefore \(âˆƒ_zs_{i;Ïƒ}(t;(-âˆ,z_i))=0\) and \(âˆƒ_zs_{i;Ïƒ}(t;[z_i,âˆ))=1\) for any fixed \(tâˆˆ[T]\) and \(Ïƒ\). Since there are only finitely many time-steps \(tâˆˆ[T]\) and spike trains \(Ïƒ\) and, we get \(s_{i;Ïƒ}(t;(-âˆ,z_i))=0\) or \(s_{i;Ïƒ}(t;[z_i,âˆ))=1\) respectively for any \(Ïƒ\) and \(tâˆˆ[T]\), as well as any \(z\) with \(z_i\) small or large enough.

  By choosing the component \(z_i\) correctly small or large enough we therefore get a \(zâˆˆC_{s'}\). We further get for any \(iâˆˆ[n_1]\) that if \(âˆ€_ts'_i(t)=0\), then by construction \(s_i(t;(-âˆ,z_i))=1\), so \(âˆ€_{Î´>0}z-Î´e_iâˆˆC_{s'}\) and therefore \(x_i=-âˆ\). If on the other hand \(âˆ€_ts'_i(t)=1\), then \(s_i(t;[z_i,âˆ))=0\), so \(âˆ€_{Î´â‰¥0}z+Î´e_iâˆˆC_{s'}\) and therefore \(y_i=âˆ\).

  Since \(Tâ‰¥1\) by definition, we have either \(x^{s'}_i=-âˆ\) or \(y^{s'}_i=âˆ\) by definition of \(s'\) for each \(iâˆˆ[n_1]\). By choosing the only finite one from \(\{x^{s'}_i,y^{s'}_i\}\) for each \(iâˆˆ[n_1]\) we get the only finite vertex of \(C_{s'}\).
\end{proof}
\end{comment}

\begin{comment}
\begin{lemma}\label{lem:region-vertex}
  If \(qâˆˆâ„^{n_1}\) is a (finite) vertex of a region \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†âˆˆC_{Î¦,1}\), \(s'âˆˆ\{0,1\}^{n_1Ã—T}\), then \(âˆ€_{iâˆˆ[n_1]}âˆ€_{Î´>0}s_i(Â·;q)â‰ s_i(Â·;q-Î´(âˆ‚_q)_ie_i)\).
  Here \(âˆ‚_qâ‰”2H(x^{s'}+y^{s'}-2q)-1\) points is a vector pointing from \(q\) roughly in the direction of the opposite vertex of the cube \(C_{s'}\) (We allow \(x^{s'},y^{s'}\) to have \(-âˆ\) or \(âˆ\) as components here).
\end{lemma}

\begin{proof}
  Proof by contradiction: If the statement is wrong, than there exists an \(iâˆˆ[n_1]\), such that for every \(Î´>0\) with \(s_i(Â·;q)=s_i(Â·;q-Î´(âˆ‚_q)_ie_i)\).
  Since the other components of \(p(Â·;x)\) as well as \(s(Â·;x)\) are only affected by the value of \(x_i\) through \(s_i(Â·;x)\) we further get \(s(Â·;q)=s(Â·;q-Î´(âˆ‚_q)_ie_i)\). So \(q-Î´(âˆ‚_q)_ie_iâˆˆC_{s'}\).

  On the other hand \(x^{s'}_iâ‰¤q_i-Î´(âˆ‚_q)_i<y^{s'}_i\) cannot be true: Assume that it is. Since \(q\) is a (finite) vertex of \(âŸ¦x^{s'},y^{s'}â¦†\), we have \(q_iâˆˆ\{x^{s'}_i,y^{s'}_i\}\). Case distinction: Suppose \(q_i=x^{s'}_i\). Then \((âˆ‚_q)_i=1\) and \(x^{s'}_iâ‰¤q_i-Î´(âˆ‚_q)_i\) is wrong. Suppose on the other hand \(q_i=y^{s'}_i\). Then \((âˆ‚_q)_i=-1\) and \(q_i-Î´(âˆ‚_q)_i<y^{s'}_i\) is wrong.
\end{proof}
\end{comment}

% I'm an idiot. The proposition I tried to proof is wrong and I could've checked it with my website in 5min
\begin{comment}

  % DONE: not essential/not actually needed in proof? still leave it?
\begin{lemma}\label{lem:convex-closure}
  Let \(MâŠ‚â„^n\) be convex. Then the closure \(\overline{M}\) is convex as well.
\end{lemma}

\begin{proof}
  Let \(x,yâˆˆ\operatorname{M}\) and \(tâˆˆ[0,1]\). We will proof \(zâ‰”x+(y-x)tâˆˆ\operatorname{M}\). By definition of the closure have sequences \((x_n)_{nâˆˆâ„•},(y_n)_{nâˆˆâ„•}âŠ‚M\) with \(x_nâ†’x\), \(y_nâ†’y\) for \(nâ†’âˆ\). We get a corresponding sequence \(z_nâ‰”x_n+(y_n-x_n)t\) with \(z_nâ†’z\), since
  \begin{align*}
   \abs{z-z_n} &=\abs{(x-x_n)(1-t)+(y-y_n)t} \\
               &â‰¤(1-t)\abs{x-x_n}+t\abs{y-y_n} \\
               &â†’0
  \end{align*}
  for \(nâ†’âˆ\).
\end{proof}

%CANCELLLED: remove collission of notation for H
\begin{lemma}\label{lem:sep-conv-set-and-point-by-hypr}
  Let \(MâŠ‚â„^n\) and \(xâˆˆâ„^mâˆ–\overline{\operatorname{conv}(M)}\). Then there exists an affine hyperplane \(HâŠ‚â„^m\) with \(xâˆˆH\) and \(Hâˆ©\operatorname{conv}(M)=âˆ…\).
\end{lemma}

% CANCELLED: image
\begin{proof}
  Let now \(Î´=\operatorname{dist}(x,\operatorname{conv}(M))â‰”\inf_{yâˆˆ\operatorname{conv}(M)}(\norm{x-y}_2)\). We clearly have \(Î´>0\), since otherwise \(xâˆˆ\operatorname{\operatorname{conv(M)}}\). Let \((y_n)_{nâˆˆâ„•}\) be a sequence with \(\norm{x,y_n}_2â†’Î´\) for \(nâ†’âˆ\). Since \(â„^n\) is complete we can assume w.l.o.g. that \((y_n)_n\) converges to \(yâˆˆâ„^n\). We now choose \(Hâ‰”x+\{zâˆˆâ„^n\mid âŸ¨z,x-yâŸ©=0\}\). This is indeed a hyperplane, since \(\norm{x-y}_2=Î´>0\) and therefore \(xâ‰ y\).

  Suppose there is a \(zâˆˆHâˆ©\operatorname{conv}(M)\). We then have \(y'â‰”y+(z-y)\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}\). By using \(\norm{Â·}_2^2=âŸ¨Â·,Â·âŸ©\) we get
  \[ \norm{x-y'}_2^2=\norm{x-y-(z-y)\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}}_2^2=\norm{x-y}_2^2-\norm{(z-y)\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}}_2^2 \]
  since \(x-y'âŠ¥y'-y\):
  \begin{align*}
    &âŸ¨x-y',(z-y)\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}âŸ© \\
    &=\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}âŸ¨x-y-(z-y)\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2},z-yâŸ© \\
    &=\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}âŸ¨x-y,z-yâŸ©-\frac{âŸ¨x-y,z-yâŸ©^2}{\norm{z-y}_2^4}âŸ¨z-y,z-yâŸ© \\
    &=0
  \end{align*}
  So \(\norm{x-y'}_2<\norm{x-y}_2=Î´\), since \(xâ‰ y\) and \(zâ‰ y\) (\(yâˆ‰H\)). We further have \(\operatorname{dist}(y',\operatorname{conv}(M))=0\):

  First notice that \(0â‰¤\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}<1\). Since \(zâˆˆH\), we have \(âŸ¨z-x,x-yâŸ©=0\) and therefore \(\norm{x-y}_2^2+\norm{z-x}^2_2=\norm{z-y}_2^2\), so we get
  \[ âŸ¨x-y,z-yâŸ©â‰¤\norm{z-y}\norm{x-y}<\norm{z-y}^2 \]
  by the Cauchy-Schwarz-Inequality and \(zâ‰ x\). Notice that we can proof \(\frac{âŸ¨z-x,z-yâŸ©}{\norm{z-y}_2^2}<1\) in the same way.
  Since we have \(\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}+\frac{âŸ¨z-x,z-yâŸ©}{\norm{z-y}_2^2}=1\) we get \(0â‰¤\frac{âŸ¨x-y,z-yâŸ©}{\norm{z-y}_2^2}\).

  %CANCELLED: notation [y,z]
  So \(y'âˆˆ[y,z]âŠ‚\overline{\operatorname{conv}(M)}\) by~\cref{lem:convex-closure}.

  So there are \(y''âˆˆ\operatorname{conv}(M)\) with \(\norm{y''-y'}_2<Î´-\norm{x-y'}_2\) and we get \(\norm{x-y''}<Î´\). This contradicts the definition of \(Î´\).
\end{proof}

\begin{lemma}\label{lem:pov-hyprplane}
  If \(HâŠ‚â„^n\) is an affine hyperplane, then there exists a dimension \(iâˆˆ[n]\) such that \(Ï€_{[n]âˆ–\{i\}}(H)=â„^{n-1}\).
\end{lemma}

%CANCELLED: intuition there is pov such that the plane appears to cover everything

\begin{proof}
  Suppose this is not the case. Let \(xâˆˆH\). Then \(0â‰ \ker(Ï€_{[n]âˆ–\{i\}}|_{H-x})âŠ‚\ker(Ï€_{[n]âˆ–\{i\}}|_{â„^n})â‰…â„\), and therefore \(e_iâˆˆH-x\). But this contradicts \(H-x\) having dimension \(n-1\).
\end{proof}

%CANCELLED: general assumption W=I_{n_1}
\begin{proposition}
  Let \(P\) be the set of all finite vertices of regions \(C_{s'}\), \(s'âˆˆ\{0,1\}^{n_1Ã—T}\), with \(âˆ€_{iâˆˆ[n_1]}âˆ€_{t,t'âˆˆ[T]}s'_i(t)=s'_i(t')\). Then all (finite) vertices of the constant regions \(C_{s'}âˆˆC_{Î¦,1}\) with \(W=I_{n_1}\) are contained in the convex hull of \(P\).

  We further have \(\operatorname{conv}(P)âŠ‚[a,b]\) where
  %CANCELLED: improve inequalities
  \begin{align*}
   a &â‰” \\
   b &â‰”\max(-Î²u(0)-Î±i(0),)+(-b)+Ï‘Â·ğŸ™_{n_1}
  \end{align*}
  % where \(a_0\) is the higher vertex of the region \(C_0â‰”\{xâˆˆâ„^{n_0}\mid âˆ€_{tâˆˆ[T]}s(t;x)=0\}\), \(C_0=âŸ¦a_0,b_0â¦†\), with constant \(0\) spike train and \(b_1\) is the lower vertex of the region \(C_1â‰”\{xâˆˆâ„^{n_0}\mid âˆ€_{tâˆˆ[T]}s(t;x)=1\}\), \(C_1=âŸ¦a_1,b_1â¦†\) with constant \(1\) spike train.
\end{proposition}

%CANCELLED: add image

%CANCELLED: add name for spike trains/regions with constant spike trains, simplify proofs/propositions

\begin{proof}
  %CANCELLED: put in proposition
  We will first proof that if \(xâˆˆâ„^{n_1}\) is a point with \(âˆƒ_{iâˆˆ[n_1]}âˆƒ_{t,t'âˆˆ[T]}s'_i(t)â‰ s'_i(t')\), then \(xâˆˆ\operatorname{conv}(P)\). Suppose \(xâˆ‰\operatorname{conv}(P)\). Then there is an affine hyperplane \(HâŠ‚â„^{n_1}âˆ–\operatorname{conv}(P)\) with \(xâˆˆH\) by~\cref{lem:sep-conv-set-and-point-by-hypr}. We further have by~\cref{lem:pov-hyprplane}, that there is a dimension \(i'âˆˆ[n]\) such that \(Ï€_{[n]âˆ–\{i'\}}(H-x)=â„^{n-1}\). So \(x+e_{i'}âˆ‰H\) and we have a vector \(x+yâˆˆH\) such that \(âˆ€_{iâˆˆ[n]âˆ–\{i'\}}y_iâ‰ 0\). If possible we choose \(y\) such that \(y_{i'}â‰ 0\). If, not we get \(Ï€_{i'}(H)=x_i\).

  Let us now regard the region \(C_{s_y}\), where we define \(s_y\) such that \(âˆ€_{iâˆˆ[n]âˆ–\{i\}}âˆ€_{tâˆˆ[T]}(s_y)_i(t)=1â‡”y_iâ‰¥0â‡”y_i>0\), and \(âˆ€_t(s_y)_{i'}(t)=1\) exactly when \(y_i>0\) if \(y_iâ‰ 0\) and else if \(âˆƒ_{qâˆˆ\operatorname{conv}(P)}(q+[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\) is true.

  By definition of \(s_y\) we can apply~\cref{lem:inf-regions} and obtain that \(C_{s_y}\) has one finite vertex \(zâˆˆP\), that \(C_{s_y}=âŸ¦x^{s_y},y^{s_y}â¦†\) with \(âˆ€_{iâˆˆ[n_1]}(x^{s_y})_i<(y^{s_y})_i\) and \(x^{s_y}_i=-âˆ\) if \(âˆ€_{tâˆˆ[T]}(s_y)_i(t)=0\) as well as \(y^{s_y}_i=âˆ\) if \(âˆ€_{tâˆˆ[T]}(s_y)_i(t)=1\) for all \(iâˆˆ[n_1]\). So by definition of \(s_y\) there exists a \(Î´>0\) such that \(âˆ€_{iâˆˆ[n]âˆ–i'}(x+Î´y)_iâˆˆ[x^{s_y}_i,y^{s_y}_i)\). If \(y_iâ‰ 0\), there even exists \(Î´>0\) with \(âˆ€_{iâˆˆ[n]âˆ–i'}(x+Î´y)_iâˆˆ[x^{s_y}_i,y^{s_y}_i)\), so \(x+Î´yâˆˆC_{s_y}\) and similarly \(x-Î´yâˆˆC_{s_{-y}}\).

  We will now proof \(x+Î´yâˆˆC_{s_y}\) and \(x-Î´yâˆˆC_{s_{-y}}\) for \(y_i=0\): First note that if \(âˆƒ_{qâˆˆ\operatorname{conv}(P)}(q+[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\) is false, we instead get \((q-[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\), since there exists a \(zâˆˆH\) with \(Ï€_{[n]âˆ–\{i'\}}(z)=Ï€_{[n]âˆ–\{i'\}}(q)\).
  Furthermore if \((q+[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\) is true for any \(qâˆˆ\operatorname{conv}(H)\), it is true for all points in \(\operatorname{conv}(P)\): Let \(zâˆˆ(q+[0,âˆ)e_{i'})âˆ©H\) and \(q'âˆˆ\operatorname{conv}(P)\) be another such point. We then have \(z'âˆˆH\) such that \(Ï€_{[n]âˆ–\{i'\}}(z')=Ï€_{[n]âˆ–\{i'\}}(q')\). Suppose \(z'âˆˆq'-[0,âˆ)e_{i'}\), we have \([q,q']âŠ‚\operatorname{conv}(P)\) and \([z,z']âŠ‚H\) with \([q,q']âˆ©[z,z']â‰ âˆ…\)\comment{CANCELLED: explain this?}, but \(\operatorname{conv}(P)âˆ©H=âˆ…\).

  Case distinction: Let \(âˆ€_{tâˆˆ[T]}(s_y)_{i'}(t)=0\). We then have \((z-[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\). By choice of \(y\), we have \(Ï€_{i'}(H)=x_{i'}\) and therefore \(x_{i'}â‰¤z_{i'}\). Since \(âˆ€_{tâˆˆ[T]}(s_y)_{i'}(t)=0\), we have \(x_{i'}^{s_y}=-âˆ\) and therefore \((x+Î´y)_{i'}=x_{i'}âˆˆ[x_{i'}^{s_y}, z_{i'})=[x_{i'}^{s_y},y_{i'}^{s_y})\).
  %DONE: note on the meaning of [-âˆ,x)

  Let now \(âˆ€_{tâˆˆ[T]}(s_y)_{i'}(t)=1\). We then have \((z+[0,âˆ)e_{i'})âˆ©Hâ‰ âˆ…\). By choice of \(y\), we have \(Ï€_{i'}(H)=x_{i'}\) and therefore \(z_{i'}â‰¤x_{i'}\). Since \(âˆ€_{tâˆˆ[T]}(s_y)_{i'}(t)=1\), we have \(y_{i'}^{s_y}=âˆ\) and therefore \((x+Î´y)_{i'}=x_{i'}âˆˆ[z_{i'},y_{i'}^{s_y})=[x_{i'}^{s_y},y_{i'}^{s_y})\).

  So we get in both cases \(x+Î´yâˆˆC_{s_y}\) and in the same way \(x-Î´yâˆˆC_{s_{-y}}\).
\end{proof}

\end{comment}

%DONE: use âŸ¦â¦† notation

\begin{comment}
\begin{lemma}
  Let \(s'âˆˆ\{0,1\}^{n_1Ã—T}\) be non-constant in every component, then \(C_{s'}\) only has finite vertices.
\end{lemma}

%DONE: define finite vertice, define vertice

\begin{proof}
  By~\autoref{prop:const-regions-cuboids} the vertices of \(C_{s'}\) can only contain \(-âˆ\) or \(âˆ\) if \(s'\) has a constant component. Otherwise the supremum and infimum are always well-defined as real numbers.
\end{proof}
\end{comment}

In the following, final proposition of this chapter we show that we can find a tight boundary around all finite vertices.

%CANCELLED: read previous paper for inspiration in grammatical style
\begin{proposition}\label{prop:finite-vertices-bounds}
  Let \(P\) be the set of all finite vertices of regions \(C_{s'}âˆˆC_{Î¦,1}\).

  We then have \(PâŠ‚âŸ¦x^C,y^CâŸ§\) for
  %CANCELLED: idea for x^C_i: leftmost border of the left-mostest regions
  \begin{align*}%MAYBE: factor indices under min/max out to defined set
   x^C_i &â‰” \min_{\substack{tâˆˆ[T],\ Ïƒâˆˆ\{0,1\}^{n_1Ã—t}\\ âˆ€_{tâˆˆ[T]}Ïƒ_i(t)=0}}g(t;Ïƒ), \\
   y^C_i &â‰” \max_{\substack{tâˆˆ[T],\ Ïƒâˆˆ\{0,1\}^{n_1Ã—t} \\ âˆ€_{tâˆˆ[T]}Ïƒ_i(t)=1}}g(t;Ïƒ).
  \end{align*}
  In fact every component in \(x^C_i\) is maximal and every component in \(y^C_i\) minimal with \(PâŠ‚âŸ¦x^C,y^CâŸ§\).
  %DONE: x^C_i etc. are minimal/maximal such that this is true
  We further have \(x^C_i = \min_{tâˆˆ[T]}g_i(t;s^{x^C_i})\) and \(y^C_i = \max_{tâˆˆ[T]}g_i(t;s^{y^C_i})\), where
  \begin{align*}
   s^{x^C_i}_j(t) &â‰”\begin{cases} 1 & (iâ‰ j) âˆ§ (v_{ij}>0) \\ 0 & (i=j) âˆ¨ (v_{ij}â‰¤0) \\ \end{cases}, \\
   s^{y^C_i}_j(t) &â‰”\begin{cases} 1 & (i=j) âˆ¨ (v_{ij}<0) \\ 0 & (iâ‰ j) âˆ§ (v_{ij}â‰¥0) \\ \end{cases}.
  \end{align*} %DONE: introduce notation âˆ§,âˆ¨
  %CANCELLED: geometric series
  %DONE: min is applied by component
\end{proposition}

\begin{remark}
  From~\cref{prop:finite-vertices-bounds} we obtain that \(C_{s'}âŠ‚âŸ¦x^C,y^Câ¦†\) for all regions \(C_{s'}âˆˆC_{Î¦,1}\) with non-constant spike train \(s'\):
  By~\cref{lem:non-finite-regions} all vertices of \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†\) are finite, so \(x^{s'},y^{s'}âˆˆPâŠ‚âŸ¦x^C,y^CâŸ§\) and therefore \(âŸ¦x^{s'},y^{s'}â¦†âŠ‚âŸ¦x^C,y^Câ¦†\).
\end{remark}

\begin{remark}
  \cref{prop:finite-vertices-bounds} is in particular useful, when computing \(\abs{C_{Î¦,1}}\): We know that every region \(C_{s'}âˆˆC_{Î¦,1}\) has at least one finite vertex by~\cref{lem:constant-components-finite-vertices} and by~\cref{prop:finite-vertices-bounds} we know where to look for those vertices.
\end{remark}

\begin{proof}
  We will first proof \(PâŠ‚âŸ¦x^C,y^Câ¦†\): Let \(vâˆˆP\) be a finite vertex of region \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†\). Now \(âˆ€_{iâˆˆ[n_1]}v_iâˆˆ\{x^{s'}_i,y^{s'}_i\}âˆ©â„\), so in particular in the case of \(v_i=x^{s'}_i\) for a \(iâˆˆ[n_1]\), there is time-step \(t_1âˆˆ[T]\) such that \(s'(t_1)=1\); is instead \(v_i=y^{s'}_i\) then there is time-step \(t_0âˆˆ[T]\) such that \(s'(t_0)=0\).

  Let now \(iâˆˆ[n_1]\) and \(v_i=x^{s'}_i\) be given and \(t_1\) be minimal with \(s'(t_1)=1\) and define \(s^1âˆˆ\{0,1\}^{n_1Ã—t_1}\) by \( s^0(t)â‰”\zeroV{n_1} \). By choice of \(t_1\), we then have  \(âˆ€_{t<t_1}s^1_i(t)=0\). Further, since \(g(t;Ïƒ)\) only consumes the first \(t-1\) time-steps of the given spike train \(Ïƒ\) (see~\cref{rem:g-prefix}), we have \(g_i(t_1;s^0)=g_i(t_1;s')\). Thus
  \[ x^C_i=\min_{\substack{tâˆˆ[T],\ Ïƒâˆˆ\{0,1\}^{n_1Ã—t}\\ âˆ€_{tâˆˆ[T]}Ïƒ_i(t)=0}}g_i(t;Ïƒ) â‰¤\sup_{\substack{tâˆˆ[T] \\ s'_i(t)=1}}g_i(t;s') \]
  since \(g_i(t_1;s^1)\) appears on the left hand and \(g_i(t_1;s')\) on the right hand. We can similarly construct \(s^0\) and obtain
  \[ y^C_i = \max_{\substack{tâˆˆ[T],\ Ïƒâˆˆ\{0,1\}^{n_1Ã—t} \\ âˆ€_{tâˆˆ[T]}Ïƒ_i(t)=1}}g_i(t;Ïƒ)â‰¥\inf_{\substack{tâˆˆ[T] \\ s'(t)=0}}g_i(t;s'). \]
  Hence \(vâˆˆâŸ¦x^C,y^CâŸ§\).
  
  In fact, \(x^C\), \(y^C\) are chosen optimal: Let \(iâˆˆ[n_1]\). Now the spike train \(s'â‰”s^{x_i^C}\) is constant in every component, so \(C_{s'}=âŸ¦x^{s'},y^{s'}â¦†\) is non-empty by~\cref{lem:inf-corner-regions} and has a finite vertex \(vâˆˆâ„^{n_1}\) (exactly one) by~\cref{lem:constant-components-finite-vertices}. This vertex must have \(v_i=y_i^{s'}\), since \(âˆ€_{tâˆˆ[T]}s'_i(t;x)=0\) by definition and therefore \(x_i^{s'}=-âˆ\). Further
  \[v_i=y_i^{s'}=\inf_{\substack{tâˆˆ[T] \\ s'_i(t)=0}}g_i(t;s')=\min_{\substack{tâˆˆ[T]}}g_i(t;s') =x^C_i\]
  by definition. We similarly get a vertex \(vâˆˆP\) with \(v_i=x_i^{s^{y_i^C}}=y^C_i\).

  Finally, it suffices to compute \(g\) on \(s_j^{x^C_i}\) and \(s_j^{y^C_i}\) to obtain the boundaries. Consider the definition of \(g\) again. Since it is quite unwieldy we have split it up into the following functions,
  \begin{align*}
    g^r_i(t;Ïƒ)&â‰”-\frac{\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki_i(0)+b+\sum_{l=1}^kÎ±^{k-l}âŸ¨v_i,Ïƒ(l-1)âŸ©\right) +Î²^tu_i(0)}{\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}}, \\
    g^{Ï‘}_i(t;Ïƒ)&â‰”Ï‘\frac{1+\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ_i(k)}{\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}}
  \end{align*}
  such that \(g(t;Ïƒ)=g^{r}(t;Ïƒ)+g^Ï‘(t;Ïƒ)\). Now fixing \(t\) and requiring \(âˆ€_{tâˆˆ[T]}Ïƒ_i(t)=0\), the function \(g^Ï‘_i(t;Ïƒ)\) is constant; further \(g^r_i(t;Ïƒ)\) is minimal for \(âˆ€_{iâ‰ j}Ïƒ_j(t)=1_{((v_i)_j>0)}\), so in particular for \(Ïƒ=s^{x_i^C}\). Hence we indeed have \(x^C_i = \min_{tâˆˆ[T]}g_i(t;s^{x^C_i})\). We similarly get \(x^C_i = \min_{tâˆˆ[T]}g_i(t;s^{y^C_i})\).
  %DONE: check if g_i or g
\end{proof}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn.png}
    \caption{The landscape of a \dtlifsnn.}
    \label{fig:ch04-upper-bound-dtlifsnn}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn.png}
    \caption{The landscape of a \rdtlifsnn.}
    \label{fig:ch04-upper-bound-rdtlifsnn}
  \end{subfigure}
  \caption{Comparison of \dtlifsnn vs. \rdtlifsnn; White lines separate regions with a different spike count; Pink lines separate regions with a different spike count on the previous time step; Black lines are borders between regions with different spike trains that are not yet colored white or pink.}
\end{figure}


Theorem 4.3 of~\cite{nguyen2025timespikeunderstandingrepresentational} states the following:

\begin{theorem}\label{thm:regions-upper-bound}
  Consider a \dtlifsnn \(Î¦\) with \(T\) time steps, input dimension \(n_0\) and \(n_1\) neurons in the first hidden layer. Then the number of constant regions is bounded by
  \begin{equation}
\abs{C_{Î¦}}â‰¤\abs{C_{Î¦,1}}â‰¤\begin{cases} \sum_{i=0}^{n_0}\left(\frac{T^2+T}{2}\right)^i\binom{n_1}{i} & n_1>n_0, \\ \left(\frac{T^2+T+2}{2}\right)^{n_1} & \text{otherwise.} \\ \end{cases}.
  \end{equation}
\end{theorem}

While we were able to get promising empirical results in~\cref{ch:experiments} that seem to confirm that~\cref{thm:regions-upper-bound} also holds for \rdtlifsnn, we were not able to proof them. We will therefore first give a sketch of a proof for the case of \(W=\idM{n_1}\) and \(V=\zeroV{n_1Ã—n_1}\) and then showcase why the generalization to arbitrary \(V\) fails. Let us first state the reduced version:

\begin{theorem}\label{thm:regions-upper-bound-stripped}
  Consider a \rdtlifsnn \(Î¦\) with \(T\) time steps, input dimension \(n_0\), trivial weights, i.e. \(W=\idM{n_1}\) and no recurrent connections, i.e. \(V=\zeroV{n_1Ã—n_1}\). Then the number of constant regions is bounded by
  \begin{equation}
    \abs{C_{Î¦}}â‰¤\abs{C_{Î¦,1}}â‰¤ \left(\frac{T^2+T+2}{2}\right)^{n_1}
  \end{equation}
\end{theorem}

\begin{proof}[Sketch]
  As we have stated before, \(\abs{C_{Î¦}}â‰¤\abs{C_{Î¦,1}}\) follows directly from the fact that the output of the whole network only depends on the binary spike trains of the first layer.

  Note that the separating lines in~\cref{fig:ch04-upper-bound-dtlifsnn} are actual lines instead of line segments like in~\cref{fig:ch04-upper-bound-rdtlifsnn}. This shows the independence of the two neurons in the first hidden layer to the used~\dtlifsnn. In fact this property is inherent to \dtlifsnn, since we get the following equations for \(W=\idM{n_1}\) and \(V=\zeroV{n_1Ã—n_1}\),
  \begin{align*}
    i(t) & = Î±i(t-1)+x, \\
    p(t) & = Î²u(t-1)+i(t)+b, \\
    s(t) & = H(p(t)-Ï‘\oneV{n}), \\
    u(t) & = p(t)-Ï‘s(t)
  \end{align*}
  and in particular
  \begin{align}
    i(t;x) &= Î±^ti(0)+x\sum_{k=1}^tÎ±^{t-k}, \\
    p(t;x) &= Î²^tu(0)+\sum_{k=1}^tÎ²^{t-k}(i(k;x)+b)-Ï‘\sum_{k=1}^{t-1}Î²^{t-k}s(k).
  \end{align}
  Notice that the components \(i_i,p_i,s_i,u_i\) are computed in parallel, without affecting each other, in particular due to \(V=0\). Further the result of \(i_i,p_i,s_i,u_i\) only depends on \(x_i\) since \(W=\idM{n_1}\). It thus suffices to analyze the possible change of the output of \(s_i\) depending on different values for \(x_i\).

  Let us now define \(Î¦_{t,i}\) as the restriction of \(Î¦\) to the \(i\)-th neuron in the first hidden layer, using \(tâˆˆ[T]\) for \(T_{Î¦_{t,i}}\). Due to the previous analysis, the \(s_i\) of \(Î¦\) has the same functionality as \(s_1\) of \(Î¦_{t,i}\).

  We will now proof that \((s(t'))_{t'âˆˆ[t]}\) can take on \(\frac{t^2+t+2}{2}\) many different values for \(tâˆˆ[T]\) by induction on \(t\).
  This suffices to proof the theorem since we then have \(n_1\) independent neurons in the first layer that each can take on a maximal number of \(\frac{T^2+T+2}{2}\) many values.
  Let \(t=1\). This case is obvious, since \(s_i(1)âˆˆ\{0,1\}\).
  Suppose on the other hand now \(t>1\). From now on, all notations like \(s,Î±,Î²,â€¦\) will refer to \(Î¦_{t,i}\), so in particular \(s(t)âˆˆ\{0,1\}\). By induction hypothesis, we have \(\frac{t^2-t+2}{2}\) possible values for \((s(t'))_{t'âˆˆ[t-1]}\).
  Let us further categorize \((s(t'))_{t'âˆˆ[t-1]}\) by the number of spikes it contains. The number must be at least \(0\) and can be at most \(t-1\), so we have \(t\) categories.

  Let us further regard the definition of \(g\) for \(Î±=0\) and \(V=0\),
  \[ g(t;Ïƒ)â‰”-\frac{\sum_{k=1}^tÎ²^{t-k}\left(Î±^ki(0)+b\right) +Î²^tu(0)-Ï‘\left(1+\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ(k)\right)}{\sum_{k=1}^tÎ²^{t-k}\sum_{l=1}^kÎ±^{k-l}}. \]
  Clearly \(g(t;Ïƒ)\) only changes in the subterm \(\sum_{k=1}^{t-1}Î²^{t-k}Ïƒ(k)\) in \(Ïƒ\).
  % Since \(s_i(t;x)\) only depends on \(x_iâˆˆâ„\), we also write \(s_i(t;z)\) for \(zâˆˆâ„\) to mean \(s_i(t;x)\) with \(âˆ€_{iâ‰ j}x_j=0\) and \(x_i=z\).
  Suppose \(x,x'âˆˆâ„\) are given with \(x<x'\) and \(s(Â·;x)â‰ s(Â·;x')\). Then there appears a spike earlier in \(s(Â·;x)\) than in \(s(Â·;x')\) due to~\cref{lem:spike-trains-ineq}.
  In fact, if both \(s(Â·;x)\) and \(s(Â·;x')\) are located in the same category, every spike in \(s(Â·;x)\) appears earlier than the corresponding spike in \(s(Â·;x')\), though we won't proof this here. So \(\sum_{k=1}^{t-1}Î²^{t-k}s(k;x)â‰¥\sum_{k=1}^{t-1}Î²^{t-k}s(k;x')\) and therefore \(g(t;s(Â·;x))â‰¥g(t;s(Â·;x'))\). Notice that the ordering has been reversed.

  Further, due to~\cref{prop:const-regions-cuboids}, a region \(C_{s'}\) with \(s'âˆˆ\{0,1\}^{t-1}\) is only split at time-step \(t\) if \(g(t;s')âˆˆC^{s'}\). But since the regions are half-open cuboids, so half-open intervals in this case, given a category of regions \(Îºâˆˆ[t-1]_0\), we have that only one region of those with spike count \(Îº\) can be split, since the ordering of \(g(t;s(Â·;x))\) is reversed to \(xâˆˆâ„\) inside of category \(Îº\).
  Thus we get at a maximum as many new regions as categories. We therefore obtain \(t+\frac{t^2-t+2}{2}=\frac{t^2+t+2}{2}\) regions in total.
  %DONE: fix cites that should be cref
\end{proof}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn/ch04-upper-bound-dtlifsnn_1.png}
    \caption{\(T=1\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn/ch04-upper-bound-dtlifsnn_2.png}
    \caption{\(T=2\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn/ch04-upper-bound-dtlifsnn_3.png}
    \caption{\(T=3\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn/ch04-upper-bound-dtlifsnn_4.png}
    \caption{\(T=4\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-dtlifsnn/ch04-upper-bound-dtlifsnn_5.png}
    \caption{\(T=5\)}
  \end{subfigure}
  \caption{Development of landscape of a \dtlifsnn through time; Development of landscape of \rdtlifsnns through time; White lines separate regions with a different spike count; Pink lines separate regions with a different spike count on the previous time step; Black lines are borders between regions with different spike trains that are not yet colored white or pink.}
  \label{fig:ch04-dtlifsnn-through-time}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn/ch04-upper-bound-rlifsnn_1.png}
    \caption{\(T=1\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn/ch04-upper-bound-rlifsnn_2.png}
    \caption{\(T=2\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn/ch04-upper-bound-rlifsnn_3.png}
    \caption{\(T=3\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn/ch04-upper-bound-rlifsnn_4.png}
    \caption{\(T=4\)}
  \end{subfigure}
  \begin{subfigure}[t]{0.19\textwidth}
    \centering
    \includegraphics[width=\textwidth]{src/figures/ch04-upper-bound-rlifsnn/ch04-upper-bound-rlifsnn_5.png}
    \caption{\(T=5\)}
  \end{subfigure}
  \caption{Development of landscape of a \rdtlifsnn through time; See~\cref{fig:ch04-dtlifsnn-through-time} for further description.}
  \label{fig:ch04-rdtlifsnn-through-time}
\end{figure}

While we have not proven it for the general case, note that the introduction of \(Î±\) did not affect the theorem, only \(V\) prevents the proof. Let us now analyze why this is the case. One of the major properties the proof relies on is the neurons being independent of each other. While it is easily derived from the equations that this is not the case for \(Vâ‰ \zeroV{n_1Ã—n_1}\), it might be more instructive to look at~\cref{fig:ch04-upper-bound-rdtlifsnn}.

To reduce the impact of the interconnectedness on our proof and â€œcontrolâ€ the chaos that the neurons might inflict on each other, one might consider it helpful to do the same categorization trick as in the previous proof, but applied to all neurons simultaneously, i.e.\ we consider regions of equal number of spikes in each respective component one by one.

Compare the different time-steps in~\cref{fig:ch04-rdtlifsnn-through-time}. The pink lines are the white lines of the respective previous time-step and all black lines are older boundaries that were white and pink before, since every separating line was at some point a new line and every new line separates regions with different spike count\footnote{The regions around a new line differ in exactly one spike}.

We further see that the plane is partitioned by the pink lines into similarly shaped regions, that each contain a (potentially degenerated) white cross. Those white lines of the crosses correspond to \(g_i(T;s(Â·;x))\). The white lines are straight, even on different regions \(C_{s'}\) inside of one category, since we use \(Î²=1\) in both figures: In that case \(g_i\) is constant on spike trains with the same spike count.
This is not the case for \(Î²â‰ 1\), see e.g.~\cref{fig:ch04-non-cross} for a \rdtlifsnn with \(Î²â‰ 1\).

Now for \(Î²=1\) it is not hard to prove that inside of a given category, there is only a single cross with no other parallel lines, which is quite a promising result.
One might think that we can just finish the proof as before since we now know how many new lines get added in time-step \(t\). But a considerable problem arises when considering how many regions exactly will be added at a maximum during this time-step.
That number can be deduced by finding the maximal number of regions from the previous time-step located horizontally/vertically next to each other in a category. In other words: How many regions can a straight line (that is parallel to a coordinate axis) touch inside of a category.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.35\textwidth]{src/figures/ch04-non-cross.png}
    \caption{For \(Î²â‰ 1\) and \(Vâ‰ \zeroV{n_1Ã—n_1}\), the white crosses inside of categories are not made of straight lines}
    \label{fig:ch04-non-cross}
\end{figure}

This question is very much non-trivial, in particular because in contrast to the case of \(V=\zeroV{n_1Ã—n_1}\), it cannot be reduced to looking at the maximal number of regions that can be touched by a straight line in the whole plane. This is due to the categories as well as the crosses being potentially shifted in relation to each other, as can be seen in e.g.~\cref{fig:ch04-rdtlifsnn-through-time}.

% DONE: Î± does not increase

% DONE: images (Î²â‰ 1+Vâ‰ 0, Î²=1+Vâ‰ 0)

% DONE: refer to images
% crosses don't necessarily cross only so many

% CANCELLED: remove lem 4.7?
% MAYBE: more time-steps don't really increase the region that is getting fitted upon

% DONE: lexiographic ordering even in general case (with specific def.) and

% DONE: the regions appear only between â€¦

% CANCELLED: generalize for W arbitrary

% DONE: show that limit can be reached/refer to previous paper

% \begin{lemma}
%   For \(yâˆˆâ„^{n_0}\) the mapping \(f_y:â„â†’\{0,1\}^T\) defined by \(xâ†¦s_i\) with \(s^{[0]}(t)=y+xe_i\) is monotone regarding lexical ordering.
% \end{lemma}

\begin{comment}
% CANCELLED: motivation for theorem
While in theory we would expect the number of constant regions to grow exponentially with time, it grows only quadratically.

\begin{theorem}\label{thm:bound-regions}
A \rdtlifsnn with \(W=I_{n_1}\), â€¦ has at a maximum \((\frac{T^2+T}{2}+1)^n\) different constant regions.
\end{theorem}

\begin{proof}
  Since the number of constant regions of a \rdtlifsnn are just unions of the constant regions of the corresponding first layer, it suffices to compute the maximum number of constant regions of that layer.

  Let us consider by how much the regions can increase going from \(t-1\) to \(tâˆˆ[T]\). We can categorize the regions at \(t-1\) by the number of spikes they have in each component. We shall write \(C_{Ïƒ,t-1}\) for the region with sums \((Ïƒ_1,â€¦,Ïƒ_{n_1})=Ïƒâˆˆ[t-1]_0^{n_1}\) at time-step \(t-1\). Let further \(C_{Î£,t-1}â‰”\{C_{Ïƒ,t-1}\mid Ïƒâˆˆ[t-1]_0^{n_1}\}\). By definition we have \(\abs{C_{Î£,t-1}}â‰¤t^{n_1}\)..

  We will now show in each region \(C_{Ïƒ,t-1}\) only
\end{proof}
% Bobachtungen:
% In den Regionen zwischen Kreuzen ist die Anzahl der gestapelten Regionen nie grÃ¶ÃŸer gleich T
% In den Regionen um die Kreuze Ã¤hnliches

% Ãœberlegungen
% Regionen mit gleicher spike train in einer komponente anschauen, mit linien vergleichen?
% DONE: comparison with Î²=1.5

\begin{corollary}
  %CANCELLED: versions for W arbitrary
\end{corollary}

\begin{proof}
\end{proof}

%CANCELLED: Î²<1 is essential
\end{comment}

%CANCELLED: add depth-search to website

%DONE: add discussion
