\section{Definitions}
\label{ch:defs}

%TODO: Bild
Our type of SNN should be thought of as a composition of an initial input layer, a number of hidden spiking layers with internal state and an affine-linear layer mapping spikes activations over time, so called spike trains, to the value of the output layer.
The following definitions deviate slightly from~\cite{nguyen2025timespikeunderstandingrepresentational}, since we already bake in some of the assumptions (direct encoding and membrane-potential outputs) the paper makes at a later point.
We shall first define the membrane potential \(u^l(t)\) and the spike activations \(s^l(t)\) of the hidden layers:

%TODO: herleitung
\begin{definition}
  The \textbf{input vector} \(i^l(t)âˆˆ\{0,1\}^{n_l}\), the \textbf{spike vector} \(s^l(t)âˆˆ\{0,1\}^{n_l}\) and the \textbf{membrane potential vector} \(u^l(t)\) of a hidden layer \(lâˆˆ[L]\) are recursively defined as
  \begin{align}
    i^l(t) & = Î±^li^l(t-1)+W^ls^{l-1}(t)+V^ls^l(t-1) \\
    p^l(t) & = Î²^lu^l(t-1)+i^l(t)+b^l \\
    s^l(t) & = H(p^l(t)-Ï‘1_{n_l}) \\
    u^l(t) & = p^l(t)-Ï‘s^l(t)
  \end{align}
  with \(s^l(0)=0\) and given
  \begin{enumerate}
    \item[â€¢] \textbf{first layer spike activations}: \((s^0(t))_{tâˆˆ[T]}âˆˆ\{0,1\}^{n_0Ã—T}\)
    \item[â€¢] \textbf{initial membrane potential}: \(u^l(0)âˆˆâ„^{n_l}\)
    \item[â€¢] \textbf{initial input}: \(i^l(0)âˆˆâ„^{n_l}\)
    \item[â€¢] \textbf{weight matrices}: \(W^lâˆˆâ„^{n_lÃ—n_{l-1}}\)
    \item[â€¢] \textbf{bias vectors}: \(b^lâˆˆâ„^{n_l}\)
    \item[â€¢] \textbf{leaky terms}: \(Î±^l,Î²^lâˆˆ[0,1]\)
    \item[â€¢] \textbf{threshold}: \(Ï‘^lâˆˆ(0,âˆ)\)
  \end{enumerate}
  where \(Hâ‰”ğŸ™_{[0,âˆ)}\) is a step function and \(Tâˆˆâ„•\) is the number of simulated time steps.
\end{definition}

In the definition of \(s^l(t)\) we first check whether the additional activation by spikes \(W^ls^{l-1}(t)\) of the previous layers and the bias \(b^l\) plus the decayed previous activation \(Î²^lu^l(t-1)\) passes over a certain threshold \(Ï‘^l1_{n_l}\). If that is not the case we use the just computed value for \(u^l(t)\). If the neuron activates, we remove a threshold worth of value. Of course, we do this for the neurons of the whole layer in parallel.

We further define d.t. LIF-SNN and the function the network realizes:

\begin{definition}
  A \textbf{discrete-time LIF-SNN} of \textbf{depth} \(L\) with \textbf{layer-widths} \((n_0,â€¦,n_{L+1})\) is given by
  \[ Î¦â‰”((W^l,b^l,u^l(0),i^l(0),Î±^l,Î²^l,Ï‘^l)_{lâˆˆ[L]},T,((a_t)_{tâˆˆ[T]},c,V) \]
  where \((a_t)_{tâˆˆ[T]}âˆˆâ„^T\), \(câˆˆâ„^{n_{L+1}}\) and \(Vâˆˆâ„^{n_{L+1}Ã—n_L}\) are the parameters of the output layer.
\end{definition}

\begin{definition}
  A discrete-time LIF-SNN \(Ï•\) \textbf{realizes} the function \(R(Î¦):â„^{n_0}â†’â„^{n_{L+1}}\):
  \[ R(Î¦)(x)=D((s^L(t))_{tâˆˆ[T]})\quad \text{with }s^0(t)â‰”x\]
  where the last layer \(D\) is defined by \( D((s(t))_{tâˆˆ[T]})=\sum^T_{t=1}a_t(Vs(t)+c)\).
\end{definition}

Let us now take a look at some simple examples:

\begin{example}
  Let \(T,Lâˆˆâ„•\). Then there exists a d.t. LIF-SNN with \(âˆ€_{tâˆˆ[T]}s^L(t)=s^0(t)\) for any \(s^0âˆˆ\{0,1\}^{n_0Ã—T}\).

  We can proof this by using constant width \(n_l=n\), weights \(W^l=I_n\), biases \(b^l=0\), initial membrane potential \(u^l(0)=0\), leaky term \(Î²^l=0\) and threshold \(Ï‘^l=1\) for all \(lâˆˆ[L]\).

  We then get by definition:
  \begin{alignat*}{3}
    s^l(t) & = H(& s^{l-1}(t)-1_{n_l})  &= \ s^{l-1}(t) \\
    u^l(t) & = & s^{l-1}(t)-s^l(t)     &= 0
  \end{alignat*}
\end{example}

\begin{example}\label{ex:2}
  Let \(T,Lâˆˆâ„•\). Then there exists a d.t. LIF-SNN with \(âˆ€_{tâˆˆT}s^L(t)=\max_{t'âˆˆ[t-1]}(s^0(t'))\) for any \(s^0âˆˆ\{0,1\}^{n_0Ã—T}\): In this network an output neuron switches on when the corresponding input neurons fires and does not switch off later.

  We can proof this by using constant width \(n_l=n\), weights \(W^l=TÂ·I_n\), biases \(b^l=0\), initial membrane potential \(u^l(0)=0\), leaky term \(Î²^l=1\) and threshold \(Ï‘^l=1\) for all \(lâˆˆ[L]\).

  We then get by definition:
  \begin{alignat*}{2}
    s^l(t) & = H(&u^l(t-1)+TÂ·s^{l-1}(t)-1_{n_l}) \\
    u^l(t) & = &u^l(t-1)+TÂ·s^{l-1}(t)-s^l(t)
  \end{alignat*}
  By adding over all timesteps we obtain
  \begin{alignat*}{1}
    s^l(t) & = H(T\sum_{i=1}^ts^{l-1}(i)-(\sum_{t=i}^{t-1}s^l(i))+1_{n_l}) ) \\
    u^l(t) & = \sum_{i=1}^t(TÂ·s^{l-1}(i)-s^l(i))
  \end{alignat*}
  Since \(\sum_{i=1}^{t-1}s^l(i)+1_{n_l}â‰¤T\), once there is any \(t_0âˆˆ[T]\) with \(s^{l-1}_i(t_0)=1\) for an \(i\), we get \(âˆ€_{tâ‰¥t_0}s^l_i(t)=1\).
  By induction over the layers we clearly get the required property.
\end{example}

\cleardoublepage
