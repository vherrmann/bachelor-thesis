\section{Definition of \rdtlifsnn}
\label{ch:defs}

\subsection{Notations and Conventions}
First a few words about the notation we will use in the thesis:

\newcommand{\rangeI}[2]{\{#1,â€¦,#2\}}
We write \(\rangeI{a}{b}â‰”\{a,a+1,â€¦,b\}\) for the range of integers from \(a\) to \(b\) and in particular \([n]â‰”\rangeI{1}{n}\) and \([n]_0â‰”\rangeI{0}{n}\) for the integers from \(1\) to \(n\) and from \(0\) to \(n\) respectively. Moreover \(âˆ¨\) is used to mean the logical â€œorâ€ and \(âˆ§\) to mean the logical â€œandâ€.

We write \([x,y)â‰”\{zâˆˆâ„\mid xâ‰¤z<y\}\) for the half-open interval between \(xâˆˆâ„\) and \(yâˆˆâ„\) and further use \(âŸ¦x,yâ¦†\) with \(x,yâˆˆâ„^n\) to denote half-open cuboids \(\prod_{i=1}^n[x_i,y_i)\). Similarly \(âŸ¦x,yâŸ§\) is the closed cuboid \(\prod_{i=1}^n[x_i,y_i]\). A cube is a cuboid such that all sides have the same length. We further take \([x,y)\) to be empty for \(x,yâˆˆâ„\) with \(xâ‰¥y\), and \([-âˆ,x)\) to mean \((-âˆ,x)\). Thus, \(âŸ¦x,yâ¦†=âˆ…\) holds exactly when \(y_iâ‰¤x_i\) for a \(iâˆˆ[n]\). 

We further use \(\operatorname{diam}_p(U)â‰”\sup_{x,yâˆˆU}\norm{x-y}_p\) to mean the diameter of \(UâŠ‚â„^m\) regarding the \(p\)-norm \(\norm{Â·}_p\).

\newcommand{\idM}[1]{I_{#1}}
\newcommand{\zeroV}[1]{\mathbf{0}_{#1}}
\newcommand{\oneV}[1]{\mathbf{1}_{#1}}
Continuing, \(e_iâ‰”(0,â€¦,1,â€¦,0)âˆˆâ„^n\) is the \(i\)-th standard basis vector; \(\zeroV{n}â‰”(0,â€¦,0)âˆˆâ„^n\) and \(\oneV{n}â‰”(1,â€¦,1)âˆˆâ„^n\) are the vector containing \(0\)s and \(1\)s in every component respectively and \(\idM{n}âˆˆâ„^{nÃ—n}\) is the identity matrix of size \(nÃ—n\). Moreover, for a matrix \(Wâˆˆâ„^{nÃ—m}\) we write \(w_iâˆˆâ„^{1Ã—n}\) for the \(i\)-th row vector and \(W_{i,j}âˆˆâ„\) for the value of the component \((i,j)\). We add an ordering \(â‰¤\) to vectors via the following: For all \(x,yâˆˆâ„^n\), \(xâ‰¤y\) holds exactly if \(âˆ€_{iâˆˆ[n]}x_iâ‰¤y_i\). Furthermore, \(x<y\) is canonically defined to hold if \(xâ‰¤y\) but not \(x=y\).

If \(f:Uâ†’â„^n\) is a function with arbitrary domain \(U\), then \(f_iâ‰”Ï€_iâˆ˜f\) is the \(i\)-th component function, where \(Ï€_i:â„^nâ†’â„\) is the projection to the \(i\)-th component. We also write \(f^{-1}\) for the inverse of \(f\) and \(f^{-1}(W)â‰”\{xâˆˆU \mid f(x)âˆˆW\}\) for the preimage of \(W\) under \(f\). The set of continuous functions from \(U\) to \(W\) will be written as \(ğ’^0(U,W)\). Analogously, the set of \(k\)-times continuously differentiable functions will be written \(ğ’^k(U,W)\). The total derivative of a differentiable function \(f:Uâ†’W\) is written as \(df:Uâ†’\operatorname{Hom}_{â„}(W,W)\).

The norms \(\norm{Â·}_p\) are either the \(p\)-norm on \(â„^n\) or the operator norm with regard to the \(p\)-norm \(\norm{Â·}_p\) in input and output space. Furthermore, we define \(\norm{f}_{âˆ,p}â‰”\sup_{xâˆˆU}\norm{f(x)}_p\) for a function \(f:Uâ†’â„^n\).

Some additional notation regarding geometry: \(\overline{A}\) is the closure of \(AâŠ‚â„^n\), \(AÂ°\) is the interior; the open balls in \(â„^n\) regarding \(p\)-Norm are notated by \(B_{Îµ,p}(x)â‰”\{y\mid \norm{y-x}_p<Îµ\}âŠ‚â„^n\) for \(Îµ>0\), \(pâ‰¥1\) and \(xâˆˆâ„^n\).

We write \(\max{U}â‰”(\max_{uâˆˆU}u_i)_{iâˆˆ[n]}\) with \(UâŠ‚(â„âˆª\{Â±âˆ\})^n\) for the maximum by component of \(U\), and similarly define \(\min{U}\), \(\inf{U}\) and \(\sup{U}\). In addition we use the conventions \(\inf(âˆ…)=âˆ\) and \(\sup(âˆ…)=-âˆ\).

%DONE: f^-1 for the inverted function, f^-1(U) for preimage
%DONE: define < for vectors (convention)

%DONE: notation for total der.

%MAYBE: introduce notation []_â€¦^â€¦ as product
%DONE: notation \(\norm{f}_{âˆ,2}\), \(\norm{g}_2\) fÃ¼r \(g\) linear

%DONE: notation \(\overline{}\), Â°, closure, interior
%CANCELLED: introduce notation N_Ï(Î©)
%DONE: introduce âˆ¨,âˆ§ notation

\newcommand{\charac}[1]{Ï‡_{#1}}
\newcommand{\oneP}[1]{1_{#1}}
We finally write
\[\charac{M}(x)â‰”\begin{cases} 1 & xâˆˆM \\ 0 & xâˆ‰M \\ \end{cases}\] %CANCELLED: change notation? e.g. 1_M instead?
for the characteristic function of a set \(M\) and
\[\oneP{A}â‰”\begin{cases} 1 & A \\ 0 & Â¬A \\ \end{cases}\] %CANCELLED: change notation? e.g. âŸ¦AâŸ§ instead
for a formula \(A\), such that in particular \(\charac{M}(x)=\oneP{(xâˆˆM)}\).

\subsection{Motivation}

%DONE: hier Ã¼berarbeiten

In~\cite{neuronaldynamics2014} neurons are described as follows (\cref{fig:ch02-structure-of-neuron}):

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{src/figures/ch02-structure-of-neuron.png}
  \caption{Structure of a neuron.\protect\footnotemark}
  \label{fig:ch02-structure-of-neuron}
\end{figure}
\footnotetext{Source: By BruceBlaus - Own work, CC BY 3.0, \url{https://commons.wikimedia.org/w/index.php?curid=28761830 }}

\begin{displayquote}
  A typical neuron can be divided into three functionally distinct parts, called dendrites, soma [also called cell body], and axon; [â€¦]. Roughly speaking, the dendrites play the role of the â€˜input deviceâ€™ that collects signals from other neurons and transmits them to the soma. The soma is the â€˜central processing unitâ€™ that performs an important non-linear processing step: If the total input arriving at the soma exceeds a certain threshold, then an output signal is generated. The output signal is taken over by the â€˜output deviceâ€™, the axon, which delivers the signal to other neurons.
\end{displayquote}

The book further describes that the neuron signals consist of short electrical pulses which we will call spikes. Since the pulses of a neuron look alike and since the pulses never overlap we can regard the output of a neuron as a list of zeros and ones, as a list of bits. We will call such lists spike trains.

Those spike trains update the cell-body by changing an internal variable, the membrane potential of the neuron. It is increased or decreased by incoming spikes depending on the neuron of origin. If the membrane potential reaches a threshold, the neuron will reset the potential to its resting potential and spike such that the subscribed neurons receive an update. If it does not reach the threshold, the potential just decays over time to the resting potential.

We will model the update of subscribed neurons with a decay factor: Instead of having the input to a neuron drop down to the resting state immediately after receiving a spike, the input will decay over time.

\begin{comment}

The nature of the signals traveling between neurons is described in the following paragraph.

\begin{displayquote}
  The neuronal signals consist of short electrical pulses [â€¦]. The form of the pulse does not change as the action potential propagates along the axon. A chain of action potentials emitted by a single neuron is called a spike train â€“ a sequence of stereotyped events which occur at regular or irregular intervals; [â€¦]. Since isolated spikes of a given neuron look alike, the form of the action potential does not carry any information. Rather, it is the number and the timing of spikes which matter. 
\end{displayquote}

Finally, the effect of signals on the state of the cell body is outlined in the last quote.

\begin{displayquote}
  The effect of a spike on the postsynaptic neuron can be recorded with an intracellular electrode which measures the potential difference u(t) between the interior of the cell and its surroundings. This potential difference is called the membrane potential. Without any input, the neuron is at rest corresponding to a constant membrane potential urest. After the arrival of a spike, the potential changes and finally decays back to the resting potential, [â€¦].
\end{displayquote}

Finally the connection between two neurons has a small gap, called synapse:

\begin{displayquote}
  The site where the axon of a presynaptic neuron makes contact with the dendrite (or soma) of a postsynaptic cell is the synapse. [â€¦] At a chemical synapse, the axon terminal comes very close to the postsynaptic neuron, leaving only a tiny gap between pre- and postsynaptic cell membrane. This is called the synaptic cleft. When an action potential arrives at a synapse, it triggers a complex chain of bio-chemical processing steps that lead to a release of neurotransmitter from the presynaptic terminal into the synaptic cleft. As soon as transmitter molecules have reached the postsynaptic side, they will be detected by specialized receptors in the postsynaptic cell membrane [â€¦].
\end{displayquote}

\end{comment}

From these biologic observations one can derive the following system of differential equations for the input \(I:â„â†’â„\) and membrane potential \(U:â„â†’â„\) of a single neuron (c.f.~\cite{neuronaldynamics2014}):
% DONE: work in {neuronaldynamics2014} and figure

% DONE: graphic, https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png ?
% DONE: link paper for equations
\begin{align*}
 I'(t) &â‰”-Ï„_Î±I(t)+\sum_{i=1}^nw_is_i(t-Î”t) \\
 U'(t) &â‰”-Ï„_Î²U(t)+I(t)+b-Ï‘s(t)
\end{align*}
Here \(s_i:â„â†’\{0,1\}\) represents the \(j\)-th connection of the given neuron spiking at time \(t\), the variables \(w_iâˆˆâ„\) represent the weights of the connections to other neuron and \(Ï‘s(t)\) resets the potential \(U(t)\) after a spike. To avoid unresolvable dependencies between neurons, the connections need to have some latency \(Î”tâˆˆâ„\). The variables \(Ï„_{Î±},Ï„_{Î²}>0\) specify the rate with which \(I\) and \(U\) decay respectively.
We also include the bias \(bâˆˆâ„\) in the differential equation of \(U\) to simplify constructions of \rdtlifsnn later. %While not so much neurologically motivated, the bias \(b\) can be replaced by a constantly spiking neuron adding that bias through the input \(I\).

%DONE: motivate bias

Since we are in an analog scenario, it is reasonable to assume that \(I\) and \(U\) are smoothly differentiable functions. We can therefore use the first-order exponential integrator method to obtain a discretization of \(I\) and \(U\) from the differential equations. Let \(t_0,hâˆˆâ„\) be arbitrary and \(t_{n+1}â‰”t_n+h\). By using the fundamental theorem of calculus we have
\begin{align*}
  &e^{Ï„_{Î±}t_{n+1}}I(t_{n+1})-e^{Ï„_{Î±}t_n}I(t_n) \\
  &=\int_{t_n}^{t_{n+1}}\frac{d}{dt}\left(e^{Ï„_{Î±}t}I(t)\right)dt \\
  &=\int_{t_n}^{t_{n+1}}e^{Ï„_{Î±}t}(I'(t)+Ï„_Î±I(t))dt \\
  &=\int_{t_n}^{t_{n+1}}e^{Ï„_{Î±}t}\left(\sum_{i=1}^nw_is_i(t-Î”t)\right)dt.
\end{align*}
If we assume the input from the spikes of other neurons to be constant during \([t_n,t_{n+1}]\), we get
\begin{align*}
 &=\frac{1}{Ï„_{Î±}}(e^{Ï„_{Î±}t_{n+1}}-e^{Ï„_{Î±}t_n})\sum_{i=1}^nw_is_i(t-Î”t) \\
 &=\frac{1}{Ï„_{Î±}}e^{Ï„_{Î±}t_{n+1}}(1-e^{Ï„_{Î±}h})\sum_{i=1}^nw_is_i(t-Î”t)
\end{align*}
which yields
\[ I(t_{n+1})=e^{-Ï„_{Î±}h}I(t_n)+\frac{1}{Ï„_{Î±}}(1-e^{Ï„_{Î±}h})\sum_{i=1}^nw_is_i(t-Î”t). \]
Let us now use \(h=1\) and absorb \(\frac{1-e^{Ï„_{Î±}}}{Ï„_{Î±}}\) into the weights \((w_i)_{iâˆˆ[n]}\), such that we have
\begin{equation}\label{eq:9}
I(t_{n+1})=Î±I(t_n)+\sum_{i=1}^nw_is_i(t-Î”t)
\end{equation}
by writing \(Î±â‰”e^{-Ï„_{Î±}}\). We similarly obtain
\begin{equation}\label{eq:11}
U(t_{n+1})=Î²U(t_n)+I(t)+b-Ï‘s(t)
\end{equation}
by using the first-order exponential integrator method, defining \(Î²â‰”e^{-Ï„_{Î²}}\) and absorbing \(\frac{1-Î²}{Ï„_{Î²}}\) into \(I(0)\), \((w_i)_{iâˆˆ[n]}\), \(b\) and \(Ï‘\).
Note that \(Î±,Î²âˆˆ[0,1]\) by construction.

\begin{figure}[h]
  \centering
  \input{src/figures/ch02-network-layout.tex}
  \caption{High-level network layout}
  \label{fig:network-layout}
\end{figure}

%DONE: motivate in def. why Î±,Î²âˆˆ[0,1]
We will now arrange those neurons into layers as shown in~\cref{fig:network-layout}, such that neurons are only connected to neurons from the previous layer or their own layer. Of course, since we allow recurrent connections inside of layers we can always just merge all layers into the first one, but as we will see in~\cref{ch:partitions}, the layers allow us to analyze the networks more easily.
For connections between different layers, we can use \(Î”t=0\) in~\eqref{eq:9} since there can be no interdependent connections between neurons in different layers. For connections between neurons in the same layer we use \(Î”t>0\) to prevent those interdependencies. Since we have previously chosen \(h=1\), it is natural to further choose \(Î”t=1\).

%MAYBE: define â€spike trainâ€œ
Since we want to work with arbitrary data and not just spike trains, we further need to encode and decode our data to and from spikes trains. Like~\cite{nguyen2025timespikeunderstandingrepresentational} we choose a simple direct encoding for the encoder \(E\) and membrane potential outputs for the decoder \(D\).
% DONE: why direct encoding, membrane potential output

\subsection{Definitions}
% DONE: graphic for whole network
Our type of SNN should be thought of as a composition of an initial input layer realizing the encoder \(E\), a number of hidden spiking layers with internal state and an affine-linear layer realizing the decoder \(D\). 

We first define the structure of the neurons of a hidden layer. Note that for any given neuron \(iâˆˆ[n_l]\) in a layer \(l\), the functions \(i^{[l]}_i\) and \(u^{[l]}_i\) correspond to \(I\) and \(U\) from the previous section.

% CANCELLED: graphic for single neuron/single layer
\begin{definition}
  The input vector \(i^{[l]}(t)âˆˆ\{0,1\}^{n_l}\), the spike vector \(s^{[l]}(t)âˆˆ\{0,1\}^{n_l}\), the pre-spike membrane potential vector \(p^{[l]}(t)\) and the post-spike membrane potential vector \(u^{[l]}(t)\) of a hidden layer \(Î»=(W^{[l]},b^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\) with index \(lâˆˆ[L]\), are recursively defined as
  \begin{align}
    i^{[l]}(t) & â‰” Î±^{[l]}i^{[l]}(t-1)+W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1), \\
    p^{[l]}(t) & â‰” Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}, \\
    s^{[l]}(t) & â‰” H(p^{[l]}(t)-Ï‘\oneV{n_l}), \\
    u^{[l]}(t) & â‰” p^{[l]}(t)-Ï‘s^{[l]}(t),
  \end{align}
  with \(s^{[l]}(0)=0\) for all \(lâˆˆ[l]\) and given
  \begin{enumerate}
    \item[â€¢] initial membrane potential \(u^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] initial input \(i^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] weight matrices \(W^{[l]}âˆˆâ„^{n_lÃ—n_{l-1}}\), \(V^{[l]}âˆˆâ„^{n_lÃ—n_l}\),
    \item[â€¢] bias vectors \(b^{[l]}âˆˆâ„^{n_l}\),
    \item[â€¢] leaky terms \(Î±^{[l]},Î²^{[l]}âˆˆ[0,1]\) and
    \item[â€¢] threshold \(Ï‘^{[l]}âˆˆ(0,âˆ)\)
  \end{enumerate}
  where \(Hâ‰”Ï‡_{[0,âˆ)}\) is a step function, \(Tâˆˆâ„•\) is the number of simulated time steps and \(Lâˆˆâ„•\) the total number of hidden layers.
\end{definition}

\begin{remark}
  While it is suppressed in the notation, \(i^{[l]}\), \(p^{[l]}\), \(s^{[l]}\) and \(u^{[l]}\) clearly not only depend on \(t\), but by recursion also on \(s^{[0]}\). Further, \(s^{[l]}\) can be represented as an element of \(\{0,1\}^{n_lÃ—T}\), which will become useful later to quantify over spike trains. In particular, we will write \(Ïƒ(t)\) for \(Ïƒâˆˆ\{0,1\}^{n_lÃ—T}\) and \(tâˆˆ[T]\) to mean the \(t\)-th column vector of \(Ïƒ\).
\end{remark}

We further define recurrent d.t. LIF-SNN and the function the network realizes:
\begin{definition}
  A \textbf{recurrent discrete-time LIF-SNN}, also called \rdtlifsnn, of \textbf{depth} \(L\) with \textbf{layer-widths} \((n_0,â€¦,n_{L+1})\) and \(Tâˆˆâ„•\) time-steps is given by
  \[ Î¦â‰”((W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})_{lâˆˆ[L]},T,(E,D)) \]
  where the \textbf{input encoder} \(E:â„^{n_0}â†’â„^{n_0Ã—T}\) maps a vector \(xâˆˆâ„^{n_0}\) to a corresponding first layer spike activation \(âˆ€_{tâˆˆ[T]}s^{[0]}(t)=E(x)(t)\) and the \textbf{output decoder} \(D:\{0,1\}^{n_LÃ—T}â†’â„^{n_{L+1}}\) maps the spike activations of the last hidden layer to values in \(â„^{n_{L+1}}\).
\end{definition}
% DONE: remark about spike trains being equiv. to \{0,1\}^â€¦
% DONE: replace â€recursive SNNâ€œ etc. by â€œrecurrent SNNâ€

% DONE: remark about layers being kinda useless
% DONE: remark comparison with previous model
% CANCELLED: layers mit kursiv?

\begin{definition}
  A recurrent discrete-time LIF-SNN \(Ï•\) \textbf{realizes} the function \(R(Î¦):â„^{n_0}â†’â„^{n_{L+1}}\) defined by
  \[ R(Î¦)(x)=D((s^{[L]}(t))_{tâˆˆ[T]})\quad \text{with }s^{[0]}â‰”E(x).\]
\end{definition}

\begin{definition}
  A recurrent discrete-time LIF-SNN employs \textbf{direct encoding} if we have
  \[ âˆ€_{tâˆˆ[T]}E(x)(t)=x \]
  for the input encoder and has \textbf{membrane potential outputs} if the output decoder can be written as
  \[ D((s^{[L]}(t))_{tâˆˆ[T]})=\sum_{t=1}^Ta_t(W^{[L+1]}s^{[L]}(t)+b^{[L+1]}) \]
  for some \((a_t)_{tâˆˆ[T]}âˆˆâ„^T\), \(b^{[L+1]}âˆˆâ„^{n_{L+1}}\) and \(W^{[L+1]}âˆˆâ„^{n_{L+1}Ã—n_L}\).
\end{definition}
\begin{remark}
  We will only consider recurrent discrete-time LIF-SNN with direct encoding and membrane potential outputs. In fact, we will use \rdtlifsnn to mean â€œ\rdtlifsnn with direct encoding and membrane potentialâ€.
\end{remark}

\begin{remark}
  Our definition of \rdtlifsnn breaks down to the one of \dtlifsnn in~\cite{nguyen2025timespikeunderstandingrepresentational} if we require \(Î±^{[l]}â‰”0\) and \(V^{[l]}â‰”\zeroV{n_lÃ—n_l}\) for all layers \(lâˆˆ[L]\). So \rdtlifsnns can have recursive dependencies inside of the layers and decaying input vectors in contrast to the more simpler \dtlifsnns.
\end{remark}

Let us now take a look at some simple examples of \rdtlifsnn:

\begin{example}\label{ex:ch02-forward-spikes}
  Let \(T,Lâˆˆâ„•\). Then there exists a \rdtlifsnn with \(âˆ€_{tâˆˆ[T]}s^{[L]}(t)=s^{[0]}(t)\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\).

  Let us use constant width \(n_l=n\), weights \(W^{[l]}=\idM{n}\), \(V^{[l]}=\zeroV{nÃ—n}\), biases \(b^{[l]}=0\), initial input \(i^{[l]}(0)=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky terms  \(Î±^{[l]}=Î²^{[l]}=0\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  It follows from the definitions that \(p^{[l]}(t)=i^{[l]}(t)=s^{[l-1]}(t)\) and therefore further
  \[ s^{[l]}(t) = H(s^{[l-1]}(t)-\oneV{n_l}) = s^{[l-1]}(t). \]
  Thus we indeed get \(âˆ€_{tâˆˆ[T]}s^{[L]}(t)=s^{[0]}(t)\) by induction.
\end{example}

\begin{example}
  Let \(T,Lâˆˆâ„•\). Then there exists a \rdtlifsnn with \(âˆ€_{tâˆˆT,iâˆˆ[n]}s^{[L]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t-1]}(s^{[0]}_i(t'))=1\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\), i.e. an output neuron switches on once the corresponding input neurons fires.

  Let us use constant width \(n_l=n\), weights \(W^{[l]}=\idM{n}\), \(V^{[l]}=\idM{n}\), biases \(b^{[l]}=0\), initial input \(i^{[l]}(0)=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky terms \(Î±^{[l]}=0\), \(Î²^{[l]}=0\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  We then get by definition
  \[p^{[l]}(t)=i^{[l]}(t)=s^{[l-1]}(t)+s^{[l]}(t-1)\]
  and therefore
  \[ s^{[l]}(t) = H(s^{[l-1]}(t)+s^{[l]}(t-1)-\oneV{n_l}). \]
  Hence \(s^{[l]}_i(t)=1\) if and only if \(s^{[l-1]}_i(t)=1\) or \(s^{[l]}_i(t-1)=1\). We therefore have \(âˆ€_{tâˆˆT}s^{[l]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t]}(s^{[l-1]}_i(t'))=1\) for all \(iâˆˆ[n],lâˆˆ[L]\). Thus \(âˆ€_{tâˆˆT,iâˆˆ[n]}s^{[L]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t-1]}(s^{[0]}_i(t'))=1\) by induction.

  Even though this construction is more natural defined using \rdtlifsnn, there is also a \dtlifsnn construction achieving the same behavior using the same parameters as before, but with \(V^{[l]}=\zeroV{nÃ—n}\), \(W^{[l]}=TÂ·\idM{n}\) and \(Î²^{[l]}=1\) for all layers \(lâˆˆ[L]\).

  By definition
  \[i^{[l]}(t)=Ts^{[l-1]}(t)\]
  and therefore
  \begin{align*}
    p^{[l]}(t) & =   p^{[l]}(t-1)+Ts^{[l-1]}(t)-s^{[l]}(t-1).
  \end{align*}
  By induction over \(t\) (see also~\cref{lem:non-recursive-defs}) we obtain 
  \[ p^{[l]}(t) = \sum_{k=1}^t\left(Ts^{[l-1]}(k)-s^{[l]}(k-1)\right). \]
  Let now \(iâˆˆ[n]\). Since \(s^{[l]}(0)=0\), we have \(\sum_{k=1}^ts^{[l]}_i(k-1)â‰¤t-1\). Now if and only if there is any \(t_0âˆˆ[T]\) with \(s^{[l-1]}_i(t_0)=1\), we get
  \[p_i^{[l]}(t')=T\sum_{k=1}^{t'}s^{[l-1]}_i(k)-\sum_{k=1}^{t'}s^{[l]}_i(k-1)â‰¥1\]
  for \(t'â‰¥t_0\) and hence \(s_i^{[l]}(t')=1\).
  Just as before we now get the required property for the whole network by induction over the layers.
\end{example}

For a easier construction of our networks we will additionally define allow building networks up from single networks using the following definition.

\begin{definition}\label{def:ch02-def-neuron}
  The \textbf{\(i\)-th neuron} of a hidden layer \(Î»=(W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\) of a \rdtlifsnn is a tuple \((w,b,v,u_0,i_0)\) with \(wâˆˆâ„^{n_{l-1}}\), \(vâˆˆâ„^{n_l}\) and \(b,u_0,i_0âˆˆâ„\), such that \(b\), \(u_0\), \(i_0\) are the \(i\)-th component of \(b^{[l]}\), \(u^{[l]}(0)\), \(i^{[l]}(0)\) respectively and \(w\), \(v\) are the \(i\)-th row vector of \(W^{[l]}\), \(V^{[l]}\) respectively.
\end{definition}

\subsection{Basic properties}

In the following we present some technical but helpful notations and lemmas for writing proofs on the behavior of \rdtlifsnns. Of particular importance are \cref{def:non-recursive-defs} and \cref{lem:non-recursive-defs}, which introduce non-recursive formulas for the defining equations of \rdtlifsnns. 
%DONE: fix references to lem:non-recursive-defs

\begin{definition}\label{def:non-recursive-defs}
  Let \(tâˆˆ[T]\), \(lâˆˆ[L]\) and spike train families \(Ïƒ=(Ïƒ^{[l']})_{l'âˆˆ[l]_0}\), \(Ïƒ'=(Ïƒ'^{[l']})_{l'âˆˆ[l]_0}\) be given, such that \(âˆ€_{l'âˆˆ[l-1]_0}Ïƒ^{[l']}âˆˆ\{0,1\}^{n_{l'}Ã—t}\) and \(Ïƒ^{[l]}âˆˆ\{0,1\}^{n_lÃ—(t-1)}\) as well as \(âˆ€_{l'âˆˆ[l]_0}Ïƒ'^{[l']}âˆˆ\{0,1\}^{n_{l'}Ã—t}\), i.e. \(Ïƒ,Ïƒ'\) have to be chosen such that the following terms are well-defined. We define
  \begin{align}
   i^{[l]}(t;Ïƒ)&â‰”(Î±^{[l]})^ti^{[l]}(0)+\sum_{k=1}^t(Î±^{[l]})^{t-k}\left(W^{[l]}Ïƒ^{[l-1]}(k)+V^{[l]}Ïƒ^{[l]}(k-1)\right), \\
   p^{[l]}(t;Ïƒ) &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ)+b^{[l]}\right)-Ï‘\sum_{k=1}^{t-1}(Î²^{[l]})^{t-k}Ïƒ^{[l]}(k),\\
   s^{[l]}(t;Ïƒ) & â‰” H(p^{[l]}(t;Ïƒ)-Ï‘\oneV{n_l}), \\
   u^{[l]}(t;Ïƒ') &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(k)\right).
  \end{align}
\end{definition}

\begin{remark}
  The spike train families \(Ïƒ\), \(Ïƒ'\) in \cref{def:non-recursive-defs} are chosen such that they only include the data that is actually needed in the definition of \(i^{[l]},p^{[l]},s^{[l]},u^{[l]}\). This is also the reason why we have to use \(Ïƒ'\) in \(u^{[l]}\), in contrast to the definitions of \(i^{[l]},p^{[l]},s^{[l]}\) the definition of \(u^{[l]}\) requires the latest spikes from the current layer layer.

  We will also allow using \(i^{[l]},p^{[l]},s^{[l]},u^{[l]}\) with extensions of the necessary spike trains. In particular, we will allow using the functions with â€œcompleteâ€ spike trains \((Ïƒ^{[l]})_{lâˆˆ[L]}\) with \(Ïƒ^{[l]}âˆˆ\{0,1\}^{n_lÃ—T}\).
\end{remark}

The notation for the previous definitions is justified due to

%DONE: â€œLet \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\) be arbitrary.â€ ?
\begin{lemma}\label{lem:non-recursive-defs} %DONE: introduce notation for s, Ïƒ, s'
  The non-recursive formulas from~\cref{def:non-recursive-defs} are equivalent to the recursive definitions for \(lâˆˆ[L]\), \(tâˆˆ[T]\) assuming previous spikes are equal: \(âˆ€_{l'âˆˆ[l-1]_0}Ïƒ^{[l']}=s^{[l']}\) and \(âˆ€_{t'âˆˆ[t-1]}Ïƒ^{[l]}(t')=s^{[l]}(t')\) as well as \(âˆ€_{l'âˆˆ[l]_0}Ïƒ'^{[l']}=s^{[l']}\).
\end{lemma}

%DONE: introduce convention regarding spike trains function or vector/matrix
\begin{proof}
  We first proof \(âˆ€_{tâˆˆ[T]}i^{[l]}(t;Ïƒ)=i^{[l]}(t)\) and \(âˆ€_{tâˆˆ[T]}u^{[l]}(t;Ïƒ)=u^{[l]}(t)\) for \(Ïƒ^{[l]}=s^{[l]}\) by induction: Let \(t=1\). We have
  \begin{align*}
   i^{[l]}(1;Ïƒ)&=Î±^{[l]}i^{[l]}(0)+W^{[l]}Ïƒ^{[l-1]}(1)+V^{[l]}Ïƒ^{[l]}(0) \\
                         &=Î±^{[l]}i^{[l]}(0)+W^{[l]}s^{[l-1]}(1)+V^{[l]}s^{[l]}(0) \\
                         &=i^{[l]}(1)
  \end{align*}
  and
  \begin{align*}
   u^{[l]}(1;Ïƒ') &= Î²^{[l]}u^{[l]}(0)+i^{[l]}(1;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(1) \\
                            &= Î²^{[l]}u^{[l]}(0)+i^{[l]}(1)+b^{[l]}-Ï‘s^{[l]}(1) \\
                            &= p^{[l]}(1)-Ï‘s^{[l]}(1) \\
                            &= u^{[l]}(1)
  \end{align*}
  by using the definitions and using our assumption \(Ïƒ^{[l]}=s^{[l]}\). We even have \(i^{[l]}(1;Ïƒ')=i^{[l]}(1)\), since \(Ïƒ'\) is an â€œextensionâ€ of \(Ïƒ\).

  Let now \(t>1\). We may assume \(i^{[l]}(t-1;Ïƒ)=i^{[l]}(t-1)\) and \(u^{[l]}(t-1;Ïƒ')=u^{[l]}(t-1)\) such that we obtain
  \begin{align*}
   i^{[l]}(t;Ïƒ)&=(Î±^{[l]})^ti^{[l]}(0)+\sum_{k=1}^t(Î±^{[l]})^{t-k}\left(W^{[l]}Ïƒ^{[l-1]}(k)+V^{[l]}Ïƒ^{[l]}(k-1)\right) \\
                           &=Î±^{[l]}i^{[l]}(t-1;Ïƒ)+\left(W^{[l]}Ïƒ^{[l-1]}(t)+V^{[l]}Ïƒ^{[l]}(t-1)\right) \\
                           &=Î±^{[l]}i^{[l]}(t-1)+W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1) \\
                           &=i^{[l]}(t)
  \end{align*}
  and similarly
  \begin{align*}
   u^{[l]}(t;Ïƒ') &= (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(k)\right) \\
                            &= Î²^{[l]}u^{[l]}(t-1;Ïƒ')+\left(i^{[l]}(t;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(t)\right) \\
                            &= Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}-Ï‘s^{[l]}(t) \\
                            &= p^{[l]}(t)-Ï‘s^{[l]}(t) \\
                            &= u^{[l]}(t).
  \end{align*}
  By substituting \(u^{[l]}\) in \(p^{[l]}\) we further obtain
  \begin{align*}
   p^{[l]}(t;Ïƒ) &= (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ)+b^{[l]}\right)-Ï‘\sum_{k=1}^{t-1}(Î²^{[l]})^{t-k}Ïƒ^{[l]}(k),\\
                            &= Î²^{[l]}u^{[l]}(t-1;Ïƒ)+\left(i^{[l]}(t;Ïƒ)+b^{[l]}\right)\\
                            &= Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}\\
                            &= p^{[l]}(t).
  \end{align*}
  By using the above we obtain
  \[ s^{[l]}(t;Ïƒ) = H(p^{[l]}(t;Ïƒ)-Ï‘\oneV{n_l})=H(p^{[l]}(t)-Ï‘\oneV{n_l})=s^{[l]}(t). \]
\end{proof}

\begin{lemma}\label{lem:charac-floor}
  Let \(a,xâˆˆâ„\), \(aâ‰ 0\) and \(bâˆˆâ„¤\). Then \(\lfloor\tfrac{x}{a}\rfloor=b â‡” 0â‰¤x-ab<a\).
\end{lemma}

\begin{proof}
  \(0â‰¤x-ab<a\) is equivalent to \(bâ‰¤\tfrac{x}{a}<b+1\), which is yet again equivalent to \(\lfloor\tfrac{x}{a} \rfloor=b\) by definition of \(\lfloor Â· \rfloor\).
\end{proof}

\begin{lemma}\label{lem:spike-sum}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(Î²=1\), then \(0â‰¤u^{[l]}_i(t_{Ï‰})<Ï‘\) holds if and only if
  \begin{equation}\label{eq:10}
    \left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_i(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_i(t)+b^{[l]}_i)\right) \right\rfloor = \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_i(t).
  \end{equation}
\end{lemma}

\begin{proof}
  We have 
  \[ u^{[l]}_i(t_{Ï‰})=u^{[l]}_i(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}_i(k)+b^{[l]}_i-Ï‘s^{[l]}_i(k)\right) \]
  by~\cref{lem:non-recursive-defs}. So~\eqref{eq:10} is equal to \(0â‰¤u^{[l]}_i(t_{Ï‰})<Ï‘\) by~\cref{lem:charac-floor}.
\end{proof}

\begin{lemma}\label{lem:u-p-nonz-eqiv}
  Let \(tâˆˆ[T]\), \(lâˆˆ[L]\) and \(iâˆˆ[n_l]\). Then \(u_i^{[l]}(t)â‰¥0â‡”p_i^{[l]}(t)â‰¥0\).
\end{lemma}

\begin{proof}
  Let \(u_i^{[l]}(t)â‰¥0\). Then \(p_i^{[l]}(t)=u_i^{[l]}(t)+Ï‘s_i^{[l]}(t)â‰¥0\) by definition.

  If we know \(p_i^{[l]}(t)â‰¥0\) instead, then suppose \(u_i^{[l]}(t)<0\). Since \(p_i^{[l]}(t)â‰ u_i^{[l]}(t)\), \(s_i^{[l]}(t)=1\) and therefore \(p_i^{[l]}(t)â‰¥Ï‘\). But this means \(u_i^{[l]}(t)=p_i^{[l]}(t)-Ï‘â‰¥0\).
\end{proof}

\begin{lemma}\label{lem:potential-through-time-plus}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\{t_0+1â€¦t_{Ï‰}\}}i^{[l]}_i(t)+b^{[l]}_iâ‰¥0\) and \(u^{[l]}_i(t_0)â‰¥0\), then \(u^{[l]}_i(t_Ï‰)â‰¥0\).
\end{lemma}

\begin{proof}
  Suppose there is a \(t\) with \(t_0â‰¤tâ‰¤t_{Ï‰}\) and \(u^{[l]}_i(t)<0\). W.l.o.g.\ we can assume \(t\) to be minimal. Clearly \(tâ‰ t_0\), since this would contradict our assumptions. So we have \(u^{[l]}_i(t-1)â‰¥0\) and \(i^{[l]}_i(t)+b^{[l]}_iâ‰¥0\). So from
  \[0>u^{[l]}_i(t)=p^{[l]}_i(t)-Ï‘s^{[l]}(t)=Î²^{[l]}u^{[l]}_i(t-1)+Î²(i^{[l]}_i(t)+b^{[l]}_i)-Ï‘s^{[l]}_i(t).\]
  we conclude \(s^{[l]}_i(t)=1\) and \(p^{[l]}_i(t)â‰¥Ï‘\). But this means \(u^{[l]}_i(t)â‰¥0\) by~\cref{lem:u-p-nonz-eqiv} since \(Ï‘>0\) by definition.
\end{proof}

\begin{lemma}\label{lem:cont-bar}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}i^{[l]}_i(t)+b^{[l]}_iâ‰¤Ï‘\) and \(u^{[l]}_i(t_0-1)<Ï‘\), then
  \begin{equation}\label{eq:8}
    âˆ€_{tâˆˆ\rangeI{t_0-1}{t_{Ï‰}}}u^{[l]}_i(t)<Ï‘.
  \end{equation}
\end{lemma}

\begin{proof}
  We proof~\eqref{eq:8} by induction over \(tâˆˆ\rangeI{t_0-1}{t_{Ï‰}}\). The base case is given by assumption. Let further \(tâˆˆ\{t_0â€¦t_{Ï‰}\}\). By definition of \(p^{[l]}\), the given assumptions and the induction hypothesis we get
  \[ p^{[l]}_i(t)=Î²^{[l]}u^{[l]}_i(t-1)+i^{[l]}_i(t)+b^{[l]}_i â‰¤ u^{[l]}_i(t-1)+i^{[l]}_i(t)+b^{[l]}_i<2Ï‘  \]
  We further get \(u^{[l]}_i(t)<Ï‘\) by definition of \(u^{[l]}\) and \(s^{[l]}\).
\end{proof}


\begin{lemma}\label{lem:potential-through-time-minus}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\{t_0+1â€¦t_{Ï‰}\}}i^{[l]}_i(t)+b^{[l]}_iâ‰¤0\) and \(u^{[l]}_i(t_Ï‰)â‰¥Ï‘\), then \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}u^{[l]}_i(t)â‰¥Ï‘\).
\end{lemma}

\begin{proof}
  Suppose there is a \(t\) with \(t_0â‰¤tâ‰¤t_{Ï‰}\) and \(u^{[l]}_i(t)<Ï‘\). Let \(t\) be maximal with this property. Clearly \(tâ‰ t_{Ï‰}\) by assumption. We therefore have a contradiction by
  \[ Ï‘â‰¤u^{[l]}_i(t+1)-Î²^{[l]}(i^{[l]}_i(t+1)+b^{[l]}_i)+Ï‘s^{[l]}_i(t)=u^{[l]}_i(t). \]
\end{proof}

%DONE: i vs. j indices

\begin{lemma}\label{lem:pos-input-pos-res-pos}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). Then \(u^{[l]}_i(t_{Ï‰})â‰¥0\) if both
  \begin{equation}\label{eq:5}
    0â‰¤(Î²^{[l]})^{t_{Ï‰}-t_0}u^{[l]}_i(t_0)+\sum_{t=t_0+1}^{t_Ï‰}(Î²^{[l]})^{t_{Ï‰}-t}\left(i^{[l]}_i(t)+b^{[l]}_i\right)
  \end{equation}
  and \(âˆ€_{tâˆˆ\rangeI{t_0+1}{t_{Ï‰}}}s^{[l]}_i(t)=0\) hold or \(âˆ€_{tâˆˆ\rangeI{t'+1}{t_{Ï‰}}}i^{[l]}_i(t)+b^{[l]}_iâ‰¥0\), where \(t'\) is the maximal time \(â‰¤t_{Ï‰}\) such that \(s^{[l]}_i(t')=1\), .
\end{lemma}

\begin{proof}
  If \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}s^{[l]}_i(t)=0\), then we get by assumption
  \begin{equation*}
    u^{[l]}_i(t_{Ï‰})=u^{[l]}_i(t_{Ï‰})+\sum_{t=t_0+1}^{t_{Ï‰}}(Î²^{[l]})^{t_{Ï‰}-t}s^{[l]}_i(t)=(Î²^{[l]})^{t_{Ï‰}-t_0}u^{[l]}_i(t_0)+\sum_{t=t_0+1}^{t_Ï‰}(Î²^{[l]})^{t_{Ï‰}-t}\left(i^{[l]}_i(t)+b^{[l]}_i\right)â‰¥0.
  \end{equation*}
  Is there on the other hand a \(t'\) with \(s^{[l]}_i(t')=1\), then let \(t'\) be maximal \(â‰¤t_{Ï‰}\). By assumption we now have \(âˆ€_{tâˆˆ\{t'+1â€¦t_{Ï‰}\}}i^{[l]}_i(t)+b^{[l]}_iâ‰¥0\). Since \(s^{[l]}_i(t')=1\) we further have \(p^{[l]}_i(t')â‰¥Ï‘>0\), so by~\autoref{lem:u-p-nonz-eqiv} \(u^{[l]}_i(t')â‰¥0\) such that we can conclude by~\autoref{lem:potential-through-time-plus}.
\end{proof}
%MAYBE: remark why more general false
%DONE: Î²=1 oder nicht in Formeln nicht vergessen

The following two propositions show that a neuron in a \rdtlifsnn has an internal â€œlinearâ€ structure.

%DONE: comment about linear structure
\begin{proposition}\label{lem:sum-spikes-slowly-through-time}
  Let \(Î²=1\), \(t_0,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). Suppose \(u^{[l]}_i(t_0-1)<Ï‘\) and further \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}i^{[l]}_i(t)+b^{[l]}_iâ‰¤Ï‘\). If moreover both
  \begin{equation}
    0â‰¤u^{[l]}_i(t_0-1)+\sum_{t=t_0}^{t_Ï‰}\left(i^{[l]}_i(t)+b^{[l]}_i\right)
  \end{equation}
  and \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}s^{[l]}_i(t)=0\) or \(âˆ€_{tâˆˆ\rangeI{t'+1}{t_{Ï‰}}}i^{[l]}_i(t)+b^{[l]}_iâ‰¥0\), where \(t'\) is the last time \(â‰¤t_{Ï‰}\) such that \(s^{[l]}_i(t')=1\), then
  \begin{equation}
    \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_i(t)=\left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_i(t_{0}-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_i(t)+b^{[l]}_i\right)\right) \right\rfloor.
  \end{equation}
\end{proposition}

\begin{proof}
  It suffices to show \(0â‰¤u^{[l]}_i(t_{Ï‰})<Ï‘\) due to~\cref{lem:spike-sum}.
  We get \(0â‰¤u^{[l]}_i(t_{Ï‰})\) due to~\cref{lem:pos-input-pos-res-pos} and \(u^{[l]}_i(t_{Ï‰})<Ï‘\) due to~\cref{lem:cont-bar}.
\end{proof}

\begin{proposition}\label{lem:sum-spikes-decaying}
  Let \(Î²=1\), \(t_0,t_m,t_{Ï‰}âˆˆ[T]\) and \(iâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_mâ‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\rangeI{t_m+1}{t_{Ï‰}}}i^{[l]}_i(t)+b^{[l]}_i=0\), \(u^{[l]}_i(t_m)â‰¥0\) and
  \begin{equation}
    0â‰¤u^{[l]}_i(t_0-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_i(t)+b^{[l]}_i\right)â‰¤Ï‘(t_Ï‰-t_m+1),
  \end{equation}
  then
  \begin{equation}
    \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_i(t)=\left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_i(t_{0}-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_i(t)+b^{[l]}_i\right)\right) \right\rfloor.
  \end{equation}
\end{proposition}

\begin{proof}
  It suffices to show \(0â‰¤u^{[l]}_i(t_{Ï‰})<Ï‘\) due to~\cref{lem:spike-sum}.
  We get \(u^{[l]}_i(t_{Ï‰})â‰¥0\) from~\autoref{lem:potential-through-time-plus} using our assumptions, in particular \(u^{[l]}_i(t_m)â‰¥0\).
  Furthermore, \(u^{[l]}_i(t_{Ï‰})<Ï‘\); otherwise, if \(u^{[l]}_i(t_{Ï‰})â‰¥Ï‘\), then we have \(âˆ€_{tâˆˆ\{t_mâ€¦t_{Ï‰}\}}u^{[l]}_i(t)â‰¥Ï‘\) by~\autoref{lem:potential-through-time-minus} and in particular \(âˆ€_{tâˆˆ\{t_mâ€¦t_{Ï‰}\}}s^{[l]}_i(t)=1\). This then yields
  \begin{align*}
   u^{[l]}_i(t_{Ï‰}) &= u^{[l]}_i(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}(k)+b^{[l]}-Ï‘s^{[l]}(k)\right)\\
                   &= u^{[l]}_i(t_0-1)+\sum_{k=t_0}^{t_m}\left(i^{[l]}(k)+b^{[l]}\right)-Ï‘\sum_{k=t_0}^{t_{Ï‰}}s^{[l]}(k) \\
                 &â‰¤ Ï‘(t_Ï‰-t_m+1)-Ï‘(t_Ï‰-t_m+1)-Ï‘\sum_{k=t_0}^{t_{m}-1}s^{[l]}(k) \\
                 &â‰¤ 0,
  \end{align*}
  contradicting \(u^{[l]}_i(t_{Ï‰})â‰¥Ï‘\).
\end{proof}
