\section{Definition of \rdtlifsnn}
\label{ch:defs}

\subsection{Notations and Conventions}
First a few words about the notation we will use in the paper:

\newcommand{\rangeI}[2]{\{#1,â€¦,#2\}}
We write \(\rangeI{a}{b}â‰”\{a,a+1,â€¦,b\}\) for the range of integers from \(a\) to \(b\) and in particular \([n]â‰”\rangeI{1}{n}\) for the integers from \(1\) to \(n\) and \([n]_0â‰”\rangeI{0}{n}\) for the integers from \(0\) to \(n\). Moreover \(âˆ¨\) is used to mean the logical â€œorâ€, \(âˆ§\) to mean the logical â€œandâ€.

We write \([x,y)â‰”\{zâˆˆâ„\mid xâ‰¤z<y\}\) for the half-open interval between \(xâˆˆâ„\) and \(yâˆˆâ„\) and further use \(âŸ¦x,yâ¦†\) with \(x,yâˆˆâ„^n\) to write half-open cuboids \(\prod_{i=1}^n[x_i,y_i)\). Similarly \(âŸ¦x,yâŸ§\) is the closed cuboid \(\prod_{i=1}^n[x_i,y_i]\). A cube is a cuboid such that all sides have the same length. We further take \([x,y)\) to be empty for \(x,yâˆˆâ„\) with \(xâ‰¥y\), and \([-âˆ,x)]\) to mean \((-âˆ,x)\). This means in particular, that \(âŸ¦x,yâ¦†=âˆ…\) holds exactly when \(y_iâ‰¤x_i\) for a \(iâˆˆ[n]\). 

We further use \(\operatorname{diam}_p(U)â‰”\sup_{x,yâˆˆU}\norm{x-y}_p\) to mean the diameter of \(UâŠ‚â„^m\) regarding the \(p\)-norm \(\norm{Â·}_p\).

\newcommand{\idM}[1]{I_{#1}}
\newcommand{\zeroV}[1]{\mathbf{0}_{#1}}
\newcommand{\oneV}[1]{\mathbf{1}_{#1}}
Continuing, \(e_iâ‰”(0,â€¦,1,â€¦,0)âˆˆâ„^n\) is the \(i\)-th standard basis vector, \(\zeroV{n}â‰”(0,â€¦,0)âˆˆâ„^n\), \(\oneV{n}â‰”(1,â€¦,1)âˆˆâ„^n\) the vectors containing \(0\) and \(1\)s in every component, respectively and \(\idM{n}âˆˆâ„^{nÃ—n}\) the identity matrix of size \(nÃ—n\). Is moreover \(Wâˆˆâ„^{nÃ—m}\) a matrix, then we will write \(w_iâˆˆâ„^{1Ã—n}\) for the \(i\)-th row vector and \(W_{i,j}âˆˆâ„\) for the value of the cell at location \((i,j)\). We add an ordering \(â‰¤\) to vectors: \(xâ‰¤y\) holds exactly for \(x,yâˆˆâ„^n\) if \(âˆ€_{iâˆˆ[n]}x_iâ‰¤y_i\). Further \(x<y\) is canonically defined, so \(x<y\) holds exactly if \(xâ‰¤y\) but not \(x=y\).

Is further \(f:Uâ†’â„^n\) a function with arbitrary domain \(U\), then \(f_iâ‰”Ï€_iâˆ˜f\) is the \(i\)-th component function. \(Ï€_i:â„^nâ†’â„\) is the projection to the \(i\)-th component. We also write \(f^{-1}\) for the inverse of \(f\) and \(f^{-1}(W)â‰”\{xâˆˆU \mid f(x)âˆˆW\}\) for the preimage of \(W\) under \(f\). The set of continuous functions of type \(Uâ†’W\) will be written by \(ğ’^0(U,W)\). Correspondingly, the set of \(k\)-times continuously differentiable functions will be written \(ğ’^k(U,W)\). The total derivative of a differentiable function \(f:Uâ†’W\) is written as \(df:Uâ†’\operatorname{Hom}_{â„}(W,W)\).

We moreover write \(\max{U}â‰”(\max_{uâˆˆU}u_i)_{iâˆˆ[n]}\) with \(UâŠ‚(â„âˆª\{Â±âˆ\})^n\) for the maximum by component of \(U\), we similarly define \(\min{U}\), \(\inf{U}\) and \(\sup{U}\). In addition we use the conventions \(\inf(âˆ…)=âˆ\) and \(\sup(âˆ…)=-âˆ\).

%DONE: f^-1 for the inverted function, f^-1(U) for preimage
%DONE: define < for vectors (convention)

%DONE: notation for total der.

%TODO: introduce notation []_â€¦^â€¦ as product
% TODO: notation \(\norm{f}_{âˆ,2}\), \(\norm{g}_2\) fÃ¼r \(g\) linear

\newcommand{\charac}[1]{Ï‡_{#1}}
\newcommand{\oneP}[1]{1_{#1}}
We finally write
\[\charac{M}(x)â‰”\begin{cases} 1 & xâˆˆM \\ 0 & xâˆ‰M \\ \end{cases}\] %CANCELLED: change notation? e.g. 1_M instead?
for the characteristic function of a set \(M\) and
\[\oneP{A}â‰”\begin{cases} 1 & A \\ 0 & Â¬A \\ \end{cases}\] %CANCELLED: change notation? e.g. âŸ¦AâŸ§ instead
for a formula \(A\), such that in particular \(\charac{M}(x)=\oneP{(xâˆˆM)}\).

\subsection{Motivation}
A neuron in the human brain has connections to other neurons, which may be weaker or stronger. A neuron further has an internal membrane potential that is increased or decreased by the input from other neurons. If that membrane potential reaches a certain point, the neuron will spike and the connected neurons will receive an update. This update is delivered by chemical transmitters or electric signals. Further this signal won't just be a binary spike, but will decay over time after the spike, e.g. because the chemical transmitters have some chance of taking a longer path to the next neuron.
The membrane potential may also decay over time.

From these biologic observations we can derive the following differential equations for a single neuron input \(I:â„â†’â„\) and membrane potential \(U:â„â†’â„\) (compare e.g.~\cite{neuronaldynamics2014}):
%TODO: work in {neuronaldynamics2014} and figure

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{src/figures/ch02-structure-of-neuron.png}
  \caption{Structure of a neuron.\protect\footnotemark}
  \label{fig:ch02-structure-of-neuron}
\end{figure}
\footnotetext{Source: By BruceBlaus - Own work, CC BY 3.0, \url{https://commons.wikimedia.org/w/index.php?curid=28761830 }}

% DONE: graphic, https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png ?
% DONE: link paper for equations
\begin{align*}
 I'(t) &â‰”-Ï„_Î±I(t)+\sum_{j=1}^nw_js_j(t-Î”t) \\
 U'(t) &â‰”-Ï„_Î²U(t)+I(t)+b-Ï‘s(t)
\end{align*}
Here \(s_j:â„â†’\{0,1\}\) represents the \(j\)-th connection of the given neuron spiking at time \(t\); the variables \(w_jâˆˆâ„\) represent the weights of the connections. Since we might get unresolvable dependencies between neurons otherwise, the connections need to have some latency \(Î”tâˆˆâ„\). The variables \(Ï„_{Î±}\), \(Ï„_{Î²}\) specify the decay rate.

Since we are in an analog scenario, it is quite reasonable to assume that \(I\) and \(U\) describe smoothly differentiable functions. We can therefore use the first-order exponential integrator method to obtain a discretization of \(I\) and \(U\) from the differential equations. Let \(t_0,hâˆˆâ„\) be arbitrary and \(t_{n+1}â‰”t_n+h\). By using the fundamental theorem of calculus we get
\begin{align*}
  &e^{Ï„_{Î±}t_{n+1}}I(t_{n+1})-e^{Ï„_{Î±}t_n}I(t_n) \\
  &=\int_{t_n}^{t_{n+1}}\frac{d}{dt}\left(e^{Ï„_{Î±}t}I(t)\right)dt \\
  &=\int_{t_n}^{t_{n+1}}e^{Ï„_{Î±}t}(I'(t)+Ï„_Î±I(t))dt \\
  &=\int_{t_n}^{t_{n+1}}e^{Ï„_{Î±}t}\left(\sum_{j=1}^nw_js_j(t-Î”t)\right)dt
\end{align*}
If we assume the input from the spikes of other neurons to be constant during \([t_n,t_{n+1}]\), we get
\begin{align*}
 &=\frac{1}{Ï„_{Î±}}(e^{Ï„_{Î±}t_{n+1}}-e^{Ï„_{Î±}t_n})\sum_{j=1}^nw_js_j(t-Î”t) \\
 &=\frac{1}{Ï„_{Î±}}e^{Ï„_{Î±}t_{n+1}}(1-e^{Ï„_{Î±}h})\sum_{j=1}^nw_js_j(t-Î”t)
\end{align*}
So we get
\[ I(t_{n+1})=e^{-Ï„_{Î±}h}I(t_n)+\frac{1}{Ï„_{Î±}}(1-e^{Ï„_{Î±}h})\sum_{j=1}^nw_js_j(t-Î”t) \]
Let us now use \(h=1\) and absorb \(\frac{1-e^{Ï„_{Î±}}}{Ï„_{Î±}}\) into the weights \((w_j)_{jâˆˆ[n]}\), such that we have
\begin{equation}\label{eq:9}
I(t_{n+1})=Î±I(t_n)+\sum_{j=1}^nw_js_j(t-Î”t)
\end{equation}
by using \(Î±â‰”e^{-Ï„_{Î±}}\). We similarly obtain
\begin{equation}\label{eq:11}
U(t_{n+1})=Î²U(t_n)+I(t)+b-Ï‘s(t)
\end{equation}
by using the first-order exponential integrator method, defining \(Î²â‰”e^{Ï„_{Î²}}\) and absorbing \(\frac{1-Î²}{Ï„_{Î²}}\) into \(I(0)\), \((w_j)_{jâˆˆ[n]}\), \(b\) and \(Ï‘\).

\begin{figure}[h]
  \centering
  \input{src/figures/ch02-network-layout.tex}
  \caption{High-level network layout}
  \label{fig:network-layout}
\end{figure}

We will now arrange those neurons into layers like shown in~\cref{fig:network-layout}, such that neurons are only connected to neurons from the previous layer or their own layer. Of course, since we allow recurrent connections we can always just merge all layers into the first one, but the layers do give us more control over the network.
For connections between different layers, we can use \(Î”t=0\) in~\eqref{eq:9} since there can be no interdependent connections between neurons in different layers. For connections between neurons in the same layer we use \(Î”t>0\) to prevent those interdependencies. Since we have previously chosen \(h=1\), it is natural to further choose \(Î”t=1\).

%TODO: define â€spike trainâ€œ
Since we want to work with arbitrary data and not just spike trains, we further need to encode our data to spikes trains and decode it from spike trains. Like~\cite{nguyen2025timespikeunderstandingrepresentational} we choose a simple direct encoding and membrane potential outputs.
% TODO: why direct encoding, membrane potential output

\subsection{Definitions}
% DONE: graphic for whole network
Our type of SNN should be thought of as a composition of an initial input layer, a number of hidden spiking layers with internal state and an affine-linear layer mapping spikes activations over time, so called spike trains, to the value of the output layer.

We first define the structure of the hidden layers:

% CANCELLED: graphic for single neuron/single layer
\begin{definition}
  The \textbf{input vector} \(i^{[l]}(t)âˆˆ\{0,1\}^{n_l}\), the \textbf{spike vector} \(s^{[l]}(t)âˆˆ\{0,1\}^{n_l}\), the \textbf{pre-spike membrane potential vector} \(p^{[l]}(t)\) and the \textbf{post-spike membrane potential vector} \(u^{[l]}(t)\) of a hidden layer \(Î»=(W^{[l]},b^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\) with index \(lâˆˆ[L]\), are recursively defined as
  \begin{align}
    i^{[l]}(t) & â‰” Î±^{[l]}i^{[l]}(t-1)+W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1), \\
    p^{[l]}(t) & â‰” Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}, \\
    s^{[l]}(t) & â‰” H(p^{[l]}(t)-Ï‘\oneV{n_l}), \\
    u^{[l]}(t) & â‰” p^{[l]}(t)-Ï‘s^{[l]}(t),
  \end{align}
  with \(âˆ€_{lâˆˆ[L]}s^{[l]}(0)=0\) and given
  \begin{enumerate}
    \item[â€¢] \textbf{initial membrane potential}: \(u^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{initial input}: \(i^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{weight matrices}: \(W^{[l]}âˆˆâ„^{n_lÃ—n_{l-1}}\), \(V^{[l]}âˆˆâ„^{n_lÃ—n_l}\),
    \item[â€¢] \textbf{bias vectors}: \(b^{[l]}âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{leaky terms}: \(Î±^{[l]},Î²^{[l]}âˆˆ[0,1]\),
    \item[â€¢] \textbf{threshold}: \(Ï‘^{[l]}âˆˆ(0,âˆ)\),
  \end{enumerate}
  where \(Hâ‰”ğŸ™_{[0,âˆ)}\) is a step function, \(Tâˆˆâ„•\) is the number of \textbf{simulated time steps} and \(Lâˆˆâ„•\) the total \textbf{number of hidden layers}.
\end{definition}

\begin{remark}
  While it is suppressed in the notation, \(i^{[l]}\), \(p^{[l]}\), \(s^{[l]}\) and \(u^{[l]}\) clearly not only depend on \(t\), but by recursion on \(s^{[0]}\).

  Moreover we will also use \(s^{[l]}\) as if it was an element of \(\{0,1\}^{n_lÃ—T}\).
\end{remark}

We further define recurrent d.t. LIF-SNN and the function the network realizes:
\begin{definition}
  A \textbf{recurrent discrete-time LIF-SNN}, also called \rdtlifsnn, of \textbf{depth} \(L\) with \textbf{layer-widths} \((n_0,â€¦,n_{L+1})\) and \(Tâˆˆâ„•\) time-steps is given by
  \[ Î¦â‰”((W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})_{lâˆˆ[L]},T,(E,D)) \]
  where the \textbf{input encoder} \(E:â„^{n_0}â†’â„^{n_0Ã—T}\) maps a vector \(xâˆˆâ„^{n_0}\) to a corresponding first layer spike activation \(âˆ€_{tâˆˆ[T]}s^{[0]}(t)=E(x)(t)\) and the \textbf{output decoder} \(D:\{0,1\}^{n_LÃ—T}â†’â„^{n_{L+1}}\) maps the spike activations of the last hidden layer to real values.
\end{definition}
% DONE: remark about spike trains being equiv. to \{0,1\}^â€¦
% DONE: replace â€recursive SNNâ€œ etc. by â€œrecurrent SNNâ€

% DONE: remark about layers being kinda useless
% DONE: remark comparison with previous model
% CANCELLED: layers mit kursiv?

\begin{definition}
  A recurrent discrete-time LIF-SNN \(Ï•\) \textbf{realizes} the function \(R(Î¦):â„^{n_0}â†’â„^{n_{L+1}}\):
  \[ R(Î¦)(x)=D((s^{[L]}(t))_{tâˆˆ[T]})\quad \text{with }s^{[0]}â‰”E(x)\]
\end{definition}

\begin{definition}
  A recurrent discrete-time LIF-SNN employs \textbf{direct encoding} if we have
  \[ âˆ€_{tâˆˆ[T]}E(x)(t)=x \]
  for the input encoder and has \textbf{membrane potential outputs} if the output decoder can be written as
  \[ D((s(t))_{tâˆˆ[T]})=\sum_{t=1}^Ta_t(W^{[L+1]}s^{[L]}(t)+b^{[L+1]}) \]
  for some \((a_t)_{tâˆˆ[T]}âˆˆâ„^T\), \(b^{[L+1]}âˆˆâ„^{n_{L+1}}\) and \(W^{[L+1]}âˆˆâ„^{n_{L+1}Ã—n_L}\).
\end{definition}
\begin{remark}
  We will only consider recurrent discrete-time LIF-SNN with direct encoding and membrane potential outputs. In fact, we will use â€œrecurrent discrete-time LIF-SNNâ€ to mean â€œ\rdtlifsnn with direct encoding and membrane potentialâ€.
\end{remark}

\begin{remark}
  Our definition of \rdtlifsnn breaks now down to the definition \dtlifsnn of~\cite{nguyen2025timespikeunderstandingrepresentational} if we require \(Î±^{[l]}â‰”0\) and \(V^{[l]}\) for all layers \(lâˆˆ[L]\). So \rdtlifsnns can have recursive dependencies inside of the layers and decaying input vectors in contrast to the more simpler \dtlifsnns.
\end{remark}

Let us now take a look at some simple examples of \rdtlifsnn:

\begin{example}
  Let \(T,Lâˆˆâ„•\). Then there exists a \rdtlifsnn with \(âˆ€_{tâˆˆ[T]}s^{[L]}(t)=s^{[0]}(t)\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\).

  Let us use constant width \(n_l=n\), weights \(W^{[l]}=\idM{n}\), \(V^{[l]}=\zeroV{nÃ—n}\), biases \(b^{[l]}=0\), initial input \(i^{[l]}(0)=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky terms  \(Î±^{[l]}=Î²^{[l]}=0\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  It follows from the definitions that \(i^{[l]}(t)=s^{[l-1]}(t)\) and therefore further
  \[ s^{[l]}(t) = H(s^{[l-1]}(t)-\oneV{n_l}) = s^{[l-1]}(t). \]
  So by induction we indeed get \(âˆ€_{tâˆˆ[T]}s^{[L]}(t)=s^{[0]}(t)\).
\end{example}

\begin{example}
  Let \(T,Lâˆˆâ„•\). Then there exists a \rdtlifsnn with \(âˆ€_{tâˆˆT,iâˆˆ[n]}s^{[L]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t-1]}(s^{[0]}_i(t'))=1\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\), i.e. an output neuron switches on once the corresponding input neurons fires.

  Let us use constant width \(n_l=n\), weights \(W^{[l]}=\idM{n}\), \(V^{[l]}=\idM{n}\), biases \(b^{[l]}=0\), initial input \(i^{[l]}(0)=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky terms \(Î±^{[l]}=0\), \(Î²^{[l]}=0\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  We then get by definition
  \[i^{[l]}(t)=s^{[l-1]}(t)+s^{[l]}(t-1)\]
  and therefore
  \[ s^{[l]}(t) = H(s^{[l-1]}(t)+s^{[l]}(t-1)-\oneV{n_l}). \]
  So if and only if \(s^{[l-1]}_i(t)=1\) or \(s^{[l]}_i(t-1)=1\), then \(s^{[l]}_i(t)=1\). We therefore have \(âˆ€_{tâˆˆT}s^{[l]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t]}(s^{[l-1]}_i(t'))=1\) for all \(iâˆˆ[n],lâˆˆ[L]\). Thus \(âˆ€_{tâˆˆT,iâˆˆ[n]}s^{[L]}_i(t)=1â‡”âˆƒ_{t'âˆˆ[t-1]}(s^{[0]}_i(t'))=1\) by induction.

  There also is a \dtlifsnn construction achieving the same behavior: We use the same parameters as before, but with \(V^{[l]}=\zeroV{nÃ—n}\), \(W^{[l]}=TÂ·\idM{n}\) and \(Î²^{[l]}=1\) for all layers \(lâˆˆ[L]\).

  We then get by definition
  \[i^{[l]}(t)=Ts^{[l-1]}(t)\]
  and therefore
  \begin{alignat*}{2}
    s^{[l]}(t) & = H(&&p^{[l]}(t)-\oneV{n_l}) \\
    p^{[l]}(t) & =   &&p^{[l]}(t-1)+Ts^{[l-1]}(t)-s^{[l]}(t-1)
  \end{alignat*}
  By induction over \(t\) (see also~\cref{lem:non-recursive-defs}) we obtain 
  \[ p^{[l]}(t) = \sum_{k=1}^t\left(Ts^{[l-1]}(k)-s^{[l]}(k-1)\right) \]
  Let now \(iâˆˆ[n]\). Since \(s^{[l]}(0)=0\), we have \(\sum_{k=1}^ts^{[l]}_i(k-1)â‰¤t-1\). Now if and only if there is any \(t_0âˆˆ[T]\) with \(s^{[l-1]}_i(t_0)=1\), we get
  \[p_i^{[l]}(t')=T\sum_{k=1}^{t'}s^{[l-1]}_i(k)-\sum_{k=1}^{t'}s^{[l]}_i(k-1)â‰¥1\]
  for \(t'â‰¥t_0\) and hence \(s_i^{[l]}(t')=1\).
  Just as before we now get the required property for the whole network by induction over the layers.
\end{example}

For a clearer construction of our networks we will additionally define neurons in our network:

\begin{definition}
  The \textbf{\(i\)-th neuron} of a hidden layer \(Î»=(W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\) of a \rdtlifsnn is a tuple \((w,b,v,u_0,i_0)\) with \(wâˆˆâ„^{n_{l-1}}\), \(vâˆˆâ„^{n_l}\) and \(b,u_0,i_0âˆˆâ„\), such that \(b\), \(u_0\), \(i_0\) are the \(i\)-th component of \(b^{[l]}\), \(u^{[l]}(0)\), \(i^{[l]}(0)\) respectively and \(w\), \(v\) are the \(i\)-th row vector of \(W^{[l]}\), \(V^{[l]}\) respectively.
\end{definition}

\subsection{Basic properties}

In the following we present some technical but helpful notations and lemmas for writing proofs about SNN. Of particular importance are \cref{def:non-recursive-defs} and \cref{lem:non-recursive-defs}, which introduce non-recursive formulas for the defining equations of \rdtlifsnns. 
%DONE: fix references to lem:non-recursive-defs

\begin{definition}\label{def:non-recursive-defs}
  Let \(tâˆˆ[T]\), \(lâˆˆ[L]\) and spike train families \(Ïƒ=(Ïƒ^{[l']})_{l'âˆˆ[l]_0}\), \(Ïƒ'=(Ïƒ'^{[l']})_{l'âˆˆ[l]_0}\) be given, such that \(âˆ€_{lâˆˆ[l-1]_0}Ïƒ^{[l']}âˆˆ\{0,1\}^{n_{l'}Ã—t}\) and \(Ïƒ^{[l]}âˆˆ\{0,1\}^{n_lÃ—t}\) as well as \(âˆ€_{l'âˆˆ[l]_0}Ïƒ'^{[l']}âˆˆ\{0,1\}^{n_{l'}Ã—t}\), i.e. \(Ïƒ,Ïƒ'\) have to be chosen such that the terms in the following definitions are well-defined. We define
  \begin{align}
   i^{[l]}(t;Ïƒ)&â‰”(Î±^{[l]})^ti^{[l]}(0)+\sum_{k=1}^t(Î±^{[l]})^{t-k}\left(W^{[l]}Ïƒ^{[l-1]}(k)+V^{[l]}Ïƒ^{[l]}(k-1)\right), \\
   p^{[l]}(t;Ïƒ) &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ)+b^{[l]}\right)-Ï‘\sum_{k=1}^{t-1}(Î²^{[l]})^{t-k}Ïƒ^{[l]}(k),\\
   s^{[l]}(t;Ïƒ) & â‰” H(p^{[l]}(t;Ïƒ)-Ï‘\oneV{n_l}), \\
   u^{[l]}(t;Ïƒ') &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(k)\right).
  \end{align}
\end{definition}

\begin{remark}
  The spike train families \(Ïƒ\), \(Ïƒ'\) in \cref{def:non-recursive-defs} are chosen such that they only include the data that is actually needed in the definition of \(i^{[l]},p^{[l]},s^{[l]},u^{[l]}\). This is also the reason why we have to use \(Ïƒ'\) for \(u^{[l]}\); its definition requires in contrast to the definitions of \(i^{[l]},p^{[l]},s^{[l]}\) the newest spikes from the current layer layer.

  We will also allow using \(i^{[l]},p^{[l]},s^{[l]},u^{[l]}\) with extensions of the by the definition necessary spike train families, in particular we will use the functions with â€œcompleteâ€ spike train families \((Ïƒ^{[l]})_{lâˆˆ[L]}\) with \(Ïƒ^{[l]}âˆˆ\{0,1\}^{n_lÃ—T}\).
\end{remark}

The notation for the previous definitions is justified due to

%TODO: â€œLet \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\) be arbitrary.â€ ?
\begin{lemma}\label{lem:non-recursive-defs} %TODO: introduce notation for s, Ïƒ, s'
  The non-recursive formulas from~\cref{def:non-recursive-defs} are equivalent to the recursive definitions for \(lâˆˆ[L]\), \(tâˆˆ[T]\) assuming previous spikes are equal: \(âˆ€_{l'âˆˆ[l-1]_0}Ïƒ^{[l']}=s^{[l']}\) and \(âˆ€_{t'âˆˆ[t-1]}Ïƒ^{[l]}(t')=s^{[l]}(t')\) as well as \(âˆ€_{l'âˆˆ[l]_0}Ïƒ'^{[l']}=s^{[l']}\).
\end{lemma}

%TODO: introduce convention regarding spike trains function or vector/matrix
\begin{proof}
  We first proof \(âˆ€_{tâˆˆ[T]}i^{[l]}(t;Ïƒ)=i^{[l]}(t)\) and \(âˆ€_{tâˆˆ[T]}u^{[l]}(t;Ïƒ)=u^{[l]}(t)\) for \(Ïƒ^{[l]}=s^{[l]}\) by induction: Let \(t=1\). We have
  \begin{align*}
   i^{[l]}(1;Ïƒ)&=Î±^{[l]}i^{[l]}(0)+W^{[l]}Ïƒ^{[l-1]}(1)+V^{[l]}Ïƒ^{[l]}(0) \\
                         &=Î±^{[l]}i^{[l]}(0)+W^{[l]}s^{[l-1]}(1)+V^{[l]}s^{[l]}(0) \\
                         &=i^{[l]}(1)
  \end{align*}
  and
  \begin{align*}
   u^{[l]}(1;Ïƒ') &= Î²^{[l]}u^{[l]}(0)+i^{[l]}(1;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(1) \\
                            &= Î²^{[l]}u^{[l]}(0)+i^{[l]}(1)+b^{[l]}-Ï‘s^{[l]}(1) \\
                            &= p^{[l]}(1)-Ï‘s^{[l]}(1) \\
                            &= u^{[l]}(1)
  \end{align*}
  by using the definitions and using our assumption \(Ïƒ^{[l]}=s^{[l]}\). We have \(i^{[l]}(1;Ïƒ')=i^{[l]}(1)\), since \(Ïƒ'\) is an â€œextensionâ€ of \(Ïƒ\).

  Let further \(t>1\). We may assume \(i^{[l]}(t-1;Ïƒ)=i^{[l]}(t-1)\) and \(u^{[l]}(t-1;Ïƒ')=u^{[l]}(t-1)\) and obtain
  \begin{align*}
   i^{[l]}(t;Ïƒ)&=(Î±^{[l]})^ti^{[l]}(0)+\sum_{k=1}^t(Î±^{[l]})^{t-k}\left(W^{[l]}Ïƒ^{[l-1]}(k)+V^{[l]}Ïƒ^{[l]}(k-1)\right) \\
                           &=Î±^{[l]}i^{[l]}(t-1;Ïƒ)+\left(W^{[l]}Ïƒ^{[l-1]}(t)+V^{[l]}Ïƒ^{[l]}(t-1)\right) \\
                           &=Î±^{[l]}i^{[l]}(t-1)+W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1) \\
                           &=i^{[l]}(t)
  \end{align*}
  and similarly
  \begin{align*}
   u^{[l]}(t;Ïƒ') &= (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(k)\right) \\
                            &= Î²^{[l]}u^{[l]}(t-1;Ïƒ')+\left(i^{[l]}(t;Ïƒ')+b^{[l]}-Ï‘Ïƒ'^{[l]}(t)\right) \\
                            &= Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}-Ï‘s^{[l]}(t) \\
                            &= p^{[l]}(t)-Ï‘s^{[l]}(t) \\
                            &= u^{[l]}(t).
  \end{align*}
  By substituting \(u^{[l]}\) in \(p^{[l]}\) we further obtain
  \begin{align*}
   p^{[l]}(t;Ïƒ) &= (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;Ïƒ)+b^{[l]}\right)-Ï‘\sum_{k=1}^{t-1}(Î²^{[l]})^{t-k}Ïƒ^{[l]}(k),\\
                            &= Î²^{[l]}u^{[l]}(t-1;Ïƒ)+\left(i^{[l]}(t;Ïƒ)+b^{[l]}\right)\\
                            &= Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}\\
                            &= p^{[l]}(t).
  \end{align*}
  By using the above we obtain
  \[ s^{[l]}(t;Ïƒ) = H(p^{[l]}(t;Ïƒ)-Ï‘\oneV{n_l})=H(p^{[l]}(t)-Ï‘\oneV{n_l})=s^{[l]}(t). \]
\end{proof}

\begin{lemma}\label{lem:charac-floor}
  Let \(a,xâˆˆâ„\), \(aâ‰ 0\) and \(bâˆˆâ„¤\). We then have \(\lfloor\tfrac{x}{a}\rfloor=b â‡” 0â‰¤x-ab<a\).
\end{lemma}

\begin{proof}
  \(0â‰¤x-ab<a\) is equivalent to \(bâ‰¤\tfrac{x}{a}<b+1\), which is yet again equivalent to \(\lfloor\tfrac{x}{a} \rfloor=b\) by definition of \(\lfloor Â· \rfloor\).
\end{proof}

\begin{lemma}\label{lem:spike-sum}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(Î²=1\), then \(0â‰¤u^{[l]}_j(t_{Ï‰})<Ï‘\) holds if and only if
  \begin{equation}\label{eq:10}
    \left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j)\right) \right\rfloor = \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t).
  \end{equation}
\end{lemma}

\begin{proof}
  We have 
  \[ u^{[l]}_j(t_{Ï‰})=u^{[l]}_j(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}_j(k)+b^{[l]}_j-Ï‘s^{[l]}_j(k)\right) \]
  by~\cref{lem:non-recursive-defs}. So~\eqref{eq:10} is equal to \(0â‰¤u^{[l]}_j(t_{Ï‰})<Ï‘\) by~\cref{lem:charac-floor}.
\end{proof}

\begin{lemma}\label{lem:u-p-nonz-eqiv}
  Let \(tâˆˆ[T]\), \(lâˆˆ[L]\) and \(jâˆˆ[n_l]\). We then have \(u_j^{[l]}(t)â‰¥0â‡”p_j^{[l]}(t)â‰¥0\).
\end{lemma}

\begin{proof}
  Let \(u_j^{[l]}(t)â‰¥0\). Then \(p_j^{[l]}(t)=u_j^{[l]}(t)+Ï‘s_j^{[l]}(t)â‰¥0\).

  If we know \(p_j^{[l]}(t)â‰¥0\) instead, then suppose \(u_j^{[l]}(t)<0\). Since \(p_j^{[l]}(t)â‰ u_j^{[l]}(t)\), \(s_j^{[l]}(t)=1\) and therefore \(p_j^{[l]}(t)â‰¥Ï‘\). But this means \(u_j^{[l]}(t)=p_j^{[l]}(t)-Ï‘â‰¥0\).
\end{proof}

\begin{lemma}\label{lem:potential-through-time-plus}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\{t_0+1â€¦t_{Ï‰}\}}i^{[l]}_j(t)+b^{[l]}_jâ‰¥0\) and \(u^{[l]}_j(t_0)â‰¥0\), then \(u^{[l]}_j(t_Ï‰)â‰¥0\).
\end{lemma}

\begin{proof}
  Suppose there is a \(t\), \(t_0â‰¤tâ‰¤t_{Ï‰}\) with \(u^{[l]}_j(t)<0\). W.l.o.g.\ we can assume \(t\) to be minimal. Clearly \(tâ‰ t_0\), since this contradicts our assumption. So we have \(u^{[l]}_j(t-1)â‰¥0\) and \(i^{[l]}_j(t)+b^{[l]}_jâ‰¥0\). So from
  \[0>u^{[l]}_j(t)=p^{[l]}_j(t)-Ï‘s^{[l]}(t)=Î²^{[l]}u^{[l]}_j(t-1)+Î²(i^{[l]}_j(t)+b^{[l]}_j)-Ï‘s^{[l]}_j(t).\]
  we conclude \(s^{[l]}_j(t)=1\) and \(p^{[l]}_j(t)â‰¥Ï‘\). But this means \(u^{[l]}_j(t)â‰¥0\) by~\cref{lem:u-p-nonz-eqiv}.
\end{proof}

\begin{lemma}\label{lem:cont-bar}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}i^{[l]}_j(t)+b^{[l]}_jâ‰¤Ï‘\) and \(u^{[l]}_j(t_0-1)<Ï‘\), then
  \begin{equation}\label{eq:8}
    âˆ€_{tâˆˆ\rangeI{t_0-1}{t_{Ï‰}}}u^{[l]}_j(t)<Ï‘.
  \end{equation}
\end{lemma}

\begin{proof}
  We proof~\eqref{eq:8} by induction over \(tâˆˆ\rangeI{t_0-1}{t_{Ï‰}}\). The base case is given by assumption. Let further \(tâˆˆ\{t_0â€¦t_{Ï‰}\}\). By definition of \(p^{[l]}\), the given assumptions and the induction hypothesis we get
  \[ p^{[l]}_j(t)=Î²^{[l]}u^{[l]}_j(t-1)+i^{[l]}_j(t)+b^{[l]}_j â‰¤ u^{[l]}_j(t-1)+i^{[l]}_j(t)+b^{[l]}_j<2Ï‘  \]
  We further get \(u^{[l]}_j(t)<Ï‘\) by definition of \(u^{[l]}\) and \(s^{[l]}\).
\end{proof}


\begin{lemma}\label{lem:potential-through-time-minus}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\{t_0+1â€¦t_{Ï‰}\}}i^{[l]}_j(t)+b^{[l]}_jâ‰¤0\) and \(u^{[l]}_j(t_Ï‰)â‰¥Ï‘\), then \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}u^{[l]}_j(t)â‰¥Ï‘\).
\end{lemma}

\begin{proof}
  Suppose there is a \(t\), \(t_0â‰¤tâ‰¤t_{Ï‰}\), with \(u^{[l]}_j(t)<Ï‘\). Let \(t\) be maximal with this property. Clearly \(tâ‰ t_{Ï‰}\) by assumption. So we have a contradiction by
  \[ Ï‘â‰¤u^{[l]}_j(t+1)-Î²^{[l]}(i^{[l]}_j(t+1)+b^{[l]}_j)+Ï‘s^{[l]}_j(t)=u^{[l]}_j(t). \]
\end{proof}

%TODO: i vs. j indices

\begin{lemma}\label{lem:pos-input-pos-res-pos}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). If
  \[0â‰¤(Î²^{[l]})^{t_{Ï‰}-t_0}u^{[l]}_j(t_0)+\sum_{t=t_0+1}^{t_Ï‰}(Î²^{[l]})^{t_{Ï‰}-t}\left(i^{[l]}_j(t)+b^{[l]}_j\right)\]
  and either \(âˆ€_{tâˆˆ\rangeI{t_0+1}{t_{Ï‰}}}s^{[l]}_j(t)=0\) or \(âˆ€_{tâˆˆ\rangeI{t'+1}{t_{Ï‰}}}i^{[l]}_j(t)+b^{[l]}_jâ‰¥0\), where \(t'\) is the maximal time \(â‰¤t_{Ï‰}\) such that \(s^{[l]}_j(t')=1\), then \(u^{[l]}_j(t_{Ï‰})â‰¥0\).
\end{lemma}

\begin{proof}
  If \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}s^{[l]}_j(t)=0\), we get by assumption
  \begin{equation}\label{eq:5}
    u^{[l]}_j(t_{Ï‰})=u^{[l]}_j(t_{Ï‰})+\sum_{t=t_0+1}^{t_{Ï‰}}(Î²^{[l]})^{t_{Ï‰}-t}s^{[l]}_j(t)=(Î²^{[l]})^{t_{Ï‰}-t_0}u^{[l]}_j(t_0)+\sum_{t=t_0+1}^{t_Ï‰}(Î²^{[l]})^{t_{Ï‰}-t}\left(i^{[l]}_j(t)+b^{[l]}_j\right)â‰¥0.
  \end{equation}
  Is there on the other hand a \(t'\) with \(s^{[l]}_j(t')=1\), then let \(t'\) be maximal \(â‰¤t_{Ï‰}\). By assumption we now have \(âˆ€_{tâˆˆ\{t'+1â€¦t_{Ï‰}\}}i^{[l]}_j(t)+b^{[l]}_jâ‰¥0\). Since \(s^{[l]}_j(t')=1\) we further have \(p^{[l]}_j(t')â‰¥Ï‘\), so by~\autoref{lem:u-p-nonz-eqiv} \(u^{[l]}_j(t')â‰¥0\) such that we can conclude by~\autoref{lem:potential-through-time-plus}.
\end{proof}
%TODO: remark why more general false
%DONE: Î²=1 oder nicht in Formeln nicht vergessen

\begin{remark}
  One might think that~\eqref{eq:5} should suffice as an assumption, since%TODO
\end{remark}

%TODO: comment about linear structure
\begin{proposition}\label{lem:sum-spikes-slowly-through-time}
  Let \(Î²=1\), \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_{Ï‰}\). Suppose \(u^{[l]}_j(t_0-1)<Ï‘\) and further \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}i^{[l]}_j(t)+b^{[l]}_jâ‰¤Ï‘\) and \(0â‰¤u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_Ï‰}\left(i^{[l]}_j(t)+b^{[l]}_j\right)\). If moreover either \(âˆ€_{tâˆˆ\rangeI{t_0}{t_{Ï‰}}}s^{[l]}_j(t)=0\) or \(âˆ€_{tâˆˆ\rangeI{t'+1}{t_{Ï‰}}}i^{[l]}_j(t)+b^{[l]}_jâ‰¥0\), where \(t'\) is the last time \(â‰¤t_{Ï‰}\) such that \(s^{[l]}_j(t')=1\), then
  \[ \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t)=\left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_j(t_{0}-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_j(t)+b^{[l]}_j\right)\right) \right\rfloor. \]
\end{proposition}

\begin{proof}
  It suffices to show \(0â‰¤u^{[l]}_j(t_{Ï‰})<Ï‘\) due to~\cref{lem:spike-sum}.
  We get \(0â‰¤u^{[l]}_j(t_{Ï‰})\) due to~\cref{lem:pos-input-pos-res-pos} and \(u^{[l]}_j(t_{Ï‰})<Ï‘\) due to~\cref{lem:cont-bar}.

\end{proof}

\begin{proposition}\label{lem:sum-spikes-decaying}
  Let \(Î²=1\), \(t_0,t_m,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) for an \(lâˆˆ[L]\) such that \(t_0â‰¤t_mâ‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\rangeI{t_m+1}{t_{Ï‰}}}i^{[l]}_j(t)+b^{[l]}_j=0\), \(u^{[l]}_j(t_m)â‰¥0\) and \(0â‰¤u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_j(t)+b^{[l]}_j\right)â‰¤Ï‘(t_Ï‰-t_m+1)\), then
  \[ \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t)=\left\lfloor \frac{1}{Ï‘}\left(u^{[l]}_j(t_{0}-1)+\sum_{t=t_0}^{t_m}\left(i^{[l]}_j(t)+b^{[l]}_j\right)\right) \right\rfloor. \]
\end{proposition}

\begin{proof}
  It suffices to show \(0â‰¤u^{[l]}_j(t_{Ï‰})<Ï‘\) due to~\cref{lem:spike-sum}.
  We get \(u^{[l]}_j(t_{Ï‰})â‰¥0\) from~\autoref{lem:potential-through-time-plus} using our assumptions, in particular \(u^{[l]}_j(t_m)â‰¥0\).
  Furthermore, \(u^{[l]}_j(t_{Ï‰})<Ï‘\); otherwise, if \(u^{[l]}_j(t_{Ï‰})â‰¥Ï‘\), then we had \(âˆ€_{tâˆˆ\{t_mâ€¦t_{Ï‰}\}}u^{[l]}_j(t)â‰¥Ï‘\) by~\autoref{lem:potential-through-time-minus} and therefore in particular \(âˆ€_{tâˆˆ\{t_mâ€¦t_{Ï‰}\}}s^{[l]}_j(t)=1\), from which we get
  \begin{align*}
   u^{[l]}_j(t_{Ï‰}) &= u^{[l]}_j(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}(k)+b^{[l]}-Ï‘s^{[l]}(k)\right)\\
                   &= u^{[l]}_j(t_0-1)+\sum_{k=t_0}^{t_m}\left(i^{[l]}(k)+b^{[l]}\right)-Ï‘\sum_{k=t_0}^{t_{Ï‰}}s^{[l]}(k) \\
                 &â‰¤ Ï‘(t_Ï‰-t_m+1)-Ï‘(t_Ï‰-t_m+1)-Ï‘\sum_{k=t_0}^{t_{m}-1}s^{[l]}(k) \\
                 &â‰¤ 0
  \end{align*}
  contradicting \(u^{[l]}_j(t_{Ï‰})â‰¥Ï‘\).
\end{proof}
