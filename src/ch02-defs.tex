\section{Definitions}
\label{ch:defs}

% TODO: graphic for whole layer
Our type of SNN should be thought of as a composition of an initial input layer, a number of hidden spiking layers with internal state and an affine-linear layer mapping spikes activations over time, so called spike trains, to the value of the output layer.
Like motivated we are adding additional structure to the definitions~\cite{nguyen2025timespikeunderstandingrepresentational}.
We shall first define the input \(i^{[l]}(t)\), the membrane potential before spike \(p^{[l]}(t)\), the membrane potential after spike \(u^{[l]}(t)\) and the spike activations \(s^{[l]}(t)\) of the hidden layers:

% TODO: graphic for single neuron/single layer
%TODO: herleitung: ~/Downloads/1901.09948v2.pdf
\begin{definition}
  The \textbf{input vector} \(i^{[l]}(t)âˆˆ\{0,1\}^{n_l}\), the \textbf{spike vector} \(s^{[l]}(t)âˆˆ\{0,1\}^{n_l}\) and the \textbf{membrane potential vector} \(u^{[l]}(t)\) of a hidden layer \(Î»=(W^{[l]},b^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\), \(lâˆˆ[L]\), are recursively defined as
  \begin{align}
    i^{[l]}(t) & â‰” Î±^{[l]}i^{[l]}(t-1)+W^{[l]}s^{{(l)}-1}(t)+V^{[l]}s^{[l]}(t-1) \\
    p^{[l]}(t) & â‰” Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]} \\
    s^{[l]}(t) & â‰” H(p^{[l]}(t)-Ï‘1_{n_l}) \\
    u^{[l]}(t) & â‰” p^{[l]}(t)-Ï‘s^{[l]}(t)
  \end{align}
  with \(s^{[l]}(0)=0\) and given
  \begin{enumerate}
    \item[â€¢] \textbf{initial membrane potential}: \(u^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{initial input}: \(i^{[l]}(0)âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{weight matrices}: \(W^{[l]}âˆˆâ„^{n_lÃ—n_{l-1}}\), \(V^{[l]}âˆˆâ„^{n_lÃ—n_l}\),
    \item[â€¢] \textbf{bias vectors}: \(b^{[l]}âˆˆâ„^{n_l}\),
    \item[â€¢] \textbf{leaky terms}: \(Î±^{[l]},Î²^{[l]}âˆˆ[0,1]\),
    \item[â€¢] \textbf{threshold}: \(Ï‘^{[l]}âˆˆ(0,âˆ)\),
  \end{enumerate}
  where \(Hâ‰”ğŸ™_{[0,âˆ)}\) is a step function and \(Tâˆˆâ„•\) is the number of simulated time steps.
\end{definition}

We further define recurrent d.t. LIF-SNN and the function the network realizes:
\begin{definition}
  A \textbf{recurrent discrete-time LIF-SNN} of \textbf{depth} \(L\) with \textbf{layer-widths} \((n_0,â€¦,n_{L+1})\) and \(Tâˆˆâ„•\) time-steps is given by
  \[ Î¦â‰”((W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})_{lâˆˆ[L]},T,(E,D)) \]
  where the \textbf{input encoder} \(E:â„^{n_0}â†’â„^{n_0Ã—T}\) maps a vector \(xâˆˆâ„^{n_0}\) to a corresponding first layer spike activation \(s^{[0]}=E(x)\) and the \textbf{output decoder} \(D:\{0,1\}^{n_LÃ—T}â†’â„^{n_{L+1}}\) maps the spike activations of the last hidden layer to real values.
\end{definition}

% TODO: remark about layers being kinda useless
% TODO: remark comparison with previous model

\begin{definition}
  A recurrent discrete-time LIF-SNN \(Ï•\) \textbf{realizes} the function \(R(Î¦):â„^{n_0}â†’â„^{n_{L+1}}\):
  \[ R(Î¦)(x)=D((s^{[L]}(t))_{tâˆˆ[T]})\quad \text{with }s^{[0]}â‰”E(x)\]
\end{definition}

\begin{definition}
  A recurrent discrete-time LIF-SNN employs \textbf{direct encoding} if we have
  \[ âˆ€_{tâˆˆ[T]}E(x)(t)=x \]
  for the input encoder and has \textbf{membrane potential outputs} if the output decoder can be written as
  \[ D((s(t))_{tâˆˆ[T]})=\sum_{t=1}^Ta_t(W^{L+1}s(t)+b^{L+1}) \]
  for some \((a_t)_{tâˆˆ[T]}âˆˆâ„^T\), \(b^{L+1}âˆˆâ„^{n_{L+1}}\) and \(W^{L+1}âˆˆâ„^{n_{L+1}Ã—n_L}\).
\end{definition}
We will only consider recurrent discrete-time LIF-SNN with direct encoding and membrane potential outputs. In fact, we will use â€œrecurrent discrete-time LIF-SNNâ€ to mean â€œ\rdtlifsnn with direct encoding and membrane potentialâ€.

For clearer construction of our networks we will additionally define neurons:

% TODO: just notation?
\begin{definition}
  The \(i\)-th neuron of a hidden layer \(Î»=(W^{[l]},b^{[l]},V^{[l]},u^{[l]}(0),i^{[l]}(0),Î±^{[l]},Î²^{[l]},Ï‘^{[l]})\) of a \rdtlifsnn is a tuple \((w,b,v,u_0,i_0)\) with \(wâˆˆâ„^{n_{l-1}}\), \(vâˆˆâ„^{n_l}\) and \(b,u_0,i_0âˆˆâ„\), such that \(b\), \(u_0\), \(i_0\) are the \(i\)-th component of \(b^{[l]}\), \(u^{[l]}(0)\), \(i^{[l]}(0)\) respectively and \(w\), \(v\) are the \(i\)-th row vector of \(W^{[l]}\), \(V^{[l]}\) respectively.
\end{definition}

The following lemmas will be very helpful for the proofs in the following sections:
\begin{lemma}\label{lem:non-recursive-defs}
  We define the following non-recursive formulas with explicit reference to the previous spikes. Let \(1â‰¤tâ‰¤T\) and \((s_p^{[l]})_{lâˆˆ\{0â€¦L'\}}\) such that \(s_p^{[l]}:\{0,1\}^{n_lÃ—t'_l}\) for \(lâˆˆ\{0â€¦L'\}\). \(L'\) and \(âˆ€_lt'_l\) have to be chosen such that the following terms are well-defined, e.g. \(L'=L\) and \(âˆ€_lt'_l=T\)
  \begin{align}
   i^{[l]}(t;(s_p^{[l]})_l)&â‰”(Î±^{[l]})^ti^{[l]}(0)+\sum_{k=1}^t(Î±^{[l]})^{t-k}\left(W^{[l]}s_p^{[l-1]}(k)+V^{[l]}s_p^{[l]}(k-1)\right), \\
   p^{[l]}(t;(s_p^{[l]})_l) &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;(s_p^{[l]})_l)+b^{[l]}\right)-Ï‘\sum_{k=1}^{t-1}(Î²^{[l]})^{t-k}s_p^{[l]}(k),\\
   s^{[l]}(t;(s_p^{[l]})_l) & â‰” H(p^{[l]}(t;(s_p^{[l]})_l)-Ï‘1_{n_l}) \\
   u^{[l]}(t;(s_p^{[l]})_l) &â‰” (Î²^{[l]})^tu^{[l]}(0)+\sum_{k=1}^t(Î²^{[l]})^{t-k}\left(i^{[l]}(k;s_p)+b^{[l]}-Ï‘s_p^{[l]}(k)\right),
  \end{align}
  These formulas are equivalent to the previous recursive definitions, given \(s_p^{[l]}=s^{[l]}\) for \(lâˆˆ\{0â€¦L'\}\).
\end{lemma}

\begin{proof}
  Let the time \(1â‰¤tâ‰¤T\) be non-zero. By induction we immediately get
  \[ i^{[l]}(t)=(Î±^{[l]})^ti^{[l]}(0)+\sum_{i=1}^t(Î±^{[l]})^{t-i}\left(W^{[l]}s^{[l-1]}(i)+V^{[l]}s^{[l]}(i-1)\right) \]
  By definition of \(u^{[l]}\) and \(p^{[l]}\) we further obtain
  \[ u^{[l]}(t) = Î²^{[l]}u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}-Ï‘s^{[l]}(t) \]
  from which
  \[ u^{[l]}(t) = (Î²^{[l]})^tu^{[l]}(0)+\sum_{i=1}^t(Î²^{[l]})^{t-i}\left(i^{[l]}(i)+b^{[l]}-Ï‘s^{[l]}(i)\right) \]
  follows by induction.
  By substituting \(u^{[l]}\) in \(p^{[l]}\) we further obtain:
  \[ p^{[l]}(t) = (Î²^{[l]})^tu^{[l]}(0)+\sum_{i=1}^t(Î²^{[l]})^{t-i}\left(i^{[l]}(i)+b^{[l]}\right)-Ï‘\sum_{i=1}^{t-1}(Î²^{[l]})^{t-i}s^{[l]}(i) \]
\end{proof}

\begin{lemma}
  Let \(t_0,t_{Ï‰}âˆˆ[T]\) and \(jâˆˆ[n_l]\) such that \(t_0â‰¤t_{Ï‰}\). If \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}i^{[l]}_j(t)+b^{[l]}_jâ‰¤Ï‘\) and \(u^{[l]}_j(t_0-1)<Ï‘\), then \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}u^{[l]}_j(t)<Ï‘\).

  If further \(Î²=1\) and \(u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j)â‰¥0\)
  \[ \left( u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j) \right) - Ï‘\sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t) <Ï‘ \]
  If either \(âˆ€_{tâˆˆ\{t_0â€¦t_{Ï‰}\}}s^{[l]}_j(t)=0\) or \(âˆ€_{tâˆˆ\{t'â€¦t_{Ï‰}\}}i^{[l]}_j(t)â‰¥0\), where \(t'\) is the latest time \(â‰¤t_{Ï‰}\) such that \(s^{[l]}_j(t')=0\), then we even get
  \[ 0â‰¤\left( u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j) \right) - Ï‘\sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t) \]
\end{lemma}

\begin{remark}
  If \(Ï‘=1\), we get in particular
  \[ \left\lfloor u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j) \right\rfloor = \sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t) \]
\end{remark}

\begin{proof}
  We proof the first part by induction: Let \(tâˆˆ\{t_0â€¦t_{Ï‰}\}\). By definition of \(p^{[l]}\), by the given assumptions and by induction hypothesis we have
  \[ p^{[l]}_j(t)=Î²^{[l]}u^{[l]}_j(t-1)+i^{[l]}_j(t)+b^{[l]}_j â‰¤ u^{[l]}_j(t-1)+i^{[l]}_j(t)+b^{[l]}_j<2Ï‘  \]
  By definition of \(u^{[l]}\) and \(s^{[l]}\), we further get \(u^{[l]}_j(t)<Ï‘\).

  Let us further assume \(Î²=1\) and \(u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j)â‰¥0\).
  We now get
  \[ u^{[l]}(t_{Ï‰}) = u^{[l]}(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}(k)+b^{[l]}-Ï‘s^{[l]}(k)\right),  \]
  due to~\autoref{lem:non-recursive-defs}. From which we immediately obtain
  \[ \left( u^{[l]}_j(t_0-1)+\sum_{t=t_0}^{t_{Ï‰}}(i^{[l]}_j(t)+b^{[l]}_j) \right) - Ï‘\sum_{t=t_0}^{t_{Ï‰}}s^{[l]}_j(t) <Ï‘ \]
  by the first part of the proof. Let us further proof the left part of the inequality: Let \(t'\) be the latest time that \(s^{[l]}_j(t')=1\) has held for \(t'âˆˆ\{t_0â€¦t_{Ï‰}\}\). We may assume that \(t'\) exists since we are otherwise immediately finished. We then get \(Ï‘â‰¤p^{[l]}_j(t')\) and therefore
  \[ 0â‰¤u^{[l]}_j(t')=u^{[l]}_j(t_0-1)+\sum_{k=t_0}^{t'}\left(i^{[l]}_j(k)+b^{[l]}_j-Ï‘s^{[l]}_j(k)\right) \]
  by~\autoref{lem:non-recursive-defs}. By assumption and choice of \(t'\) we can conclude
  \[ â‰¤u^{[l]}_j(t_0-1)+\sum_{k=t_0}^{t_{Ï‰}}\left(i^{[l]}_j(k)+b^{[l]}_j-Ï‘s^{[l]}_j(k)\right) \]
\end{proof}

Let us now take a look at some simple examples:

%TODO: examples Ã¼berarbeiten
\begin{example}
  Let \(T,Lâˆˆâ„•\). Then there exists a d.t. LIF-SNN with \(âˆ€_{tâˆˆ[T]}s^{[L]}(t)=s^{[0]}(t)\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\).

  We can proof this by using constant width \(n_l=n\), weights \(W^{[l]}=I_n\), biases \(b^{[l]}=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky term \(Î²^{[l]}=0\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  We then get by definition:
  \begin{alignat*}{3}
    s^{[l]}(t) & = H(s^{[l-1]}(t)-1_{n_l})  &= \ s^{[l-1]}(t) \\
    u^{[l]}(t) & = \phantom{H(}s^{[l-1]}(t)-s^{[l]}(t)&= 0
  \end{alignat*}
\end{example}

\begin{example}\label{ex:2}
  Let \(T,Lâˆˆâ„•\). Then there exists a d.t. LIF-SNN with \(âˆ€_{tâˆˆT}s^{[L]}(t)=\max_{t'âˆˆ[t-1]}(s^{[0]}(t'))\) for any \(s^{[0]}âˆˆ\{0,1\}^{n_0Ã—T}\): In this network an output neuron switches on when the corresponding input neurons fires and does not switch off later.

  We can proof this by using constant width \(n_l=n\), weights \(W^{[l]}=TÂ·I_n\), biases \(b^{[l]}=0\), initial membrane potential \(u^{[l]}(0)=0\), leaky term \(Î²^{[l]}=1\) and threshold \(Ï‘^{[l]}=1\) for all \(lâˆˆ[L]\).

  We then get by definition:
  \begin{alignat*}{2}
    s^{[l]}(t) & = H(&u^{[l]}(t-1)+TÂ·s^{[l-1]}(t)-1_{n_l}) \\
    u^{[l]}(t) & = &u^{[l]}(t-1)+TÂ·s^{[l-1]}(t)-s^{[l]}(t)
  \end{alignat*}
  By adding over all timesteps we obtain
  \begin{alignat*}{1}
    s^{[l]}(t) & = H(T\sum_{i=1}^ts^{[l-1]}(i)-(\sum_{t=i}^{t-1}s^{[l]}(i))+1_{n_l}) ) \\
    u^{[l]}(t) & = \sum_{i=1}^t(TÂ·s^{[l-1]}(i)-s^{[l]}(i))
  \end{alignat*}
  Since \(\sum_{i=1}^{t-1}s^{[l]}(i)+1_{n_l}â‰¤T\), once there is any \(t_0âˆˆ[T]\) with \(s^{[l-1]}_i(t_0)=1\) for an \(i\), we get \(âˆ€_{tâ‰¥t_0}s^{[l]}_i(t)=1\).
  By induction over the layers we clearly get the required property.
\end{example}
