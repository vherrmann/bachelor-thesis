\section{Conclusion}

To summarize, we have examined the properties of~\Rdtlifsnn, in particular in comparison to~\dtlifsnn:
In~\cref{ch:defs} we motivated our definitions, introduced helpful notation and a few technical lemmas. In~\cref{ch:struct} we showed that~\rdtlifsnn allow more compact and efficient approximation than~\dtlifsnn of a certain class of differentiable continuous functions.

In~\cref{ch:partitions} we showed that the graph of~\Rdtlifsnns consists of a finite number of constant regions, which are half-open cuboids for trivial \(W^{[1]}\). We also showed that~\rdtlifsnn with trivial \(W^{[1]}\) can only fit data inside of a square.
While we were unable to prove it in~\cref{ch:partitions}, we are expecting to eventually find a proof that the number of different values a~\Rdtlifsnn with trivial \(W^{[1]}\), given \(T\) and \(n_1\) can obtain is bound by \((\frac{T^2+T+2}{2})^{n_1}\).

For that reason we continued our analysis in~\cref{ch:experiments}, where we described two algorithms to generate empirical data as well as visualizations to obtain an intuitive understanding of the problem at hand. While the data supports the hypothesis of the upper bound being correct, the data might be too specific, since we were not able to simulate many neurons or time-steps.

To conclude: We were only able to scratch the surface of the theory of~\rdtlifsnn. Much more research is needed to answer if~\rdtlifsnn are indeed better suited for machine learning and in particular for reasoning than current approaches. Since this thesis focused on theoretic aspects, further research investigating the practicality of the model is in particular needed.

%DONE: how do they act under training?

%DONE: problem might be not enough neurons/not enough time-steps/floating point numbers too imprecise/unclear if the question is even relevant

%CANCELLED: border of actually fitted regions → good to know

%DONE: ergebnisse zusammenfassen/offene fragen/kurze einschätzung der ergebnisse
% (nicht zu technisch)
