\section{Introduction}
\label{ch:intro}

%MAYBE: Zitat
The capabilities of AI have improved significantly in recent years. Especially large language models have found their way into most people's lives. But even though LLMs already pass a simple Turing Test~\cite{jones2025largelanguagemodelspass}, it is still unclear whether they are fundamentally capable of reasoning. While they are able to solve many problems using lots of compute, the process of finding a solution often involves far more trial and error than it does for a human~\cite{collins2023evaluatinglanguagemodelsmathematics} and reaches the correct result for the wrong reasons~\cite{mondorf2024comparinginferentialstrategieshumans}. Human intuition on the other hand seems to require less attempts and can often lead to better solutions~\cite{collins2023evaluatinglanguagemodelsmathematics}. Another limitation are hallucinations that appear to be an intrinsic property of LLMs~\cite{10.1145/3703155}.

Furthermore, current models are extremely energy inefficient compared to the human brain. While the human brain operates only on around \(20\text{W}\)~\cite{20wsleepwalker2009}, the usage of LLMs consumes huge amounts of energy: The training of GPT3 consumed ca.\ \(1\text{GWh}\) and inference of a short query using GPT-4o uses around \(0.4\text{Wh}\)~\cite{jegham2025hungryaibenchmarkingenergy}.
%DONE: zahlen für energieineffizienz

While LLMs might eventually overcome some of those problems, especially the energy inefficiency seems to be an inherent property of the current training process. Thus it seems to be reasonable that other models might be better fitted to the task of reasoning. Many technological advances — like airplanes or sonars — have been inspired by the longest ongoing optimization progress in history, the evolution of life\footnote{https://xkcd.com/1605/}. We therefore propose that neural networks more similar to the human brain might be smarter and more efficient. In this thesis we will investigate spiking neural networks with recursive connections.

% the 3rd generation of AI models, after the 2nd generation that currently drives most successful models.

% MAYBE: hard to train since recursive established methods backpropagation through time
% DONE: 
While the idea behind spiking neural networks is quite old, they have not been researched as much since finding an efficient training algorithm seems to be harder in comparison. Therefore, a lot of open questions about these networks still remain.
In this paper we will extend the work done in~\cite{nguyen2025timespikeunderstandingrepresentational}, by adding a decaying factor to the input of the neurons and allowing recursive connections between neurons of the same layer. This type of network will be called recurrent discrete time leaky-integrate-and-fire SNN, in short \rdtlifsnns.

This thesis will roughly follow the structure of~\cite{nguyen2025timespikeunderstandingrepresentational}: In~\cref{ch:defs} we motivate and formally introduce \rdtlifsnns. In~\cref{ch:struct} we will show that a certain class of continuously differentiable functions can be approximated arbitrarily well by \rdtlifsnns. While this has already been shown more generally for continuous functions in~\cite{nguyen2025timespikeunderstandingrepresentational}, we present a construction using far less neurons, using the internal linear structure of \rdtlifsnns.
After this, in~\cref{ch:partitions}, we analyze the output landscape of a \rdtlifsnn regarding the shape of constant output regions. While we were not able to generalize the upper bound on the number of constant regions of \dtlifsnn that~\cite{nguyen2025timespikeunderstandingrepresentational} established to \rdtlifsnn, we present some promising experimental results in~\cref{ch:experiments} and explain how exactly the algorithms are implemented and why they compute the correct result.
