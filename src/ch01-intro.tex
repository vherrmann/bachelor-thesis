\section{Introduction}
\label{ch:intro}

%MAYBE: Zitat
The capabilities of AI have improved significantly in recent years. Especially large language models have found their ways into most people's lives. But even though LLMs (see~\cite{jones2025largelanguagemodelspass}) already pass a simple Turing Test, it is still unclear whether they are fundamentally able of reasoning. While they are able to solve many problems with lots of compute, the process of finding a solution quite often involves far more try and error than it does for a human (see~\cite{collins2023evaluatinglanguagemodelsmathematics}) and obtains the correct result for the wrong reasons (see~\cite{mondorf2024comparinginferentialstrategieshumans}). Human intuition on the other hand does not seem to require as many attempts and quite often leads to a better solution (see~\cite{collins2023evaluatinglanguagemodelsmathematics}). Another limitation are hallucinations that appear to be an intrinsic property of LLMs (see~\cite{10.1145/3703155}).

Another problem with current models is their incredible energy inefficiency compared to a human brain. While the human brain operates only on \(~20\text{W}\) (see~\cite{20wsleepwalker2009}), the training by itself of LLM consumes huge amounts of energy: The training of GPT3 consumed \(~1\text{GWh}\) and inference of a short query using e.g. GPT-4o uses \(~0.4\text{Wh}\) (see~\cite{jegham2025hungryaibenchmarkingenergy}).
%DONE: zahlen für energieineffizienz

While LLMs might eventually overcome some of those problems, especially the extreme energy-inefficiency seems to be an inherent property of the current training process. We conclude therefore that other models might be better fitted to the task of reasoning. Since many human technological advances have been inspired by the longest ongoing optimization progress in history — \href{https://xkcd.com/1605/}{the evolution of life} — like airplanes or sonars, we propose that neural networks with more similar inner workings to human brains might be smarter and more efficient. In particular we will investigate a spiking neural network with recursive connections.

% the 3rd generation of AI models, after the 2nd generation that currently drives most successful models.

% MAYBE: hard to train since recursive established methods backpropagation through time
% DONE: 
While the idea behind spiking neural networks is quite old, they have not been researched as much, since finding a good training algorithm seems to be harder in comparison. Therefore there still remain a lot of open questions about these networks.
In this paper we shall extend on the work done in~\cite{nguyen2025timespikeunderstandingrepresentational}. We will extend the model of the networks by adding a decaying factor to the input of the neurons and allowing recursive connections between neurons in a layer.

We will roughly follow the structure of~\cite{nguyen2025timespikeunderstandingrepresentational}: In~\cref{ch:defs}. we will motivate and formally introduce recurrent discrete time leaky-integrate-and-fire SNN, in short \rdtlifsnns. In~\cref{ch:struct} we will show that some continuously differentiable functions can be approximated arbitrarily well by \rdtlifsnns. While this has already been show in~\cite{nguyen2025timespikeunderstandingrepresentational} more generally for continuous functions, we present a construction using far less neurons, using the internal linear structure of \rdtlifsnns.
In the following, in~\cref{ch:partitions}, we analyze the landscape of a \rdtlifsnn regarding the shape of constant output regions. While we were not able to generalize the upper bound on the number of constant regions of \dtlifsnn that~\cite{nguyen2025timespikeunderstandingrepresentational} established to \rdtlifsnn, we show some promising experimental results in~\cref{ch:experiments}. We additionally explain how we implemented the algorithms 
