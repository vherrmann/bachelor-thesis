\section{Structure of computations in \rdtlifsnn s}
\label{ch:struct}

This section concern itself with the approximation of continuous functions by d.t. LIF-SNN.

In~\cite{nguyen2025timespikeunderstandingrepresentational} the following theorem was proved:
\begin{theorem}\label{thm:approx-snn-constant}
  Let \(f\) be a continuous function on a compact set \(Î©âŠ‚â„^{n_0}\). For all \(Îµ>0\), there exists a d.t. LIF-SNN \(Î¦\) with direct encoding, membrane potential output, \(L=2\) and \(T=1\) such that
  \[ \norm{(R(Î¦)-f)|_{Î©}}_{âˆ}â‰¤Îµ\]
  Moreover, if \(f\) is \(Î“\)-Lipschitz, then \(Î¦\) can be chosen with width parameter \(n=(n_1,n_2)\) given by
  \begin{align*}
   n_1 &=\left(\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil,1\right\}+1\right)n_0  \\
   n_2 &=\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil^{n_0},1\right\}
  \end{align*}
  where \(\operatorname{diam}_âˆ(Î©)=\sup_{x,yâˆˆÎ©}\norm{x-y}_âˆ\).
\end{theorem}

\begin{proof}
  See~\cite{nguyen2025timespikeunderstandingrepresentational}.
\end{proof}

\begin{figure}[h!]
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        axis lines=middle,
        xmin=0, xmax=5,
        ymin=0, ymax=5,
        xtick={0, 1, 2, 3, 4},
        ytick={0, 1, 2, 3, 4},
        domain=0:5,
        samples=200,
        thick,
        legend pos=north west,
        width=\textwidth,
        height=6cm
        ]
        \addplot[blue]{x};
        \addplot[red]{floor(x)+0.5};
        \addlegendentry{$Îµ=0.5$}
      \end{axis}
    \end{tikzpicture}
    \caption{A \dtlifsnn approximating the identity}
    \label{id:sin-approx-by-dtlifsnn}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.55\textwidth}
    \centering
    \begin{tikzpicture}
      \begin{axis}[
        axis lines=middle,
        xmin=0, xmax=pi,
        ymin=0, ymax=1.2,
        xtick={0, pi/2, pi},
        xticklabels={$0$, $\frac{\pi}{2}$, $\pi$},
        ytick={0, 1},
        domain=0:pi,
        samples=200,
        thick,
        legend pos=north west,
        width=\textwidth,
        height=6cm
        ]
        \addplot[blue]{sin(deg(x))};
        \addplot[red]{(-cos(deg(floor(x*2+1)/2))+cos(deg(floor(x*2)/2)))*2};
        \addlegendentry{$Îµ=0.25$} % Îµâ‰ˆ0.5*(1/2) (since sin(x)â‰ˆx for xâ‰ª1)
        \addplot[purple]{(-cos(deg(floor(x*5+1)/5))+cos(deg(floor(x*5)/5)))*5};
        \addlegendentry{$Îµ=0.1$} % Îµâ‰ˆ0.5*(1/5) (since sin(x)â‰ˆx for xâ‰ª1)
      \end{axis}
    \end{tikzpicture}
    \caption{A \dtlifsnn approximating a sinus wave}
    \label{fig:sin-approx-by-dtlifsnn}
  \end{subfigure}
\end{figure}

The proof of~\cref{thm:approx-snn-constant} works by first showing that a continuous function can be arbitrarily approximated by step functions, in particular by step functions constant on hypercubes in \(Î©\).
Then the \dtlifsnn is constructed by using the first layer to cut the input space by hyperplanes along the cubes and using the second layer to represent the hypercubes.

While quite simple, this construction does not use the unique feature of (r.) \dtlifsnn, the ability of neurons to accumulate state over time. It therefore needs quite a lot more neurons than actually needed for many functions with (almost) linear segments, e.g. a sinus wave, see also~\cref{fig:sin-approx-by-dtlifsnn}.

We will now show a more efficient construction for \rdtlifsnn that uses the fact that \rdtlifsnn can quite efficiently approximate linear segments. The general intuition behind is to use piece-wise linear functions to approximate continuously differentiable functions and then approximate those piece-wise linear functions by \rdtlifsnn. In the end, we will see that we can in fact approximate any continuous function like this, by using mollification to extract an continuously differentiable approximation of the function.

% TODO: motivation (previous theorem + previous lower bound example), insatisfactory since d.t. SNN. have somewhat linear structure

% TODO: extend to arbitrarily continuous functions by choosing an continuously diff. function close to it
% TODO: extend to continuously differentiable functions apart from lebesgue-zero sets (where they are still continuous?)
% TODO: differentiable, but defined on compact subset of euclidean space???
%
% TODO: use generalized inverse of the modulus of continuity

%TODO: demand differentiability only for Î©Â°
We will now provide an alternative version of this theorem which uses an increased latency to reduce the number of neurons:
\begin{theorem}\label{thm:approx-snn}
  Let \(fâˆˆğ’^1(U,â„^m)\), with \(âˆ…â‰ UâŠ‚â„^n\) open, be a continuously differentiable function. Let further \(Î©âŠ‚U\) be a compact set. For all \(Îµ,Î¼,Î½>0\), \(Îµ=Î¼+Î½\), there exists a \rdtlifsnn \(Î¦\) with \(L=3\) and
  \begin{align*}
   T   &= (K(Î¼)+1)T_r(Î½)+1 \\
   n_1 &= n+1 \\
   n_2 &= K^n(Î¼)Â·(n+1)+3
  \end{align*}
  such that
  \[ \norm{(R(Î¦)-f)|_{Î©}}_{âˆ,2}â‰¤Îµ.\]
  Here \(T_râ‰”T_r(Î½)â‰”\frac{\operatorname{diam}(Î©)}{K}\frac{\norm{f'}_{âˆ,2}}{2Î½}\).
  The definition of \(K(Î¼)\) is given in~\cref{lem:approx-by-lin}.
\end{theorem}

\begin{lemma}\label{lem:smallest-cube}
  Let \(Î©âŠ‚â„^n\) be compact. Then there exists a half-open cube \(C\) with width \(\operatorname{diam}_âˆ(Î©)\) such that \(Î©âŠ‚\overline{C}\).
\end{lemma}

%TODO: we later use âŸ¦y,zâ¦† instead of âŸ¦a,bâ¦†; unify notation
\begin{proof}[\proofofref{lem:smallest-cube}.]
  We can first define \(xâ‰”(\min_{xâˆˆÎ©} x_i)_i\) since \(Î©\) is compact We further have \(yâ‰”x+\operatorname{diam}_âˆ(Î©)Â·ğŸ™_n\). We now have \(Câ‰”âŸ¦x,yâ¦†\). Suppose now a point \(zâˆˆÎ©âˆ–\overline{C}\) exists. By definition of \(x\), we have \(xâ‰¤z\). By definition of \(C\) we further get \(z\nleq y\), and therefore \(âˆƒ_iy_i<z_i\) by definition of \(C\). But this means \(\norm{x-z}_âˆ>\operatorname{diam}_âˆ(Î©)\).
\end{proof}

\begin{comment}
\begin{lemma}\label{lem:uniform-cont-â‰¤}
  Let \(M\) be a normed vector space and \(N\) be a metric space. Let \(f:Mâ†’N\) be uniformly continuous with modulus \(Î´\). We then have for every \(Îµ>0\):
  \[ âˆ€_{x,yâˆˆM}(\norm{x-y}_Mâ‰¤Î´(Îµ)â‡’d_N(x,y)â‰¤Îµ). \]
\end{lemma}

\begin{proof}
  Let \(Îµ>0\) with modulus \(Î´(Îµ)\), as well as \(xâˆˆM\) be given. Since \(f\) is uniformly continuous with modulus \(Î´(Îµ)\), we have \(f(B_{Î´}(x))âŠ‚B_{Îµ}(f(x))\). Since \(f\) is in particular continuous, we get
  \[ f(\overline{B}_{Î´}(x))=f(\overline{B_{Î´}(x)})âŠ‚\overline{f(B_{Î´}(x))}âŠ‚\overline{B}_{Îµ}(f(x)) \]
  We need to use the fact that \(M\) is a normed vector space in the first equality to get \(\overline{B}_{Î´}(x)=\overline{B_{Î´}(x)}\).
\end{proof}
\end{comment}

\begin{lemma}\label{lem:inverse-cont-mod}
  Let \(Ï‰:[0,âˆ]â†’[0,âˆ]\) be a modulus of uniform continuity of a uniformly continuous function \(f:Mâ†’N\), where \(M,N\) are metric spaces. We then define the generalized inverse of \(Ï‰:[0,âˆ]â†’[0,âˆ]\) by \(Ï‰^{â€ }(s)â‰”\inf\{tâˆˆ[0,âˆ]\mid Ï‰(t)>s\}\).

  We have the following properties
  \begin{enumerate}
  \item \(âˆ€_{x,yâˆˆM,sâˆˆ[0,âˆ]}d_M(x,y)â‰¤Ï‰^{â€ }(s)â‡’d_N(f(x),f(y))â‰¤s\).
  \item \(âˆ€_{sâˆˆ[0,âˆ]}s=0â‡”w^{â€ }(s)=0\).
  \end{enumerate}
\end{lemma}

\begin{proof}\phantom{}

  \begin{enumerate}
  \item Let \(x,yâˆˆM\) and \(sâˆˆ[0,âˆ]\) be given such that \(d_M(x,y)â‰¤Ï‰^{â€ }(s)\). By definition of \(Ï‰^{â€ }\), this means \(Ï‰(d_M(x,y))â‰¤s\). Since \(Ï‰\) is a modulus of uniform continuity of \(f\), we have \(d_N(f(x),f(y))â‰¤Ï‰(d_M(x,y))\) and therefore overall \(d_N(f(x),f(y))â‰¤s\).
  \item Since \(Ï‰\) is a modulus of uniform continuity, it is by definition continuous at \(0\). Let us choose an arbitrary sequence \((t_n)_{nâˆˆâ„•}\) with \(t_nâ†’0\). Then \(Ï‰(t_n)â†’0\) and therefore \(Ï‰^{â€ }(0)â‰¤\inf_{nâˆˆâ„•}t_n=0\).

  Is on the other hand \(Ï‰^{â€ }(s)=0\), then there is a sequence \((t_n)_{nâˆˆâ„•}\) with \(t_nâ†’0\) and therefore \(Ï‰(t_n)â†’0\). By definition of \(Ï‰^{â€ }(s)\), we have \(Ï‰(t_n)>s\), so we get \(s=0\).
  \end{enumerate}
\end{proof}

% TODO: differentiable, but defined on compact subset of euclidean space???
% TODO: quote formulation?
% TODO: instead of fixing \(Îµ^{t}\), get supremum of
% TODO: why Kâ‰ 0?
% TODO: only demand differentiability on CÂ°

\begin{lemma}\label{lem:approx-by-lin}
  Let \(fâˆˆğ’^1(U,â„^m)\), with \(âˆ…â‰ UâŠ‚â„^n\) open, be a continuously differentiable function. Let further \(Î©âŠ‚U\) be a compact set, such that there is a half-open cube \(C\) with \(Î©âŠ‚\overline{C}âŠ‚U\).

  For every \(Î¼>0\) we can compose \(C\) into
  \[ K^nâ‰”K(Î¼)^nâ‰”\min_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸)} \right\rceil\right\}^n \]
  half-open subcubes \((C_i)_{i=1..K^n}\) such that affine linear functions \(g_i:C_iâ†’â„^m\) exist with \(\norm{f-g}_{âˆ,2}<Î¼\), where \(g\) is the continuous extension of \((\sum_{i=1}^mg_iğŸ™_{C_i})|_C\) on \(\overline{C}\).
  Here \(Ï‰^{â€ }\) is the generalized inverse of a modulus of uniform continuity (with regard to \(\norm{Â·}_2\)) of the total derivative \(dF|_{\overline{C}}\). Since \(Î¾>0\), we also have \(Ï‰^{â€ }(Î¾)>0\) by~\cref{lem:inverse-cont-mod}.
\end{lemma}

\begin{remark}{}\\
\begin{enumerate}
\item If \(fâˆˆğ’^1(U,â„^m)\) is given with \(âˆ…â‰ UâŠ‚â„^n\) and a compact \(Î©âŠ‚â„^n\), but with no half-open cube \(C\) such that \(Î©âŠ‚\overline{C}âŠ‚U\), we can extend \(f|_{Î©}\) to a function \(f'âˆˆğ’^1(â„^n,â„^m)\): There is a partition of one \(Ï†_1,Ï†_2âˆˆğ’^{âˆ}(â„^n)\) subordinate to \(U\) and \(â„^nâˆ–Î©\). We get \(Ï†_1|_{Î©}=1\) and therefore \((Ï†_1f)|_{Î©}=f|_{Î©}\). \(f'â‰”Ï†_1f\) is further clearly \(ğ’^1(U,â„^m)\).
% \item We would like to only demand differentiability on \(Î©Â°\), â€¦ %TODO
\end{enumerate}

%TODO â†“
  % We would like to only demand differentiability on \(Î©Â°\), but the theorem would not be true anymore. Imagine the function \(f(x)=Ï‡_{[0,1]}\) with \(Î©=[0,1]âˆª[2,3]\). Clearly, \(f|_{[0,1]âˆª[2,3]}\) is continuous and \(f|_{(0,1)âˆª(2,3)}âˆˆğ’^1(â„)\). But on the other hand a half-open cube around \(Î©\) at least needs to include \((1,2)\) and would need to have
\end{remark}

% TODO: remark about why we are using 2-norm

% TODO: fix issue with C_i s not completely fitting in open \sqrt{2}Î´(Î¼) balls due to closed part of C_i
% TODO: F might not be defined at c_i, since \(Î©â‰ \overline{C}\)
\begin{proof}[\proofofref{lem:approx-by-lin}.]
  By~\cref{lem:smallest-cube} we have a half-open cube \(C\) with width \(\operatorname{diam}_âˆ(Î©)\) and \(Î©âŠ‚\overline{C}\). We can assume w.l.o.g. we can assume \(\overline{C}âŠ‚U\), since we can extend \(f\) to a function \(f'âˆˆğ’^1(â„^n,â„^m)\):

  Let further \(Î¼>0\) be given. Since \(Î©\) is compact and \(fâˆˆğ’^1(Î©,â„^m)\), \(dF\) is uniformly continuous on \(Î©\). Let \(Ï‰\) be a modulus of uniform continuity of \(dF\mid_{Î©}\).
  We will now partition \(C\) in
  \[K^nâ‰”\min_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸)} \right\rceil\right\}^n\]
  smaller half-open cubes. There are \(Î¾,Î¸\) such that the minimum in the definition of \(K^n\) is obtained, since we take it over the set of natural numbers. \(K\) is further well-defined, since \(âˆ€_{s>0}Ï‰^{â€ }(s)>0\),
  The subcubes have width \(wâ‰”\frac{\operatorname{diam}_âˆ(Î©)}{K}â‰¤\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾),Î¸)\). % TODO: should be < instead of â‰¤ in def of w???

  %TODO: why do we not have to proof <Î¼ on the boundary of C, on the extension

  Let us further define \(g_i:C_iâ†’â„^m\) by \(g_i(x)â‰”f(c_i)+df_{c_i}(x-c_i)\) where \(c_i\) is the center of \(C_i\), so in particular
  \begin{equation}\label{eq:4}
  \norm{x-c_i}_2=\sqrt{\sum_{j=1}^n\norm{(x-c_i)_je_j}_2^2}â‰¤\frac{w}{2}\sqrt{n}â‰¤\min(Ï‰^{â€ }(Î¾),Î¸).
  \end{equation}

  It suffices now to show \(\norm{f|_{C_i}-g_i}_âˆ<Î¼\). Let \(xâˆˆC_i\) and \(h(t)â‰”f(t(x-c_i)+c_i)\) with
  \[h'(t)=(df_{(x-c_i)t+c_i}âˆ˜d(tâ†¦t(x-c_i)+c_i)_t)(1)=df_{(x-c_i)t+c_i}(x-c_i).\]
  We obtain by the Fundamental theorem of calculus:
  \begin{align*}
  \norm{f(x)-f(c_i)-df_{c_i}(x-c_i)}_2 &= \norm{h(1)-h(0)-df_{c_i}(x-c_i)}_2 \\
                                    &= \norm{\int_0^1df_{(x-c_i)t+c_i}(x-c_i)dt-df_{c_i}(x-c_i)}_2
  \end{align*}
  Due to the generalized Minkowski-Inequality we can move the norm inside the integral:
  %TODO: ref for gen. Minkowski-Ineq?
  \begin{align*}
   \norm{\int_0^1df_{(x-c_i)t+c_i}(x-c_i)dt-df_{c_i}(x-c_i)}_2 &â‰¤ \int_0^1\norm{df_{(x-c_i)t+c_i}(x-c_i)-df_{c_i}(x-c_i)}_2dt \\
                                    &= \int_0^1\norm{(df_{(x-c_i)t+c_i}-df_{c_i})(x-c_i)}_2dt \\
                                    &â‰¤ \int_0^1\norm{df_{(x-c_i)t+c_i}-df_{c_i}}\norm{x-c_i}_2dt \\
                                    &â‰¤ \int_0^1Î¾\norm{x-c_i}_2dt \\ % wâ‰¤2\sqrt{n}^{-1}Ï‰^{â€ }(Î¾) %TODO: should be <???
                                    &= Î¾\norm{x-c_i}_2 \\
                                    &â‰¤ Î¾Î¸ \\
                                    &= Î¼
  \end{align*}
  In the fourth step we use \(\norm{df_{(x-c_i)t+c_i}-df_{c_i}}â‰¤Î¾\), which holds due to \(âˆ€_{tâˆˆ[0,1]}(x-c_i)t+c_iâˆˆC_i\), \eqref{eq:4} and \cref{lem:inverse-cont-mod}.
\end{proof}
%TODO: change notation of c_i to sth clearly not meant component

\begin{proof}[\proofofref{thm:approx-snn}.]
  Let a continuously differentiable function \(fâˆˆğ’^1(Î©,â„^m)\) on a compact set \(Î©âŠ‚â„^n\) be given. Let there further be \(Îµ,Î¼,Î½>0\) with \(Îµ=Î¼+Î½\).
  %TODO: clarify notation âŸ¦y,zâ¦† (line or proper subspace)
  By~\cref{lem:approx-by-lin} we have a half-open cube \(C=âŸ¦x^C,y^Câ¦†\) with \(Î©âŠ‚C\) with composition \(K^n\) half-open subcubes \((C_i)_{i=1..K^n}\) and linear functions \(g_i:C_iâ†’â„^m\), such that \(\norm{f-g|_Î©}_âˆ<Î¼\) for a \(gâ‰”\sum_{i=1}^mg_iğŸ™_{C_i}\).

  We will now define a \rdtlifsnn \(Î¦\) with direct input encoding and membrane-potential outputs such that \(\norm{R(Î¦)|_Î©-g}_âˆ<Î½\).

  Before anything else we shall set the following basic parameters \(i^{[l]}(0)=0\), \(Î±^{[l]}=0\) and \(Î²^{[l]}=Ï‘^{[l]}=1\) for all layers.

  % TODO: graphic

  The intuitive idea for the construction of the network is the following: In the first layer we have \(n\) neurons which capture the position of the input vector regarding \(C\) in their respective dimension and one last neuron that acts as an â€œalarm clockâ€ that shuts down the other neurons of the layer after the first \(KT_r\) time steps.

  The general idea of the second layer is that it just listens for the first \(KT_r\) time steps to the first layer and then in time-step \(KT_r+1\) it is decided which region \(C_i\) the input is located in. The last \(T_r\) time-steps are used to encode the location of \(x\) inside of \(C_i\).

  For each region \(C_i\) we have \(n+1\)-neurons. Each of the first \(n\) neurons encodes a commponent of the linear part of \(g_i\). They are also used to inform the \(n+1\)-th neuron of the group if the \(x\) has at least as big as the base point of \(C_i\). The \(n+1\)-th deactivates all other neurons of regions with smaller base point and encodes the constant part of \(g_i\). The last \(3\) neurons act as â€œclock neuronsâ€ enabling and disabling the other ones.

  % In the second layer we have \(m+1\)-neurons for each affine linear region \(C_i\). Each of the first \(m\) neurons encodes the linear part of a component of the output vector. The last, additional neuron acts as an activator to the specific region by recursively enabling the other neurons of this region and disabling all regions with smaller base point (that would otherwise be enabled).

  \begin{figure}[h]
    \centering
    \input{src/figures/approx-first-layer-timeline}
    \caption{Timeline of first-layer neuron}
  \end{figure}

  \begin{enumerate}
  \item First layer:
  We define the \(i\)-th neuron of the \(n\) neurons of the first layer by parameters
  \[ w=\frac{1}{y^C_i-x^C_i}e_i, \quad b=-\frac{x^C_i}{y^C_i-x^C_i}, \quad v=-e_{n+1},\quad u_0=0, \quad i_0=0. \]
  The â€œclock neuronâ€ of the first layer, with index \(c_1â‰”n+1\), is defined by:
  \begin{equation*}
    w=0, \quad b=\frac{1}{KT_r}, \quad v=e_{c_1}, \quad u_0=0, \quad i_0=0.
  \end{equation*}

  % TODO: check that proof works for xâˆˆ\(\overline{C}âˆ–C\)

  % We further define the \(n+1\)-th neuron of the first layer by \(w=0\), \(b=\frac{1}{K-1}\), \(v=\)
  % By~\cref{lem:non-recursive-defs} we get

  % TODO: proof correct behaviour

  %TODO: add timeline
  \item Second layer:
  Let us now construct the second layer in the following way: For each of the \(K^n(Î¼)\) subcubes in \(C\) we define \(n+1\) neurons like so: Let \(C_j=âŸ¦x^{C_j},y^{C_j}â¦†\) be one such subcube with position \(qâˆˆ\{0,â€¦,K(Î¼)-1\}^{n_1}\) in \(C\). We will write \(Î¹_j(i)â‰”j(n+1)+i\) to index the first \(n\) neurons in the layer and \(Ï‰_jâ‰”(j+1)(n+1)\) to index the last neuron of each group.

  The \(i\)-th neuron of the first \(n\) neurons, with index \(Î¹_j(i)\) in the second layer, has the parameters
  \begin{gather*}
    w=e_i, \quad b=0, \quad v=T(e_{c_2}-2e_{c_3}+e_{Ï‰_j}-r(q)), \\
    u_0=-q_iT_r-T+1, \quad i_0=0.
  \end{gather*}
  where \(j(q)\) is the index of the subcube at position \(q\) and â€œthe switchâ€ is
  \[ r(q)â‰”e_{Ï‰_j}-\sum_{\substack{q'âˆˆ\{0,â€¦,K(Î¼)-1\}^{n_1} \\ q'<q}}e_{Ï‰_j(q)}. \]
  The final neuron of the group, with index \(Ï‰_j\) in its layer, has the parameters
  \begin{gather*}
    w=0, \quad b=0, \quad v=\frac{1}{n}\sum_{i=1}^ne_{Î¹_j(i)}+e_{Ï‰_j}-e_{c_4}-r(q), \quad u_0=0, \quad i_0=0.
  \end{gather*}
  We also define the three â€œclock neuronsâ€, with index \(c_2â‰”(j+1)K^n(Î¼)+1\), \(c_3â‰”(j+1)K^n(Î¼)+2\) and \(c_4â‰”(j+1)K^n(Î¼)+3\) with very parameters:
  %TODO: KT_r=1?
  \begin{equation*}
    w=0, \quad b=b_{c_i}, \quad v=-2e_{c_i}, \quad u_0=0, \quad i_0=0.
  \end{equation*}
  where \(b_{c_1}=\frac{1}{KT_r-1}\), \(b_{c_2}=\frac{1}{KT_r}\) and \(b_{c_3}=\frac{1}{KT_r}\).
  %TODO: use c_{-1}, c_0 and c_{1} and a (for alarm) for the first layer neuron.
  \item Output decoder:
  We further define the parameters of the output decoder by \(a_t=0\), for \(tâ‰¤KT_r+1\) and otherwise \(a_t=1\). We further set \(b^{[L+1]}=0\) and \((W^{[L+1]})_{l,Î¹_j(i)}=g(x^{C_j}+\frac{1}{T_r}e_i)-g(x^{C_j})\) for \(lâˆˆ[m]\), \(jâˆˆK^n\) and \(iâˆˆ[n]\), as well as \((W^{[L+1]})_{l,Ï‰_j}=g(x^{C_j})\) for \(lâˆˆ[m]\) and \(jâˆˆK^n\).
  \end{enumerate}

  We will now proof that this construction indeed approximates \(g\) well enough. It will be helpful to consider the following, by choice of \(i^{[l]},Î±^{[l]},Î²^{[l]},Ï‘^{[l]}\), simplified equations:
  \begin{align*}
    p^{[l]}(t) & = u^{[l]}(t-1)+W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1)+b^{[l]} \\
    s^{[l]}(t) & = H(p^{[l]}(t)-1_{n_l}) \\
    u^{[l]}(t) & = p^{[l]}(t)-s^{[l]}(t)
  \end{align*}
  and in particular by~\cref{lem:non-recursive-defs}
  \begin{align*}
   i^{[l]}(t) &= W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1)  \\
   p^{[l]}(t) &= u^{[l]}(0)+\sum_{k=1}^t\left(W^{[l]}s^{[l-1]}(k)+V^{[l]}s^{[l]}(k-1)+b^{[l]}\right)-\sum_{k=1}^{t-1}s^{[l]}(k).
  \end{align*}

  Let now \(s^{[0]}(t)=xâˆˆC\). We will proof \(\norm{R(Î¦)(x)-g(x)}_{âˆ,2}â‰¤Î½\) in steps, by first proofing some simple properties of the previously defined neurons:

  \begin{enumerate}
  \item Characterization of the â€œclock neuronâ€ of the first layer:\\
  Let us first regard the neuron in the first layer: By choice of parameters we get:
  \begin{equation*}
   p^{[1]}_{c_1}(t) = \frac{t}{KT_r}+\sum_{k=1}^ts^{[1]}_{c_1}(k-1)-\sum_{k=1}^{t-1}s^{[1]}_{c_1}(k) = \frac{t}{KT_r}
  \end{equation*}
  So \(s^{[1]}_{c_1}(t)=1â‡”tâ‰¥KT_r\).
  \item Characterization of \(i\)-th neuron of the first layer, â€œcapturingâ€ the \(i\)-th dimension:\\
  We have:
  \begin{align*}
   i^{[1]}_i(t)+b^{[1]}_i=\frac{x_i-x^C_i}{y^C_i-x^C_i}-s^{[1]}_{c_1}(t-1)
  \end{align*}
  So we can use~\cref{lem:sum-spikes-over-time} for \(x^C_iâ‰¤x_i<y_i^C\) and \(tâ‰¤KT_r\) to obtain
  \begin{equation*}
  \left\lfloor KT_r\frac{x_i-x^C_i}{y^C_i-x^C_i} \right\rfloor =\left\lfloor u^{[1]}_i(0)+\sum_{t=1}^{KT_r}(i^{[1]}_i(t)+b^{[1]}_i) \right\rfloor = \sum_{t=1}^{KT_r}s^{[1]}_i(t)
  \end{equation*}
  and \(u_i^{[1]}(KT_r)<1\). We further clearly have \(i^{[1]}_i(t)+b^{[1]}_i<0\) for \(x^C_iâ‰¤x_i<y_i^C\) and therefore
  \[p_i^{[1]}(t)=u_i^{[1]}(KT_r)+\sum_{k=KT_r+1}^t\left(i^{[1]}_i(t)+b^{[1]}_i\right)-\sum_{k=KT_r+1}^{t-1}s^{[1]}(k) <1\]
  for \(t>KT_r\). So in particular
  \begin{equation}\label{eq:3}
  \left\lfloor KT_r\frac{x_i-x^C_i}{y^C_i-x^C_i} \right\rfloor = \sum_{t=1}^{KT_r}s^{[1]}_i(t) = \sum_{t=1}^Ts^{[1]}_i(t)
  \end{equation}
  % The equation clearly also holds for \(x_i=y_i\).
  \item Characterization of the â€œclock neuronsâ€ of the second layer:\\
  In contrast to the clock neuron of the first layer, these neurons only fire once:
  \begin{equation*}
   p^{[2]}_{c_i}(t) = tb_{c_i}-2\sum_{k=1}^ts^{[2]}_{c_i}(k-1)-\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k)=tb_{c_i}-3\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k)
  \end{equation*}
  %TODO: proof t<3(KT_r-1)
  Let us first consider \(c_2\): We clearly have \(p^{[2]}_{c_2}(t)<1\) for \(t<KT_r-1\), but \(p^{[2]}_{c_2}(KT_r-1)=1\). Further \(t<3(KT_r-1)\) for all \(tâ‰¤T\), so \(p^{[2]}_{c_2}(t)<1\) for \(t>KT_r-1\). So \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_2}(t)=1_{\{KT_r-1\}}(t)\).

  Similarly we obtain \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_3}(t)=Ï‡_{\{KT_r\}}(t)\) and \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_4}(t)=Ï‡_{\{KT_r+1\}}(t)\).
  \item â€œNon-activator neuronsâ€ of the groups in the second layer don't fire before \(t=KT_r\). \\
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(t) &= -q_iT_r-T+1+\sum_{k=1}^t\left(s^{[1]}_i(k)+âŸ¨v_i,s^{[2]}(k-1)âŸ©\right)-\sum_{k=1}^{t-1}s^{[2]}(k). \\
    âŸ¨v_i,s^{[2]}(k-1)âŸ©&=T(s^{[2]}_{c_2}(k-1)-2s^{[2]}_{c_3}(k-1)+s^{[2]}_{Ï‰_j}(k-1)-âŸ¨r(q),s^{[2]}(k-1)âŸ©)
  \end{align*}
  with \(\).
  \item â€œActivator neuronsâ€ of the second layer at most once for \(tâ‰¤K(Î¼)T_r(Î½)\)/â€œActivator neuronsâ€œ activate at earliest at \(t=K(Î¼)T_r(Î½)+1\):\\
  We get
  \item â€œActivator neuronsâ€ of the second layer don't fire before \(t=KT_r+1\). \\
  \item â€œActivator neuronsâ€ of the second layer fire at most once at \(t=KT_r+1\) and \(t=KT_r+2\). \\
  \item â€œActivator neuronsâ€ of the second layer fire once if \(x\) is bigger than the base point of their group  and twice if \(x\) is in the region. \\
  \item â€œNon-activator neuronsâ€ of the groups in the second layer fire only fire after \(KT_r+1\), if \(x\) is the corresponding region. \\

  %TODO: replace K(Î¼) by K and T_r(Î½) by T_r where appropriate
  \begin{align*}
   p^{[2]}_{Î¹_j(i)}(t) &= 1-q_iT_r+\sum_{k=1}^t\left(s^{[1]}_i(k)+TâŸ¨r(q),s^{[2]}(k-1)âŸ©\right)-(T+1)\sum_{k=1}^{t-1}s^{[2]}_{Î¹_j(i)}(k) \\
   p^{[2]}_{Ï‰_j}(t) &= \sum_{k=1}^t\left(\frac{1}{n+1}\left(s^{[1]}_{c_1}(t)+\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(t-1)\right)+KT_râŸ¨r(q),s^{[2]}(k-1)âŸ©\right)
  \end{align*}
  Notice first, that

  for \(tâ‰¤K(Î¼)T_r(Î½)\), since as we will later show, \(s^{[1]}_{Ï‰_j}(t)=0\) for all \(jâˆˆ[K^n]\) and \(tâ‰¤K(Î¼)T_r(Î½)\). We therefore have \(s^{[2]}_{Î¹_j(i)}(t)=1\) for a \(tâ‰¤K(Î¼)T_r(Î½)\) exactly if \(\sum_{k=1}^{K(Î¼)T_r(Î½)}s^{[1]}_i(k)â‰¥q_iT_r(Î½)\) which is equivalent to \(\) by~\eqref{eq:3}

  \item â€œActivator neuronâ€ activates exactly if \(x\) is as large as the lower vertex:\\
  \item The neurons of a subcube \(C_j\) fire after \(K(Î¼)T_r(Î½)+1\), exactly if \(xâˆˆC_j\)\\
  \item If \(xâˆˆC_j\), then \(\norm{R(Î¦)(x)-g(x)}_2â‰¤Î½\)\\
  \item Characterization of the \(Ï‰_j\)-th neuron of the second layer, the â€œactivator neuronâ€ of group~\(j\):\\
  We get
  \end{enumerate}

  Let now \(s^{[0]}(t)=xâˆˆ\overline{C}âˆ–C\).

  % TODO: proof correct behaviour

  %TODO: show construction that has binary state (flip-flop; )

  % TODO: why Kâ‰ 0
  % TODO: check on â€off by one errorsâ€œ
\end{proof}

%TODO (maybe): mollification

%TODO: cleanup
%TODO: remark about SNN construction. By adding more neurons to first layer ((K-1)n to be exact) you can half the required time steps.
% TODO: steelman comparison by arguing that L can be replaced by sth like 1/Î´(Îµ)
Sadly the size of the network in this construction is not always smaller than the one from~\cref{thm:approx-snn-constant}. A concrete counter example is a sinus wave with high frequency and small ampitude, like \(f(x)â‰”\frac{\sin(nx)}{n}\) with \(nâˆˆâ„•\) on \(Î©=[0,2Ï€]\). Since \(f'(x)=\cos(nx)\) and \(\norm{f'|_{Î©}}_{âˆ}=1\), we can choose \(L=1\) as a Lipschitz constant for \(f\). At the same time, since \(f''(x)=n\sin(nx)\) and \(\norm{f''|_{Î©}}_{âˆ}=n\), the biggest possible modulus of uniform continuity on \(Î©\) we can give for \(f'\) is \(Î´(Îµ)â‰”\frac{Îµ}{n}\). So we get
\[ \max_{\substack{Î¾,Î¸>0\\Î¾Î¸=Îµ}}\min(Î´(Î¾),Î¸)=\max_{\substack{Î¾>0}}\min(\frac{Î¾}{n},\frac{Îµ}{Î¾})=\sqrt{\frac{Îµ}{n}} \]
So we get that \(K(Îµ)\) has a value of \(\lfloor \frac{Ï€\sqrt{n}}{\sqrt{Îµ}} \rfloor\) by definition. We therefore get for the layer sizes:
\begin{alignat*}{2}
   & \cref{thm:approx-snn-constant} \qquad &\qquad  &\cref{thm:approx-snn} \\
  &n_1 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil+1 \qquad &\qquad &n_1 = 2 \\
  &n_2 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil \qquad &\qquad &n_2 = 2\lfloor \tfrac{Ï€\sqrt{n}}{\sqrt{Îµ}} \rfloor
\end{alignat*}
While the first and second layer are clearly far smaller than the first layer of the other construction, especially for small \(Îµ\), the third layer of our construction is arbitrarily bad for \(nâ†’âˆ\) compared to the last layer of the other construction.

But there is one big difference. Since the size of last layer grows only proportionally to \(\frac{1}{\sqrt{Îµ}}\) and not proportionally to \(\frac{1}{Îµ}\), it is arbitrarily smaller than the other constructions last layer for \(Îµâ†’0\) and any \(nâˆˆâ„•\).
%TODO: image

We will generalize this observation with the following theorem:
\begin{theorem}
  Let \(fâˆˆğ’^1(U,â„)\), with \(âˆ…â‰ UâŠ‚â„^n\) open, be a continuously differentiable function, such that \(dF|_{Î©}\) is \(L\)-Lipschitz. Let further \(Î©âŠ‚U\) be a compact set.

  We can choose parameters \((Î¼_{Îµ})_{Îµ>0}\), \(Î¼_{Îµ}â‰”\sqrt{Îµ}\), and a modulus of continuity \(Ï‰(x)â‰”Lx\) of \(dF|_{Î©}\), such that
  \[ \lim_{Îµâ†’0}\left(\frac{K(Î¼_{Îµ})}{Îµ^{-1}}\right)=0 \]
  where \(K\) as defined in~\cref{lem:approx-by-lin} for \(Î©\).
\end{theorem}

%TODO: proof modulus of continuity stuff?
%TODO: fix statement, Ï‰
\begin{proof}
  First \(Ï‰(x)â‰”L(x)\) is obviously indeed a modulus of continuity of \(dF|_{Î©}\), since by definition of \(L\), we have \(âˆ€_{x,yâˆˆÎ©}\norm{dF_x-dF_y}â‰¤L\norm{x-y}_2=Ï‰(\norm{x-y}_2)\) and \(Ï‰\) is clearly continuous at \(0\). We further get \(Ï‰^{â€ }(s)=\inf\{tâˆˆ[0,âˆ]\mid Lt>s\}=\frac{s}{L}\).

  It further suffices to show there exist \((Î¾_{Î¼_{Îµ}})_{Îµ>0}\) and \((Î¸_{Î¼_{Îµ}})_{Îµ>0}\) with \(âˆ€_{Îµ>0}Î¾_{Î¼_{Îµ}}Î¸_{Î¼_{Îµ}}=Î¼_{Îµ}\) and
  \[ \lim_{Îµâ†’0}\frac{Îµ}{\min(Ï‰^{â€ }(Î¾_{Î¼_{Îµ}}),Î¸_{Î¼_{Îµ}})}=0 \]
   But this is obvious, if we choose \(Îµ_{Î¼_{Îµ}}â‰”\sqrt{Î¼_{Îµ}}\) and \(Î¸_{Î¼_{Îµ}}â‰”\sqrt{Î¼_{Îµ}}\), since
   \[ \frac{Îµ}{\min(Ï‰^{â€ }(Î¾_{Î¼_{Îµ}}),Î¸_{Î¼_{Îµ}})}=\frac{Îµ}{\min(\frac{1}{L},1)\sqrt{Î¼_{Îµ}}}=\max(L,1)Îµ^{\frac{3}{4}}. \]
\end{proof}

%TODO: betrachtung durch kombinatorik/wahrscheinlichkeit?

%TODO: there are also constructions for \dtlifsnn that use far less neurons, general structure likeâ€¦

% TODO: encoding with fewer layers for on finite segments continuously differentiable functions
