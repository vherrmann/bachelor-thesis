\section{Structure of computations in \rdtlifsnns}
\label{ch:struct}

When working with neural networks a fundamental question is how well they are able to approximate functions. Towards that end the following theorem was proved in~\cite{nguyen2025timespikeunderstandingrepresentational}.

\begin{theorem}\label{thm:approx-snn-constant}
  Let \(f\) be a continuous function on a compact set \(Î©âŠ‚â„^{n_0}\). For all \(Îµ>0\), there exists a \dtlifsnn \(Î¦\) with direct encoding, membrane potential output, \(L=2\) and \(T=1\) such that
  \[ \norm{(R(Î¦)-f)|_{Î©}}_{âˆ}â‰¤Îµ\]
  Moreover, if \(f\) is \(Î“\)-Lipschitz, then \(Î¦\) can be chosen with width parameters \((n_1,n_2)\) given by
  \begin{align*}
   n_1 &=\left(\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil,1\right\}+1\right)n_0, \\
   n_2 &=\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil^{n_0},1\right\}.
  \end{align*}
\end{theorem}

\begin{figure}[h!]
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \input{src/figures/ch03-approx-id-dtlifsnn.tex}
    \caption{\Dtlifsnns approximating the identity}
    \label{fig:id-approx-by-dtlifsnn}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.55\textwidth}
    \centering
    \input{src/figures/ch03-approx-sinus-dtlifsnn.tex}
    \caption{\Dtlifsnns approximating a sinus wave}
    \label{fig:sin-approx-by-dtlifsnn}
  \end{subfigure}
  \caption{\Dtlifsnns approximating functions}
\end{figure}

The proof of~\cref{thm:approx-snn-constant} first shows that a continuous function can be arbitrarily approximated by step functions, in particular by step functions constant on hypercubes in \(Î©\).
Then a \dtlifsnn is constructed by using the first layer to partition the input space along hyperplanes into cubes and the second layer to assign fitting values to the hypercubes.

While quite simple, this construction does not use the unique feature of \dtlifsnns/\rdtlifsnns, the ability of neurons to accumulate state over time. It therefore needs quite a lot more neurons than actually needed for many functions with (almost) linear segments, like a sinus wave. E.g. in~\cref{fig:sin-approx-by-dtlifsnn} a neuron is needed for every constant region of the graphs in the first and second layer each.

We will show a more efficient construction for \rdtlifsnn using the fact that \rdtlifsnn can efficiently approximate linear regions. The general intuition is to use piece-wise linear functions to approximate continuously differentiable functions and to then construct a \rdtlifsnn approximating the piece-wise linear function by discretizing the input dimensions into spike trains in the first layer that are consumed by groups of neurons in the second layer, one group for each almost linear region.

To state our theorem we first need to define the notions of â€œmodulus of uniform continuityâ€ and â€œgeneralized inverse of a modulus of uniform continuityâ€ and prove some simple properties:

%MAYBE: quote def?
\begin{definition}
  Let \(M,N\) be metric spaces. A modulus of uniform continuity of a uniformly continuous function \(f:Mâ†’N\) is a function \(Ï‰:[0,âˆ]â†’[0,âˆ]\) which vanishes at \(0\), i.e. \(\lim_{xâ†’0}Ï‰(x)=0\) and further fullfills
  \[ âˆ€_{x,yâˆˆM}d_N(f(x),f(y))â‰¤Ï‰(d_M(x,y)). \]
  The generalized inverse of \(Ï‰\) is defined as
  \[Ï‰^{â€ }(s)â‰”\inf\{tâˆˆ[0,âˆ]\mid Ï‰(t)>s\}.\]
\end{definition}

\begin{lemma}\label{lem:inverse-cont-mod}
  Let \(Ï‰:[0,âˆ]â†’[0,âˆ]\) be a modulus of uniform continuity of a uniformly continuous function \(f:Mâ†’N\), where \(M,N\) are metric spaces.

  We have the following properties:
  \begin{enumerate}
  \item \(âˆ€_{x,yâˆˆM,sâˆˆ[0,âˆ]}d_M(x,y)â‰¤Ï‰^{â€ }(s)â‡’d_N(f(x),f(y))â‰¤s\),
  \item \(âˆ€_{sâˆˆ[0,âˆ]}w^{â€ }(s)=0â‡’s=0\).
  \end{enumerate}
\end{lemma}

\begin{proof}\phantom{}

  \begin{enumerate}
  \item Let \(x,yâˆˆM\) and \(sâˆˆ[0,âˆ]\) be given such that \(d_M(x,y)â‰¤Ï‰^{â€ }(s)\). By definition of \(Ï‰^{â€ }\), this means \(Ï‰(d_M(x,y))â‰¤s\). Since \(Ï‰\) is a modulus of uniform continuity of \(f\), we have \(d_N(f(x),f(y))â‰¤Ï‰(d_M(x,y))\) and therefore overall \(d_N(f(x),f(y))â‰¤s\).
  \item Let \(Ï‰^{â€ }(s)=0\), then there is a sequence \((t_n)_{nâˆˆâ„•}\) with \(t_nâ†’0\), such that \(Ï‰(t_n)>s\) by the definition of \(Ï‰^{â€ }\). But since \(Ï‰(t_n)â†’0\) by definition of \(Ï‰\), we get \(s=0\).
  \end{enumerate}
\end{proof}

% CANCELLED: In the end, we will see that we can in fact approximate any continuous function like this, by using mollification to extract an continuously differentiable approximation of the function.

% DONE: motivation (previous theorem + previous lower bound example), insatisfactory since d.t. SNN. have somewhat linear structure

% CANCELLED: extend to continuously differentiable functions apart from lebesgue-zero sets (where they are still continuous?)
% DONE: differentiable, but defined on compact subset of euclidean space???
%
% DONE: use generalized inverse of the modulus of continuity

\begin{figure}[!ht]
  \centering
  \input{src/figures/ch03-approx-sets}
  \caption{A possible configuration of the sets in~\cref{thm:approx-snn}}
  \label{fig:ch03-approx-sets}
\end{figure}

Let us now state our theorem:

% CANCELLED: demand differentiability only for Î©Â°
\begin{theorem}\label{thm:approx-snn}
  Let a \(fâˆˆğ’^0(U,â„^n)\) be a continuous function, such that \(f\mid_{UÂ°}âˆˆğ’^1(UÂ°,â„^m)\) is continuously differentiable with bounded differential, i.e. \(\norm{d(f|_{UÂ°})}_{âˆ,2}<âˆ\). Let further \(âˆ…â‰ Î©âŠ‚U\) be an arbitrary non-empty subset and \(CâŠ‚â„^n\) a half-open cube such that \(Î©âŠ‚C\) and \((Î©+B_{Ï,2}(0))âˆ©CÂ°âŠ‚UÂ°\) for a \(Ï>0\). %DONE: graphic of Î©, U, C, N_Ï(Î©)

  Then for all \(Îµ,Î¼,Î½>0\) with \(Îµ=Î¼+Î½\) there exists a \rdtlifsnn \(Î¦\) with \(L=2\) and
  \begin{align*}
   T   &= (K(Î¼)+1)T_r(Î½)+2, \\
   n_1 &= n+1, \\
   n_2 &= K(Î¼)^n(n+1)+3
  \end{align*}
  such that
  \[ \norm{R(Î¦)|_Î©-f}_{âˆ,2}â‰¤Îµ.\]
  Where we use 
  \begin{align*} %DONE: add Ï to later statements, replace d(f|_{CÂ°}) by d(f|_{UÂ°})
   T_r&â‰”T_r(Î½)â‰”\max\left(2,\left\lceil \sqrt{n}\frac{\operatorname{diam}_{âˆ}(C)}{K}\frac{\norm{d(f|_{UÂ°})}_{âˆ,2}}{Î½} \right\rceil\right)âˆˆâ„•, \\ %DONE: fix T_{r} later
   K&â‰”K(Î¼)â‰” \min_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸,\frac{Ï}{2})} \right\rceil\right\}âˆˆâ„•.
  \end{align*} %DONE: if Ï‰^{â€ }=âˆ, then still Kâ‰ 0?
  Here \(Ï‰^{â€ }\) is the generalized inverse of a modulus of uniform continuity of the total derivative \(d(f|_{UÂ°})\) with regard to \(\norm{Â·}_2\). The differential \(d(f|_{UÂ°})\) is uniformly continuous because it is bounded.
  Due to \(Î¾â‰ 0\), we have \(Ï‰^{â€ }(Î¾)>0\) by~\cref{lem:inverse-cont-mod}. Furthermore, \(K\) is well-defined, since there are \(Î¾,Î¸\) such that the minimum in the definition of \(K^n\) is obtained, because the minimum is taken over the set of natural numbers. Notice that further \(Kâ‰ 0\), because \(\min(Ï‰^{â€ }(Î¾),Î¸,\frac{Ï}{2})<âˆ\) as well as \(\operatorname{diam}_{âˆ}(C)â‰ 0\), since \(Î©âŠ‚C\) and \(Î©â‰ âˆ…\). Hence \(T_râˆˆâ„•\) is well-defined.
\end{theorem}

\begin{remark}\label{rem:ch03-use-cube}
  Let \(f:Câ†’â„^n\) be defined on a half-open cube \(C\) and take \(Î©=C\). We can then remove \(\frac{Ï}{2}\) from the definition of \(K\), since we can choose \(Ï\) arbitrarily big,
  \[ K= \min_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸)} \right\rceil\right\}. \]
\end{remark}

\begin{remark}\label{rem:ch03-differential-lipschitz}
  If the differential \(d(f|_{UÂ°})\) is \(L\)-Lipschitz in addition to the conditions in~\cref{thm:approx-snn}, we can simplify the definition of \(K\). Let us choose the canonical modulus of uniform continuity \(Ï‰(x)â‰”Lx\), such that \(Ï‰^{â€ }(x)=\frac{1}{L}x\) (with \(\frac{1}{L}=âˆ\) for \(L=0\)).
  Now for \(L>0\) we have
  \[ \max_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\min(Ï‰^{â€ }(Î¾) ,Î¸)=\max_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\min(\frac{1}{L}Î¾ ,\frac{Î¼}{Î¾})=\sqrt{\frac{Î¼}{L}} \]
  since \(\frac{1}{L}Î¾\) and \(\frac{Î¼}{Î¾}\) are monotonically increasing, monotonically decreasing respectively in \(Î¾\) such that \(\frac{1}{L}Î¾=\frac{Î¼}{Î¾}\) is equivalent to \(Î¾=\sqrt{Î¼L}\) for \(Î¾>0\).
  So we have
  \[ K= \left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(\sqrt{\frac{Î¼}{L}},\frac{Ï}{2})} \right\rceil \]
  for \(L>0\). For \(L=0\) we have \(\min(Ï‰^{â€ }(Î¾) ,Î¸)=Î¸\), so \(K=1\).
\end{remark}

\begin{figure}[!ht]
  \centering
  \input{src/figures/ch03-spiral-staircase.tex}
  \caption{A spiral staircase function}
  \label{fig:ch03-spiral-staircase}
\end{figure}

\begin{remark}
  The conditions of the theorem regarding \(U,Î©,C\) might seem unnecessarily complex at first, but they are indeed necessary to approximate some functions efficiently.
  For our construction of the \rdtlifsnn we need piece-wise affine-linear approximations of \(f\), defined on subcubes of a cube \(CâŠ‚â„^n\). Suppose, the theorem just required \(f\) to be defined on that cube \(C\) instead and further \(Î©=C\).

  Now picture a function \(f\) like the one shown in~\cref{fig:ch03-spiral-staircase}. If we want to approximate the whole complete input space \(Î©\) of \(f\), we need to put a half-open square \(C\) around the arc due to our construction. To further use the theorem with \(U=Î©=C\) we would need to have differentiability on the whole of \(CÂ°\), but \(f\) is only defined on an arc inside of \(C\).
  This problem can be avoided by extending \(f\) to all of \(â„^n\). However, this might introduce a very steep gradient around \((-0.5,0.0)\), such that \(\norm{d(f|_{CÂ°})}_{âˆ,2}\) is very large and changes unnecessarily quick, making \(T_r\) and \(K\) excessively large.

  On the other hand,~\cref{thm:approx-snn} only uses the values of \(df\) on a thin region around \(Î©\) to determine the network size.
\end{remark}

\begin{comment}
\begin{remark}
  \cref{thm:approx-snn} cannot be generalized to arbitrary input spaces like compact sets: Consider a spiral staircase function like shown in~\cref{fig:ch03-spiral-staircase} defined on a compact set \(Î©â‰”\{(r\cos(Ï†),r\sin(Ï†))\mid 0.5â‰¤râ‰¤1,\ \abs{Ï†-Ï€}â‰¥0.2\}\). Such a function is clearly continuous and continuously differentiable on its interior \(Î©Â°\), such that in particular the differential is bounded, \(\norm{d(f|_{Î©Â°})}_{âˆ,2}<âˆ\).

  For our construction of the SNN we now need an encompassing half-open rectangle of \(Î©\), let e.g. \(C=[0,2)^2\). Now in~\cref{lem:approx-by-lin} we will partition \(C\) into sub-cubes, such that \(f\) is almost linear on those  w%CANCELLED
  
  We need the half-open cube \(C\) with \(Î©âŠ‚CâŠ‚U\) to correctly choose \(T_r\) and \(K\): Suppose we regard the function \(f:((0,1) âˆª (2,3))â†’â„\), \(xâ†¦Ï‡_{[0,1]}(x)\). This function is certainly continuously differentiable, in particular, all derivatives are zero. In our construction of the \rdtlifsnn we use a cube in the input space as our canvas,
  %CANCELLED
  Suppose, \(K\) would depend on the modulus of uniform continuity of \(df\) on \(U\) and \(T_r\) on \(\norm{df|_U}_{âˆ,2}\), without the a half-open cube \(C\) with \(Î©âŠ‚CâŠ‚U\)
\end{remark}
\end{comment}

%DONE: remark about usage with \(Î©=U=C\).

\begin{remark}
  In our construction \(Î¼\) and \(Î½\) determine whether to optimize the number of neurons or the number of time-steps. As we see later, \(K^n\) is the number of subcubes we will split \(C\) into such that \(f\) is almost linear on each of them. From the definition it is clear that \(K\) and therefore the number of neurons only depend on \(f\) through \(Ï‰^{â€ }(Î¾)\). Furthermore, \(Ï‰^{â€ }(Î¾)\) is a measure of how strongly the slope of \(f\) is changing and thus into how many subcubes we need to split \(C\) such that \(f\) is sufficiently almost linear on each subcube.

  We will moreover see that \(T_r\) corresponds to the number of constant intervals with which we approximate \(f\) on the almost affine linear regions. It is therefore to be expected that \(T_r\) depends on the width \(\frac{\operatorname{diam}_{âˆ}(C)}{K}\) of the subcubes and the maximal slope \(\norm{d(f|_{UÂ°})}_{âˆ,2}\).
\end{remark}

\begin{comment}
\begin{remark}
If \(fâˆˆğ’^1(U,â„^m)\) is given with \(âˆ…â‰ UâŠ‚â„^n\) and a compact subset \(Î©âŠ‚â„^n\), we can extend \(f|_{Î©}\) to a function \(dfâˆˆğ’^1(â„^n,â„^m)\): There is a partition of one \(Ï†_1,Ï†_2âˆˆğ’^{âˆ}(â„^n)\) subordinate to \(U\) and \(â„^nâˆ–Î©\). We get \(Ï†_1|_{Î©}=1\) and therefore \((Ï†_1f)|_{Î©}=f|_{Î©}\). \(f'â‰”Ï†_1f\) is further clearly \(ğ’^1(U,â„^m)\).
\end{remark}
\end{comment}

\begin{comment}
\begin{remark}
If \(fâˆˆğ’^1(U,â„^m)\) is given with \(âˆ…â‰ UâŠ‚â„^n\) and a compact \(Î©âŠ‚â„^n\), but with no half-open cube \(C\) such that \(Î©âŠ‚\overline{C}âŠ‚U\), we can extend \(f|_{Î©}\) to a function \(dfâˆˆğ’^1(â„^n,â„^m)\): There is a partition of one \(Ï†_1,Ï†_2âˆˆğ’^{âˆ}(â„^n)\) subordinate to \(U\) and \(â„^nâˆ–Î©\). We get \(Ï†_1|_{Î©}=1\) and therefore \((Ï†_1f)|_{Î©}=f|_{Î©}\). \(f'â‰”Ï†_1f\) is further clearly \(ğ’^1(U,â„^m)\).
% \item We would like to only demand differentiability on \(Î©Â°\), â€¦ %CANCELLED

%CANCELLED â†“
  % We would like to only demand differentiability on \(Î©Â°\), but the theorem would not be true anymore. Imagine the function \(f(x)=Ï‡_{[0,1]}\) with \(Î©=[0,1]âˆª[2,3]\). Clearly, \(f|_{[0,1]âˆª[2,3]}\) is continuous and \(f|_{(0,1)âˆª(2,3)}âˆˆğ’^1(â„)\). But on the other hand a half-open cube around \(Î©\) at least needs to include \((1,2)\) and would need to have
\end{remark}
\end{comment}


%MAYBE: include?
\begin{comment}
\begin{lemma}\label{lem:smallest-cube}
  Let \(Î©âŠ‚â„^n\) be compact. Then there exists a half-open cube \(C\) with width \(\operatorname{diam}_âˆ(Î©)\) such that \(Î©âŠ‚\overline{C}\).
\end{lemma}

\begin{proof}[\proofofref{lem:smallest-cube}]
  We can first define \(xâ‰”(\inf_{xâˆˆÎ©} x_i)_{iâˆˆ[n]}âˆˆâ„^n\) since \(Î©\) is compact. We further define \(yâ‰”x+\operatorname{diam}_âˆ(Î©)Â·ğŸ™_n\). We now have \(Câ‰”âŸ¦x,yâ¦†\). Suppose now a point \(zâˆˆÎ©âˆ–\overline{C}\) exists. By definition of \(x\), we have \(xâ‰¤z\). By definition of \(C\) we further get \(z\nleq y\), and therefore \(y_i<z_i\) for an \(iâˆˆ[n]\) by definition of \(C\). But this means \(\norm{x-z}_âˆ>\operatorname{diam}_âˆ(Î©)\).
\end{proof}
\end{comment}

\begin{comment}
\begin{lemma}\label{lem:uniform-cont-â‰¤}
  Let \(M\) be a normed vector space and \(N\) be a metric space. Let \(f:Mâ†’N\) be uniformly continuous with modulus \(Î´\). We then have for every \(Îµ>0\):
  \[ âˆ€_{x,yâˆˆM}(\norm{x-y}_Mâ‰¤Î´(Îµ)â‡’d_N(x,y)â‰¤Îµ). \]
\end{lemma}

\begin{proof}
  Let \(Îµ>0\) with modulus \(Î´(Îµ)\), as well as \(xâˆˆM\) be given. Since \(f\) is uniformly continuous with modulus \(Î´(Îµ)\), we have \(f(B_{Î´,2}(x))âŠ‚B_{Îµ,2}(f(x))\). Since \(f\) is in particular continuous, we get
  \[ f(\overline{B}_{Î´}(x))=f(\overline{B_{Î´,2}(x)})âŠ‚\overline{f(B_{Î´,2}(x))}âŠ‚\overline{B}_{Îµ}(f(x)) \]
  We need to use the fact that \(M\) is a normed vector space in the first equality to get \(\overline{B}_{Î´}(x)=\overline{B_{Î´,2}(x)}\).
\end{proof}
\end{comment}

% DONE: differentiable, but defined on compact subset of euclidean space???
% DONE: instead of fixing \(Îµ^{t}\), get supremum of
% DONE: why Kâ‰ 0?
% CANCELLED: only demand differentiability on CÂ°

We first proof that continuous differentiable functions with uniformly continuous differential can be efficiently approximated by piece-wise linear function. Compare e.g.~\cref{fig:sin-approx-sinus-linear} to~\cref{fig:sin-approx-by-dtlifsnn}

\begin{figure}[h!]
  \centering
  \begin{minipage}{0.55\textwidth}
    \centering
    \input{src/figures/ch03-approx-sinus-linear.tex}
  \end{minipage}
  \caption{Piece-wise affine linear functions approximating the sinus.}
  \label{fig:sin-approx-sinus-linear}
\end{figure}


\begin{lemma}\label{lem:approx-by-lin}
  Let a \(fâˆˆğ’^0(U,â„^n)\) be a continuous function, such that \(f\mid_{UÂ°}âˆˆğ’^1(UÂ°,â„^m)\) is continuously differentiable with a uniformly continuous differential \(d(f|_{UÂ°})\). Let further \(âˆ…â‰ Î©âŠ‚U\) be an arbitrary non-empty subset and \(CâŠ‚â„^n\) a half-open cube such that \(Î©âŠ‚C\) and \((Î©+B_{Ï,2}(0))âˆ©CÂ°âŠ‚UÂ°\) for a \(Ï>0\).
  Furthermore, let \(K\) be defined as in~\cref{thm:approx-snn}.

  For every \(Î¼>0\) we can then compose \(C\) into \(K^n\) half-open subcubes \((C^{(j)})_{jâˆˆ[K^n]}\) such that there exist affine linear functions \(g^{(j)}:C^{(j)}â†’â„^m\) with  \(\norm{d(g^{(j)})}_{âˆ,2}â‰¤\norm{d(f|_{UÂ°})}_{âˆ,2}\) and \(\norm{(f-g)|_Î©}_{âˆ,2}â‰¤Î¼\) for \(gâ‰”\sum_{i=1}^mg^{(j)}Ï‡_{C^{(j)}}\).
\end{lemma}

% MAYBE: remark about why we are using 2-norm

% DONE: fix issue with C^(j) s not completely fitting in open \sqrt{2}Î´(Î¼) balls due to closed part of C^(j)
% CANCELLED: F might not be defined at c^(j), since \(Î©â‰ \overline{C}\)

\begin{proof}[\proofofref{lem:approx-by-lin}]
  Let \(Î¼>0\) be given. Let \(Ï‰\) be a modulus of uniform continuity of \(df\mid_{UÂ°}\).
  We will now partition \(C\) in \(K^n\) half-open subcubes with \(K\) defined as in~\cref{thm:approx-snn}.
  Let \(Î¾,Î¸\) be given, such that the minimum in the definition of \(K\) is attained. Then the subcubes have width
  \[ wâ‰”\frac{\operatorname{diam}_âˆ(C)}{K}â‰¤\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾),Î¸,\frac{Ï}{2}). \]

  Let now \(c^{(j)}\) be the center of \(C^{(j)}\), such that in particular for all \(xâˆˆC^{(j)}\)
  \begin{equation}\label{eq:4}
  \norm{x-c^{(j)}}_2=\sqrt{\sum_{i=1}^n\abs{(x-c^{(j)})_i}^2}â‰¤\frac{w}{2}\sqrt{n}â‰¤\min(Ï‰^{â€ }(Î¾),Î¸,\frac{Ï}{2}).
  \end{equation}

  Let us further define \(g^{(j)}:C^{(j)}â†’â„^m\) by
  \[ g^{(j)}â‰”\begin{cases} xâ†¦f(c^{(j)})+df_{c^{(j)}}(x-c^{(j)}), & C^{(j)}âˆ©Î©â‰ âˆ… \\ xâ†¦0, & C^{(j)}âˆ©Î©=âˆ… \\ \end{cases}. \]
  The first case is well-defined, since \(C^{(j)}âˆ©Î©â‰ âˆ…\) implies that there exists a \(xâˆˆâ„^n\) with \(xâˆˆÎ©\) and \(âˆ€_{yâˆˆC^{(j)}}\norm{x-y}_2â‰¤Ï\) by~\eqref{eq:4}, so \(C^{(j)}âŠ‚(Î©+\overline{B}_{Ï,2}(0))âˆ©C\) and therefore \((C^{(j)})Â°âŠ‚(Î©+\overline{B}_{Ï,2}(0))âˆ©CÂ°âŠ‚UÂ°\).
  Now by definition of \(g^{(j)}\) we already have \(\norm{d(g^{(j)})}_{âˆ,2}=\norm{df_{c^{(j)}}}â‰¤\norm{d(f|_{UÂ°})}_{âˆ,2}\) for \(C^{(j)}âˆ©Î©â‰ âˆ…\) and otherwise \(0=\norm{d(g^{(j)})}_{âˆ,2}â‰¤\norm{d(f|_{UÂ°})}_{âˆ,2}\).

  It only remains to show \(\norm{(f-g^{(j)})|_{Î©âˆ©C^{(j)}}}_âˆ<Î¼\). This is clearly trivial for \(C^{(j)}âˆ©Î©=âˆ…\), so suppose \(C^{(j)}âˆ©Î©â‰ âˆ…\). Let \(xâˆˆC^{(j)}\) and \(h(t)â‰”f(t(x-c^{(j)})+c^{(j)})\). We then have
  \[ h'(t)=(df_{(x-c^{(j)})t+c^{(j)}}âˆ˜d(tâ†¦t(x-c^{(j)})+c^{(j)})_t)(1)=df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)}). \]
  Since \((C^{(j)})Â°âŠ‚UÂ°\), we obtain by the fundamental theorem of calculus
  \begin{align*}
    \norm{f(x)-g^{(j)}(x)}_2 &= \norm{f(x)-f(c^{(j)})-df_{c^{(j)}}(x-c^{(j)})}_2 \\
          &= \norm{h(1)-h(0)-df_{c^{(j)}}(x-c^{(j)})}_2 \\
          &= \norm{\int_0^1df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)})dt-df_{c^{(j)}}(x-c^{(j)})}_2.
  \end{align*}
  Now, due to the generalized Minkowski-Inequality we can move the norm inside the integral:
  %MAYBE: ref for gen. Minkowski-Ineq?
  \begin{align*}
    &â‰¤ \int_0^1\norm{df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)})-df_{c^{(j)}}(x-c^{(j)})}_2dt \\
    &= \int_0^1\norm{(df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}})(x-c^{(j)})}_2dt \\
    &â‰¤ \int_0^1\norm{df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}}}_2\norm{x-c^{(j)}}_2dt \\
    &â‰¤ \int_0^1Î¾\norm{x-c^{(j)}}_2dt \\
    &= Î¾\norm{x-c^{(j)}}_2 \\
    &â‰¤ Î¾Î¸ \\
    &= Î¼
  \end{align*}
  In the fourth step we use \(\norm{df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}}}â‰¤Î¾\), which holds due to \(âˆ€_{tâˆˆ[0,1]}(x-c^{(j)})t+c^{(j)}âˆˆC^{(j)}\),~\eqref{eq:4} and~\cref{lem:inverse-cont-mod}.
  %CANCELLED: proof for cont. extension
\end{proof}
%DONE change notation of c_i to sth clearly not meant component

\begin{proof}[\proofofref{thm:approx-snn}.]
  Let there be \(Îµ,Î¼,Î½>0\) with \(Îµ=Î¼+Î½\).
  %DONE: clarify notation âŸ¦y,zâ¦† (line or proper subspace)
  By~\cref{lem:approx-by-lin} we have a composition \(K^n\) of \(C\) into half-open subcubes \((C^{(j)})_{jâˆˆ[K^n]}\) and linear functions \(g^{(j)}:C^{(j)}â†’â„^m\), such that \(\norm{d(g^{(j)})}_{âˆ,2}â‰¤\norm{df}_{âˆ,2}\) and \(\norm{(f-g)|_{Î©}}_âˆ<Î¼\) for \(gâ‰”\sum_{i=1}^mg^{(j)}Ï‡_{C^{(j)}}\). %CANCELLED: cont ext?

  We will now define a \rdtlifsnn \(Î¦\) with direct input encoding and membrane-potential outputs such that \(\norm{R(Î¦)|_C-g}_âˆ<Î½\).

  Let us first set the basic parameters \(i^{[l]}(0)=0\), \(Î±^{[l]}=0\) and \(Î²^{[l]}=Ï‘^{[l]}=1\) for all layers.

  For our construction and proof we use the following five phases. We define
  \begin{gather*}
    T_1â‰”\rangeI{1}{KT_r},\qquad T_2â‰”\{KT_r\}, \qquad T_3â‰”\{KT_r+1\},\\
    T_4â‰”\{KT_r+2\},\qquad T_5â‰”\rangeI{KT_r+3}{T}.
  \end{gather*}
  Note that \(T_2\) overlaps with \(T_1\). For ease of notation, we will also use \(T_2,T_3,T_4\) as numbers for their respective element, e.g.~\(T_2\) for \(KT_r\).

  During the following proof it will be helpful to look at~\cref{fig:ch03-whole-network} and~\cref{fig:ch03-group-network} to get a visual feeling for the constructed network. For understanding the procedure of the network it will be useful to compare the results with the timelines in~\cref{fig:timeline-first-layer} and~\cref{fig:timeline-second-layer}.

  \begin{figure}[h!]
    \centering
    \input{src/figures/ch03-whole-network}
    \caption{Structure of the whole network}
    \label{fig:ch03-whole-network}
  \end{figure}

  \begin{figure}[h!]
    \centering
    \input{src/figures/ch03-group-network}
    \caption{Structure of the network, focused on the \(j\)-th group of the second layer}
    \label{fig:ch03-group-network}
  \end{figure}

  The higher level idea is that we accumulate the state during \(T_1\), compute the subregion \(C^{(j)}\) of the input during \(T_2,â€¦,T_4\) and flush out the position of the input inside of \(C^{(j)}\) during \(T_5\).
  %DONE:: replace j by 1 in 3.5

  The constructed first layer is only active during the first phase, \(T_1\). It is composed of \(n+1\) neurons, where the first \(n\) neuron convert the input vector regarding its position in \(C\) into spike trains. The last neuron, the â€œalarm clockâ€, shuts down the first layer after \(T_1\) ends.

  During \(T_1âˆ–T_2\) the second layer only accumulates state without spiking. Then during \(T_2âˆªT_3âˆªT_4\) the network decides in which region \(C^{(j)}\) the input \(x\) is located; during \(T_4âˆªT_5\) the location of \(x\) in \(C^{(j)}\) as well as \(x\) being located in \(C^{(j)}\) is encoded as spike trains.

  For each region \(C^{(j)}\) we have \(n+1\)-neurons in the second layer. Each of the first \(n\) neurons encodes a component of the linear part of \(g^{(j)}\). They are also used to inform the \(n+1\)-th neuron of the group if the \(x\) is at least as big as the base point of \(C^{(j)}\). The \(n+1\)-th neuron deactivates all other neurons of regions with smaller base point and encodes the constant part of \(g^{(j)}\). Apart from the neuron groups, the last \(3\) neurons in the second layer act as â€œclock neuronsâ€ enabling and disabling the other ones.

  % In the second layer we have \(m+1\)-neurons for each affine linear region \(C^{(j)}\). Each of the first \(m\) neurons encodes the linear part of a component of the output vector. The last, additional neuron acts as an activator to the specific region by recursively enabling the other neurons of this region and disabling all regions with smaller base point (that would otherwise be enabled).
  
  %DONE: properly label/name and ref graphics
  \input{src/figures/ch03-approx-first-layer-timeline}
  \begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \begin{timeline}
        \SpikeOutAt{$(T2)$}
        \SpikeOutAt{$(T3)$}
        \SpikeOutAt{$(T4)$}
        \SpikeOutAt{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt{$(T)$}
      \end{timeline}
      
      \caption{$a_1$}
      \label{fig:timeline-a1}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[][$x_i$]{$(O)+(0.3,0)$}
        \SpikeOut[dashed]{$(O)+(0.6,0)$}{}
        \dotsBtSp{$(O)+(0.8,0)$}{$(T2)+(-0.8,0)$}
        \SpikeIn[][$x_i$]{$(T2)+(-0.6,0)$}
        \SpikeOut[dashed]{$(T2)+(-0.3,0)$}

        \SpikeIn[][$x_i$]{$(T3)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T3)+(-1.3,0)$}

        \SpikeIn[][$x_i$]{$(T4)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T4)+(-1.3,0)$}

        \SpikeIn[][$x_i$]{$(T4+1)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T4+1)+(-1.3,0)$}

        \dotsBtSp{$(T4+1)-(1.1,0)$}{$(T)+(-1,0)$}

        \SpikeIn[][$x_i$]{$(T)+(-0.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T)+(-0.3,0)$}
      \end{timeline}
      
      \caption{$i$-th neuron}
      \label{fig:timeline-i}
    \end{subfigure}
    \caption{Timelines of first layer neurons. Upward arrows depict outgoing spikes, downward arrows depict ingoing spikes of the given neuron. Solid arrows depict certainly occurring spikes, dashed arrows depict possibly occurring spikes.}
    \label{fig:timeline-first-layer}
  \end{figure}
  

  \begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T2-1)$}
      \end{timeline}
      
      \caption{$c_1$}
      \label{fig:timeline-c_1}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T2)$}
      \end{timeline}
      
      \caption{$c_2$}
      \label{fig:timeline-c2}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T4)$}
        \SpikeOutAt{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt{$(T)$}
      \end{timeline}
      \caption{$a_2$}
      \label{fig:timeline-a2}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[double][$\sum_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}$][]{$(T3)-(1.7,0)$}
        \SpikeOut[dashed][][$x^{C^{(j)}}{â‰¤}x$]{$(T3)-(0.3,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$][]{$(T4)-(1.5,0)$}
        \SpikeOut[dashed][][$x{âˆˆ}C^{(j)}$]{$(T4)-(0.2,0)$}

        \SpikeIn[][$s_{a_2}^{[1]}$]{$(T4+1)+(-1.7,0)$}

        \dotsBtSp{$(T4+1)-(1.5,0)$}{$(T)+(-0.5,0)$}

        \SpikeIn[][$s_{a_2}^{[1]}$]{$(T)+(-0.3,0)$}
      \end{timeline}
      \caption{$Ï‰_j$}
      \label{fig:timeline-ommega}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[][$x_i$]{$(O)+(0.3,0)$}
        \dotsBtSp[12]{$(O)+(0.5,0)$}{$(T2)+(-2,0)$}
        \SpikeIn[][$x_i$]{$(T2)+(-1.8,0)$}
        \SpikeIn[][$s^{[2]}_{c_1}$]{$(T2)+(-1.3,0)$}
        \SpikeIn[][$s^{[2]}_{c_2}$]{$(T3)+(-1.7,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$]{$(T4)+(-1.7,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$]{$(T4+1)+(-1.7,0)$}

        \SpikeOut[dashed][][$x^{C^{(j)}}_i{â‰¤}x_i$]{$(T2)+(-0.2,0)$}

        \SpikeOutAt[dashed]{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt[dashed]{$(T)$}
        % \draw [decorate,decoration={brace,amplitude=6pt}] ($(T4+1)+(-0.4,1.6)$) -- ($(T)+(-0.2,1.6)$) node[midway,above=6pt]{\small If \(xâˆˆC_j\), };
      \end{timeline}
      \caption{$Î¹_j(i)$}
      \label{fig:timeline-iota_i}
    \end{subfigure}
    \caption{Timelines of second layer neurons. A few possible outgoing spikes are labeled with their respective condition in gray. Thick arrows depict possible input spikes from multiple neurons.}
    \label{fig:timeline-second-layer}
  \end{figure}

  To obtain the normalized location of a value in \(C\) we will often write \(o_i(z)\) for
  \[ o_i(z)â‰”\frac{z_i-x_i^C}{y_i^C-x_i^C} \]
  with \(iâˆˆ[n]\) in the following.

  Consider now the following rigorous construction.
  \begin{enumerate}
  \item \textbf{First layer}:
  We define the \textbf{\(i\)-th neuron} of the \(n\) neurons of the first layer with parameters using the notation from~\cref{def:ch02-def-neuron}:
  \begin{equation}
    w=\frac{1}{y^C_i-x^C_i}e_i, \quad b=-\frac{x^C_i}{y^C_i-x^C_i}
    , \quad v=-e_{a_1},\quad u_0=0, \quad i_0=0.\tag{\(i\)}
  \end{equation}
  The \textbf{â€œalarm neuronâ€} of the first layer, with index \(a_1â‰”n+1\), is defined by:
  \begin{equation}
    w=0, \quad b=\frac{1}{T_2}, \quad v=e_{a_1}, \quad u_0=0, \quad i_0=0.\tag{\(a_1\)}
  \end{equation}

  % CANCELLED: check that proof works for xâˆˆ\(\overline{C}âˆ–C\)

  % We further define the \(n+1\)-th neuron of the first layer by \(w=0\), \(b=\frac{1}{K-1}\), \(v=\)
  % By~\cref{lem:non-recursive-defs} we get

  % DONE: proof correct behaviour

  %DONE: a_2 is used wrongly
  %DONE: add timeline
  \item \textbf{Second layer}:
  Let us now construct the second layer in the following way: For each of the \(K^n\) subcubes in \(C\) we define \(n+1\) neurons like so: Let \(C^{(j)}=âŸ¦x^{C^{(j)}},y^{C^{(j)}}â¦†\) be one such subcube with position \(q^{(j)}âˆˆ([K-1]_0)^n\) in \(C\), i.e.
  \[q^{(j)}_i=Ko_i(x^{C^{(j)}}),\quad q^{(j)}_i+1=Ko_i(y^{C^{(j)}})\quad\text{for all }iâˆˆ[n].\]
  We will write \(Î¹_j(i)â‰”j(n+1)+i\) to index the first \(n\) neurons in the layer and \(Ï‰_jâ‰”(j+1)(n+1)\) to index the last neuron of each group.

  The \textbf{\(i\)-th neuron} of the first \(n\) neurons (of the \(j\)-th group), with index \(Î¹_j(i)\) in the second layer, has the parameters
  \begin{equation}
  \begin{gathered}
    w=e_i, \quad b=0, \quad v=T(e_{c_1}-2e_{c_2}+r(q^{(j)})), \\
    u_0=-q^{(j)}_iT_r-T+1, \quad i_0=0.
  \end{gathered}\tag{\(Î¹_j(i)\)}
  \end{equation}
  where
  \[ r(q)â‰”e_{Ï‰_{j(q)}}-\sum_{\substack{q'âˆˆ[K-1]_0^n \\ q<q'}}e_{Ï‰_{j(q')}} \]
  is â€œthe switchâ€ and \(j(q)\) is the index of the subcube at position \(q\). We further define the applied variant
  \[ r_s(q;t)â‰”âŸ¨r(q),s^{[2]}(t)âŸ©=s^{[2]}_{Ï‰_{j(q)}}(t)-\sum_{\substack{q'âˆˆ[K-1]_0^n \\ q<q'}}s^{[2]}_{Ï‰_{j(q')}}(t). \]
  The \textbf{final neuron} of the group, with index \(Ï‰_j\) in its layer, has the parameters
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=0, \quad v=\frac{1}{n}\sum_{i=1}^ne_{Î¹_j(i)}-2e_{a_2}+r(q^{(j)}), \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(Ï‰_j\)}
  \end{equation}
  We also define the two \textbf{â€œclock neuronsâ€}, with index \(c_1â‰”(n+1)K^n+1\) and \(c_2â‰”(n+1)K^n+2\) with parameters
  %DONE: T_2=1?
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=b_{c_i}, \quad v=-(T-1)e_{c_i}, \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(c_1,c_2\)}
  \end{equation}
  where \(b_{c_1}=\frac{1}{T_2-1}\) and \(b_{c_2}=\frac{1}{T_2}\).
  Finally, the \textbf{â€œalarm neuronâ€}, with index \(a_2â‰”(j+1)K^n(Î¼)+3\), is defined by
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=\frac{1}{T_4}, \quad v=e_{a_2}, \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(a_2\)}
  \end{equation}
  %DONE: use c_{-1}, c_0 and c_{1} and a (for alarm) for the first layer neuron.
  \item Output decoder:
    We define the parameters of the output decoder by \(a_t=0\), for \(tâ‰¤T_3\) and otherwise \(a_t=1\). We further set \(b^{[L+1]}=0\) and
    \[ W^{[L+1]}_{k,Î¹_j(i)}=d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{1}{T_r}e_i) \]
    for \(kâˆˆ[m]\), \(jâˆˆK^n\) and \(iâˆˆ[n]\). Note that \(d(g^{(j)})\) is in particular the linear part of \(g^{(j)}\), i.e. \(g^{(j)}(x)=d(g^{(j)})(x-x^{C^{(j)}})+g(x^{C^{(j)}})\) for all \(xâˆˆC^{(j)}\). We moreover set
    \[ W^{[L+1]}_{k,Ï‰_j}=g_k(x^{C^{(j)}}) \]
    for \(kâˆˆ[m]\) and \(jâˆˆK^n\). We finally define \(W^{[L+1]}_{k,c_i}=0\) for \(iâˆˆ\{2..4\}\).
    %DONE: indices in correct order?
    %DONE: make easier to read
  \end{enumerate}

  We will now prove that this construction indeed approximates \(g\) up to a precision of \(Î½\). It will be helpful to consider the following simplified equations, which follow by choice of \(i^{[l]},Î±^{[l]},Î²^{[l]},Ï‘^{[l]}\):
  \begin{align*}
    i^{[l]}(t) & = W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1), \\
    p^{[l]}(t) & = u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]}, \\
    s^{[l]}(t) & = H(p^{[l]}(t)-\oneV{n_l}), \\
    u^{[l]}(t) & = p^{[l]}(t)-s^{[l]}(t)
  \end{align*}
  and in particular by~\cref{lem:non-recursive-defs}
  \begin{align*}
   p^{[l]}(t) &= u^{[l]}(0)+\sum_{k=1}^t\left(i^{[l]}(t)+b^{[l]}\right)-\sum_{k=1}^{t-1}s^{[l]}(k).
  \end{align*}

  Let now \(s^{[0]}(t)=xâˆˆC\). We will prove \(\norm{R(Î¦)(x)-g(x)}_{âˆ,2}â‰¤Î½\) in four steps, beginning with a characterization of the first layer.
  \begin{enumerate}
  \item Characterization of the â€œalarm neuronâ€ \(a_1\):\\
  Let us first regard the neuron in the first layer: By choice of parameters we get:
  \begin{equation*}
   p^{[1]}_{a_1}(t) = \frac{t}{T_2}+\sum_{k=1}^ts^{[1]}_{a_1}(k-1)-\sum_{k=1}^{t-1}s^{[1]}_{a_1}(k) = \frac{t}{T_2}
  \end{equation*}
  So \(s^{[1]}_{a_1}(t)=1â‡”tâ‰¥T_2\).
  \item Characterization of \(i\)-th neuron, \(iâˆˆ[n]\):\\
  We have
  \begin{align*}
   i^{[1]}_i(t)+b^{[1]}_i=\frac{x_i-x^C_i}{y^C_i-x^C_i}-s^{[1]}_{a_1}(t-1)=o_i(x)-s^{[1]}_{a_1}(t-1).
  \end{align*}
  So in particular
  \begin{align*}
   0â‰¤i^{[1]}_i(t)+b^{[1]}_i=o_i(x)â‰¤1
  \end{align*}
  for \(tâˆˆT_1\), since \(x^C_iâ‰¤x_i<y_i^C\).
  Due to \(0â‰¤u^{[1]}_i(0)=0<1\) we can use~\cref{lem:sum-spikes-slowly-through-time} with \(t_0â‰”0\) and \(t_{Ï‰}â‰”T_2\) to obtain
  \begin{equation*}
  \left\lfloor T_2o_i(x) \right\rfloor =\left\lfloor u^{[1]}_i(0)+\sum_{t=1}^{T_2}(i^{[1]}_i(t)+b^{[1]}_i) \right\rfloor = \sum_{t=1}^{T_2}s^{[1]}_i(t).
  \end{equation*}
  
  We further have
  \begin{equation*}
   i^{[1]}_i(t)+b^{[1]}_i=o_i(x)-s^{[1]}_{a_1}(t-1)=o_i(x)-1<0
  \end{equation*}
  for \(t>T_2\), so we get
  \[p_i^{[1]}(t)=u_i^{[1]}(T_2)+\sum_{k=T_3}^t\left(i^{[1]}_i(k)+b^{[1]}_i\right)-\sum_{k=T_3}^{t-1}s^{[1]}(k) <1\]
  and therefore \(s_i^{[1]}(t)=0\) for \(t>T_2\).

  % So in particular
  % \begin{equation}\label{eq:3}
  % \left\lfloor T_2\frac{x_i-x^C_i}{y^C_i-x^C_i} \right\rfloor = \sum_{t=1}^{T_2}s^{[1]}_i(t) = \sum_{t=1}^Ts^{[1]}_i(t)
  % \end{equation}

  % The equation clearly also holds for \(x_i=y_i\).
  \end{enumerate}
  
  \noindent We will now continue with characterizing the second layer. We start with the â€œclock neuronsâ€ and the â€œalarm neuronâ€.
  
  \begin{enumerate}
  \item Characterization of the â€œclock neuronsâ€: \\
  In contrast to the â€œalarm neuronâ€ of the first layer, the two â€œclockâ€ neurons only fire once,
  \begin{equation*}
   p^{[2]}_{c_i}(t) = tb_{c_i}-(T-1)\sum_{k=1}^ts^{[2]}_{c_i}(k-1)-\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k)=tb_{c_i}-T\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k).
  \end{equation*}
  %DONE: proof t<3(T_2-1)
  Let us first consider \(c_1\): We clearly have \(p^{[2]}_{c_1}(t)<1\) for \(t<T_2-1\), but \(p^{[2]}_{c_1}(T_2-1)=1\).
  Since by definition \(Kâ‰¥1\) and \(T_râ‰¥2\) and in particular \(T_2=KT_râ‰¥2\), we have \(tâ‰¤Tâ‰¤T(T_2-1)\). Therefore \(p^{[2]}_{c_1}(t)<1\) for \(t>T_2-1\) due to \(s^{[2]}_{c_1}(T_2-1)=1\). So \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_1}(t)=Ï‡_{\{T_2-1\}}(t)\).

  We similarly obtain \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_2}(t)=Ï‡_{T_2}(t)\).
  \item Characterization of the â€œalarm neuronâ€:\\
  Just like for \(a_1\), we also get \(s^{[1]}_{a_2}(t)=1â‡”tâ‰¥T_4\).
  \end{enumerate}

  \noindent We now prove the behavior of the remaining neurons in the second layer step by step throughout the phases.
  \begin{enumerate}
  \item Phase 1 \\
  We will show \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(t)=0\) and \(s^{[2]}_{Ï‰_j}(t)=0\) for all \(jâˆˆ[K^n]\) and \(tâˆˆ[T_2-1]_0\) by induction over \(t\). Let \(t=0\). We then get \(s^{[2]}_{Î¹_j(i)}(0)=s^{[2]}_{Ï‰_j}(0)=0\) by definition.
  Let further \(1â‰¤tâ‰¤T_2-1\). First notice that by the induction hypothesis, we have \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(k)=0\) and \(r_s(q^{(j)};k)=0\) for \(k<t\). It follows that
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(k)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(k)+T(s^{[2]}_{c_1}(k-1)-2s^{[2]}_{c_2}(k-1)+r_s(q^{(j)};k-1))=s^{[1]}_i(k), \\
   i^{[2]}_{Ï‰_j}(k)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(k-1)-2s^{[2]}_{a_2}(k-1)+r_s(q^{(j)};k-1)=0
  \end{align*}
  for all \(kâ‰¤t\). Thus, we further get
  \begin{align*}%MAYBE: t' anstatt k
   p^{[2]}_{Î¹_j(i)}(t) &= -q^{(j)}_iT_r-T+1+\sum_{k=1}^ts^{[1]}_i(k)-\sum_{k=1}^{t-1}s^{[2]}_{Î¹_j(i)}(k)= -q^{(j)}_iT_r-T+1+\sum_{k=1}^ts^{[1]}_i(k),\\
   p^{[2]}_{Ï‰_j}(t) &= \sum_{k=1}^t0-\sum_{k=1}^{t-1}s^{[2]}_{Ï‰_j}(k) = 0.
  \end{align*}
   Since \(\sum_{k=1}^ts^{[1]}_i(k)â‰¤T_2-1<T-1\) and \(s^{[2]}_{c_1}(t)=Ï‡_{\{T_2-1\}}(t)\), we get \(p^{[2]}_{Î¹_j(i)}(t)â‰¤0\). Thus \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(t)=0\) and \(s^{[2]}_{Ï‰_j}(t)=0\).
  \item Phase 2 \\
  Just as in phase 1, we have
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_2)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2-1)-2s^{[2]}_{a_2}(T_2-1)+r_s(q^{(j)};T_2-1)=0.
  \end{align*}
  Thus we also get \(p^{[2]}_{Ï‰_j}(T_2)=0\) and \(s^{[2]}_{Ï‰_j}(T_2)=0\). This differs for \(Î¹_j(i)\) due to the dependence on \(c_1\). Let \(jâˆˆ[K^n]\) and \(iâˆˆ[n]\) be given. The neuron with index \(Î¹_j(i)\) then fires exactly at \(T_2\), if \(x_iâ‰¥x^{C^{(j)}}_i\):
  First notice that
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_2)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_2)+T(s^{[2]}_{c_1}(T_2-1)-2s^{[2]}_{c_2}(T_2-1)+r_s(q^{(j)};T_2-1))=s^{[1]}_i(T_2)+T,
  \end{align*}
  and conclude that
  \begin{align*}
   p^{[2]}_{Î¹_j(i)}(T_2) &= p^{[2]}_{Î¹_j(i)}(T_2-1) + i^{[2]}_{Î¹_j(i)}(T_2)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_2-1) \\
                         &= -q^{(j)}_iT_r+1+\sum_{k=1}^{T_2}s^{[1]}_i(k) \\
                         &= -q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor
  \end{align*}
  holds using the characterization of layer 1. Therefore, we have \(s^{[2]}_{Î¹_j(i)}(T_2)=1\) exactly if \(q^{(j)}_iT_râ‰¤T_2o_i(x)\), which is equivalent to \(\frac{q^{(j)}_i}{K}(y^C_i-x^C_i)+x_i^Câ‰¤x_i\) by definition of \(o_i\). Further \(\frac{q^{(j)}_i}{K}(y^C_i-x^C_i)+x_i^C\) is equal to \(x^{C^{(j)}}\) by definition of \(q^{(j)}\). So \(s^{[2]}_{Î¹_j(i)}(T_2)=1\) holds exactly if \(x^{C^{(j)}}_iâ‰¤x_i\).
  \item Phase 3 \\
  The â€œactivator neuronâ€ \(Ï‰_j\) fires at \(T_3\) if and only if \(x^{C^{(j)}}â‰¤x\): First, notice
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_3)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2)-2s^{[2]}_{a_2}(T_2)+r_s(q^{(j)};T_2)=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2)
  \end{align*}
  from which we derive
  \begin{align*}
   p^{[2]}_{Ï‰_j}(T_3) &= p^{[2]}_{Ï‰_j}(T_2) + i^{[2]}_{Ï‰_j}(T_3)+b^{[2]}_{Ï‰_j} - s^{[2]}_{Ï‰_j}(T_2) = \frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2).
  \end{align*}
  So \(0â‰¤p^{[2]}_{Ï‰_j}(T_3)â‰¤1\) and we get \(p^{[2]}_{Ï‰_j}(T_3)=1\), as well as \(s^{[2]}_{Ï‰_j}(T_3)=1\) exactly if \(âˆ€_{iâˆˆ[n]}x^{C^{(j)}}_iâ‰¤x_i\), i.e. if \(x^{C^{(j)}}â‰¤x\).

  Let further \(iâˆˆ[n]\). We then have
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_3)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_3)+T(s^{[2]}_{c_1}(T_2)-2s^{[2]}_{c_2}(T_2)+r_s(q^{(j)};T_2))=-2T
  \end{align*}
  and therefore
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(T_3) &= p^{[2]}_{Î¹_j(i)}(T_2)+i^{[2]}_{Î¹_j(i)}(T_3)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_2) \\
                          &= -q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor-2T - s^{[2]}_{Î¹_j(i)}(T_2).
  \end{align*}
  Thus \(p^{[2]}_{Î¹_j(i)}(T_3)â‰¤-T\) and \(s^{[2]}_{Î¹_j(i)}(T_3)=0\), since \(\left\lfloor T_2o_i(x) \right\rfloorâ‰¤T_2â‰¤T-1\).
  \item Phase 4 \\
  Let \(iâˆˆ[n]\). The neuron \(Î¹_j(i)\) stays inactive at \(T_4\), since
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_4)+T(s^{[2]}_{c_1}(T_3)-2s^{[2]}_{c_2}(T_3)+r_s(q^{(j)};T_3))=Tr_s(q^{(j)};T_3)
  \end{align*}
  and hence
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(T_4) &= p^{[2]}_{Î¹_j(i)}(T_3)+i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_3) \\
                        &= p^{[2]}_{Î¹_j(i)}(T_3)+Tr_s(q^{(j)};T_3).
  \end{align*}
  Since \(p^{[2]}_{Î¹_j(i)}(T_3)â‰¤-T\), we conclude \(p^{[2]}_{Î¹_j(i)}(T_4)â‰¤0\) and \(s^{[2]}_{Î¹_j(i)}(T_4)=0\).

  Further the â€œactivator neuronâ€ \(Ï‰_j\) fires at \(T_4\) exactly if \(xâˆˆC^{(j)}\): First, notice that
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_4)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_3)-2s^{[2]}_{a_2}(T_3)+r_s(q^{(j)};T_3)=r_s(q^{(j)};T_3).
  \end{align*}
  We thus have
  \begin{align*}
    p^{[2]}_{Ï‰_j}(T_4) &= p^{[2]}_{Ï‰_j}(T_3)+i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_3) \\
                       &= p^{[2]}_{Ï‰_j}(T_3)+r_s(q^{(j)};T_3) - s^{[2]}_{Î¹_j(i)}(T_3).
  \end{align*}
  By definition of \(q\), \(q^{(j)}_i<q^{(j')}_i\) holds exactly if \(x^{C^{(j)}}_i<x^{C^{(j')}}_i\) for all \(iâˆˆ[n]\) and \(j,j'âˆˆ[K^n]\). We thus conclude \(âˆ€_{j,j'âˆˆ[K^n]}q^{(j)}<q^{(j')}â‡”x^{C^{(j)}}<x^{C^{(j')}}\).
  Note that we have shown \(âˆ€_{j'âˆˆK^n}s^{[2]}_{Ï‰_{j'}}(T_3)=1â‡”x^{C^{(j')}}â‰¤x\) before. We therefore have
  \begin{align*}
  r_s(q^{(j)};T_3)&=s^{[2]}_{Ï‰_j}(T_3)-\sum_{\substack{q'âˆˆ([K-1]_0)^{n} \\ q^{(j)}<q'}}s^{[2]}_{Ï‰_{j(q')}}(T_3) \\
                 &=s^{[2]}_{Ï‰_j}(T_3)-\sum_{\substack{j'âˆˆ[K^n] \\ x^{C^{(j)}}<x^{C^{(j')}}â‰¤x}}1.
  \end{align*}%DONE: n_1 vs n

  Now if \(xâˆˆC^{(j)}\), i.e. \(x^{C^{(j)}}â‰¤x<y^{C^{(j)}}\), then \(s^{[2]}_{Ï‰_j}(T_3)=1\). If there also was a \(j'\) with \(s^{[2]}_{Ï‰_{j'}}(T_3)=1\) and \(x^{C^{(j)}}<x^{C^{(j')}}â‰¤x\), we would further get \(x^{C^{(j)}}<x^{C^{(j(q'))}}<y^{C^{(j)}}\) due to \(x<y^{C^{(j)}}\). But this contradicts the construction of the subcubes \((C^{(j)})_{jâˆˆ[K^n]}\).
  
  We have also shown \(p^{[2]}_{Ï‰_j}(T_3)=1\) and \(s^{[2]}_{Ï‰_j}(T_3)=1\) for this case, so we can conclude \(p^{[2]}_{Ï‰_j}(T_4)=1\) as well as \(s^{[2]}_{Ï‰_j}(T_4)=1\).

  Now suppose \(xâˆ‰C^{(j)}\). Since we assumed \(xâˆˆC\) and constructed the subcubes \((C^{(j)})_{jâˆˆ[K^n]}\) as a partition of \(C\), we have \(xâˆˆC^{(j')}\) for some \(j'â‰ j\).

  Now if \(x^{C^{(j)}}<x^{C^{(j')}}\), then \(r_s(q^{(j)};T_3)â‰¤0\). In this case we also have \(p^{[2]}_{Ï‰_j}(T_3)=1\) and \(s^{[2]}_{Ï‰_j}(T_3)=1\) such that \(p^{[2]}_{Ï‰_j}(T_4)â‰¤0\) and \(s^{[2]}_{Ï‰_j}(T_4)=0\).

  Is on the other hand \(x^{C^{(j)}}â‰®x^{C^{(j')}}\), then there is an index \(iâˆˆ[n]\) such that \(x^{C^{(j')}}_i<x^{C^{(j)}}_i\) and therefore \(x_i<y^{C^{(j')}}_iâ‰¤x^{C^{(j)}}_i\). This implies \(x^{C^{(j)}}\nleq x\) and we get in particular \(r_s(q^{(j)};T_3)â‰¤0\). We thus have \(p^{[2]}_{Ï‰_j}(T_3)<1\) and \(s^{[2]}_{Ï‰_j}(T_3)=0\) such that \(p^{[2]}_{Ï‰_j}(T_4)<1\) and \(s^{[2]}_{Ï‰_j}(T_4)=0\).

  To summarize, \(s^{[2]}_{Ï‰_j}(T_4)=1\) exactly if \(xâˆˆC^{(j)}\) just as we claimed and in general \(p^{[2]}_{Ï‰_j}(T_4)â‰¤1\).
  %DONE: more rigorous?
  \item Phase 5 \\
  The â€œactivator neuronâ€ \(Ï‰_j\) is inactive during \(T_5\), since for \(t>T_4\)
  \begin{align*}
   i^{[2]}_{Ï‰_j}(t)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(t-1)-2s^{[2]}_{a_2}(t-1)+r_s(q^{(j)};t-1)â‰¤0
  \end{align*}
  and
  \begin{align*}
     p^{[2]}_{Ï‰_j}(t) &= u^{[2]}_{Ï‰_j}(T_4)+\sum_{k=T_4+1}^{t}\left(i^{[2]}_{Ï‰_j}(k)+b^{[2]}_{Ï‰_j}\right)-\sum_{k=T_4+1}^{t-1}s^{[2]}_{Ï‰_j}(k) \\
                      &â‰¤ u^{[2]}_{Ï‰_j}(T_4). \\
  \end{align*}
  Further \(u^{[2]}_{Ï‰_j}(T_4)<1\), since \(p^{[2]}_{Ï‰_j}(T_4)â‰¤1<2\). So \(âˆ€_{t>T_4}s^{[2]}_{Ï‰_j}(t)=0\). In particular the â€œswitchâ€ \(r_s(q^{(j)};t)=0\) is off for all \(jâˆˆ[K^n]\) and \(t>T_4\).
  
  Let \(iâˆˆ[n]\). We show next that during \(T_5\) the neuron \(Î¹_j(i)\) captures the position of \(x_i\) in \([x^{C^{(j)}}_i,y^{C^{(j)}}_i)\) if \(xâˆˆC^{(j)}\) and stays inactive otherwise.

  Let us first assume \(xâˆ‰C^{(j)}\). We have previously shown \(s^{[2]}_{Ï‰_j}(T_4)=0\) and just now \(âˆ€_{t>T_4}s^{[2]}_{Ï‰_j}(t)=0\). So \(âˆ€_{tâ‰¥T_4}r_s(q^{(j)};t)=0\) and therefore for all \(t>T_4\)
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(t)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(t)+T(s^{[2]}_{c_1}(t-1)-2s^{[2]}_{c_2}(t-1)+r_s(q^{(j)};t-1))â‰¤0
  \end{align*}
  as well as
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(t) &= p^{[2]}_{Î¹_j(i)}(T_4)+\sum_{k=T_4+1}^t\left(i^{[2]}_{Î¹_j(i)}(k)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(k-1)\right) \\
                        &= p^{[2]}_{Î¹_j(i)}(T_4)+\sum_{k=T_4+1}^t\left(i^{[2]}_{Î¹_j(i)}(k)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(k-1)\right) \\
                      &â‰¤p^{[2]}_{Î¹_j(i)}(T_4) \\
                      &â‰¤0. \\
  \end{align*}
  So in particular \(âˆ€_{t>T_4}s^{[2]}_{Î¹_j(i)}(t)=0\).

  Suppose now \(xâˆˆC^{(j)}\). As we have seen before, we have \(r_s(q^{(j)};T_4)=1\) and \(âˆ€_{t>T_4}r_s(q^{(j)};t)=0\).
  Hence
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_4+1)+T(s^{[2]}_{c_1}(T_4)-2s^{[2]}_{c_2}(T_4)+r_s(q^{(j)};T_4)) = T
  \end{align*}
  and for all \(t>T_4+1\)
  \begin{align*}
    i_{Î¹_j(i)}^{[2]}(t)+b^{[2]}_{Î¹_j(i)}=&s^{[1]}_i(t)+T(s^{[2]}_{c_1}(t-1)-2s^{[2]}_{c_2}(t-1)+r_s(q^{(j)};t-1))=0.
  \end{align*}
    
  Using previous results and in particular \(s^{[2]}_{Î¹_j(i)}(T_2)=1â‡”x^{C^{(j)}}_iâ‰¤x_i\), we obtain
  \begin{align*}
    p_{Î¹_j(i)}^{[2]}(T_4+1)&=p_{i_j(i)}^{[2]}(T_4)+i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(T_4) \\
                           &=p^{[2]}_{Î¹_j(i)}(T_3)+Tr_s(q^{(j)};T_3)+T \\
                           &=-q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor-2T - s^{[2]}_{Î¹_j(i)}(T_2)+2T \\
                           &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor.
  \end{align*}
  We now have \(KT_ro_i(x^{C^{(j)}})â‰¤KT_ro_i(x) \) due to \(x^{C^{(j)}}_iâ‰¤x_i\) and thus \(p_{Î¹_j(i)}^{[2]}(T_4+1)â‰¥0\).

  Furthermore,
  \[ o_i(y^{C^{(j)}})-o_i(x^{C^{(j)}})=\frac{y_i^{C^{(j)}}-x_i^{C^{(j)}}}{y_i^C-x_i^C}=\frac{1}{K} \]
  and \(o_i(x)<o_i(y^{C^{(j)}})=o_i(x^{C^{(j)}})+\frac{1}{K}\) for all \(xâˆˆC^{(j)}\). We can deduce
  \begin{align*}
    p_{Î¹_j(i)}^{[2]}(T_4+1) &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor \\
    &â‰¤-KT_ro_i(x^{C^{(j)}})+KT_ro_i(y^{C^{(j)}}) \\
    &â‰¤-KT_ro_i(x^{C^{(j)}})+T_r+KT_ro_i(x^{C^{(j)}}) \\
    &â‰¤T_r
  \end{align*}
  and have thus shown
  \[ 0â‰¤p_{Î¹_j(i)}^{[2]}(T_4+1)=u_{i_j(i)}^{[2]}(T_4)+i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}â‰¤T_r=T-(T_4+1). \]
  By~\autoref{lem:pos-input-pos-res-pos} we get \(u^{[2]}_j(T_4+1)â‰¥0\) and can therefore use~\autoref{lem:sum-spikes-decaying} with \(t_0â‰”T_4+1\), \(t_mâ‰”T_4+1\) and \(t_Ï‰â‰”T\) to obtain
  \begin{align*}
    \sum_{t=T_4+1}^Ts^{[2]}_{Î¹_{j(i)}}(t)&=\left\lfloor u^{[2]}_{Î¹_{j(i)}}(T_4)+i^{[2]}_{Î¹_{j(i)}}(T_4+1)+b^{[2]}_{Î¹_{j(i)}} \right\rfloor \\
                                         &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor.
  \end{align*}
  \end{enumerate}

  \noindent We have now reached the final step, where we will show that the spikes actually approximate \(g\). Let us consolidate our results. For \(j\) with \(xâˆˆC^{(j)}\) we have
  \begin{align*}
    &\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t)=\sum_{t=T_4+1}^Ts^{[2]}_{Î¹_{j(i)}}(t)= -KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor, \\
    &\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)=s^{[2]}_{Ï‰_j}(T_4)=1.
  \end{align*}
  And therefore for all \(kâˆˆ[m]\) %DONE: split into parts
  \begin{align*}
   W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t) &= g_k(x^{C^{(j)}}), \\
   W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t) &= \left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x)\right\rfloor\right)d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\tfrac{1}{T_r}e_i).
  \end{align*}
    %CANCELLED: write from which steps
    %MAYBE: make dependence on _i explicit
    %DONE: introduce notation _k for functions
  Let now \(jâˆˆ[K^n]\) be such that \(xâˆ‰C^{(j)}\), in that case
  \begin{align*}
    \sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t)=0,\qquad \sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)=0.
  \end{align*}
  Thus the group of neurons with index \(j\) does not contribute to the output of the network,
  \begin{align*}
   W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t) = 0,\qquad W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t) = 0.
  \end{align*}
  Finally note that by choice of \(W^{[L+1]}\), the neurons \(c_1,c_2,a_2\) don't contribute to the output as well. For any \(jâˆˆ\{c_1,c_2,a_2\}\)
  \[ W^{[L+1]}_{k,j}\sum_{t=T_4}^Ts^{[2]}_j(t)=0. \]
          %DONE: introduce W_k,j?
          %CANCELLED: g_k wiederspricht hier vorheriger notation g_j
          %DONE: notation e_i
  Let now \(jâˆˆ[K^n]\) be such that \(xâˆˆC^{(j)}\). We consequently have
  \[ R(Î¦)_k(x)=\sum_{t=T_4}^T(W^{[L+1]}s^{[2]}(t))_k= W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)+W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t), \]
  which is further equal to
  \begin{align*}
    & g_k(x^{C^{(j)}})+\sum_{iâˆˆ[n]}\left(d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{1}{T_r}e_i)\right)\left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor\right) \\
    &= g_k(\underset{â‰•x'}{\underbrace{x^{C^{(j)}}+\sum_{iâˆˆ[n]}\tfrac{y^{C^{(j)}}_i-x^{C^{(j)}}_i}{T_r}\left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor\right)e_i}}).
  \end{align*}
  Now by assumption, we have
  \[T_r=\sqrt{n}\frac{y^C_i-x^C_i}{K}\frac{\norm{df}_{âˆ,2}}{2Î½}â‰¥\sqrt{n}(y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{\norm{d(g^{(j)})}_{âˆ,2}}{2Î½} \]
  for any \(iâˆˆ[n]\) and therefore
  \[ Î¾_iâ‰”\frac{1}{T_r}\left(\left\lfloor KT_ro_i(x)\right\rfloor-KT_ro_i(x)\right)â‰¤\frac{1}{\sqrt{n}(y^{C^{(j)}}_i-x^{C^{(j)}}_i)}\frac{2Î½}{\norm{d(g^{(j)})}_{âˆ,2}}. \]
  Hence, by definition of \(Î¾_i\) and \((y^{C^(j)}_i-x^{C^(j)}_i)K = (y^{C}_i-x^{C}_i)\) we have
  \begin{align*}
    \norm{x'-x}_2^2 &=\sum_{iâˆˆ[n]}\left(x'-x_i\right)^2 \\
    &=\sum_{iâˆˆ[n]}\left((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\left(\tfrac{1}{T_r}\left\lfloor KT_ro_i(x)\right\rfloor- Ko_i(x^{C^{(j)}})\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n]}\left((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\left(Î¾_i+K\frac{x_i-x^{C^{(j)}}_i}{y^C_i-x^C_i}\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n]}\left(\left(Î¾_i(y^{C^{(j)}}_i-x^{C^{(j)}}_i)+(x_i-x^{C^{(j)}}_i)\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n]}Î¾_i^2(y^{C^{(j)}}_i-x^{C^{(j)}}_i)^2 \\
    &â‰¤\frac{Î½^2}{\norm{d(g^{(j)})}_{âˆ,2}^2}.
  \end{align*}
  and thus conclude
  \[ \norm{g(x)-R(Î¦)_k(x)}_2=\norm{g(x)-g(x')}_2=\norm{d(g^{(j)})(x-x')}_2 â‰¤\norm{d(g^{(j)})}_{âˆ,2}\norm{x-x'}_2 â‰¤Î½. \]

  %CANCELLED: ensure usage of \left( \right)
  %DONE: give KT_r a shorter name
  %DONE: show how the inputs change in the different phases at the beginning of a phase paragraph

  %DONE: proof correct behaviour

  %CANCELLED: show construction that has binary state (flip-flop; )

  % DONE: why Kâ‰ 0
  % CANCELLED: check on â€off by one errorsâ€œ
\end{proof}

%CANCELLED (maybe): mollification

%CANCELLED: remark about SNN construction. By adding more neurons to first layer ((K-1)n to be exact) you can half the required time steps.
% CANCELLED: steelman comparison by arguing that L can be replaced by sth like 1/Î´(Îµ)

Unfortunately the size of the network in this construction is not always smaller than the one from~\cref{thm:approx-snn-constant}. A concrete counter example is a sinus wave with high frequency and small amplitude, like \(f(x)â‰”\frac{\sin(nx)}{n}\) with \(nâˆˆâ„•\) on \(C=U=[0,3Ï€)\) and \(Î©=[0,2Ï€]\). Since \(f'(x)=\cos(nx)\) and \(\norm{f'|_{(0,3Ï€)}}_{âˆ}=1\), \(Î“â‰”1\) is the optimal Lipschitz-constant for \(f\).
At the same time, since \(f''(x)=n\sin(nx)\) and \(\norm{f''|_{(0,3Ï€)}}_{âˆ}=n\), the biggest possible modulus of uniform continuity on \((0,3Ï€)\) for \(f'\) is \(Î´(Îµ)â‰”\frac{Îµ}{n}\). Thus
\[ \max_{\substack{Î¾,Î¸>0\\Î¾Î¸=Îµ}}\min(Î´(Î¾),Î¸)=\max_{\substack{Î¾>0}}\min(\frac{Î¾}{n},\frac{Îµ}{Î¾})=\sqrt{\frac{Îµ}{n}}. \]
Hence \(K(Îµ)â‰”\lfloor Ï€\sqrt{\frac{n}{Îµ}} \rfloor\). We therefore get the following network sizes:
\begin{alignat*}{2}
  & \cref{thm:approx-snn-constant} \qquad &\qquad  &\cref{thm:approx-snn} \\
  &n_1 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil+1 \qquad &\qquad &n_1 = 2 \\
  &n_2 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil \qquad &\qquad &n_2 = 2\lfloor Ï€\sqrt{\tfrac{n}{Îµ}} \rfloor+3
\end{alignat*}
While the first layer of~\cref{thm:approx-snn} is clearly arbitrarily smaller than the first layer of the other construction for \(Îµâ†’0\), the second layer of~\cref{thm:approx-snn} is arbitrarily worse than the second layer of~\cref{thm:approx-snn-constant} for \(nâ†’âˆ\).

On the other hand, even for large \(nâˆˆâ„•\), the second layer of~\cref{thm:approx-snn} is arbitrarily more efficient for \(Îµâ†’0\), since the size of the second layer of~\cref{thm:approx-snn} only grows proportionally to \(\tfrac{1}{\sqrt{Îµ}}\) and not \(\tfrac{1}{Îµ}\).
%MAYBE: image

%MAYBE: burden of Î“ is placed on T in our theorem

%DONE: introduce notation C^0, C^1
We generalize this observation to arbitrary functions with the following theorem.
\begin{theorem} %DONE: switch to U,Î©,C
  Let the function \(f\) and the sets \(U\), \(C\), \(Î©\) be given as in~\cref{thm:approx-snn}. Let further \(Î©\) be compact and assume that \(f\) and \(d(f|_{UÂ°})\) are \(Î“\),\(Î“'\)-Lipschitz respectively.

  We then have
  \[ \lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}=0, \]
  where \(n_2'(Îµ)\) is \(n_2(Îµ)\) as defined in~\cref{thm:approx-snn-constant} for \(Î©\), \(f|_{Î©}\) with Lipschitz-constant \(Î“'\) and \(Îµ\); and where \(n_2(Îµ)\) is defined as in~\cref{thm:approx-snn} for sets \(U\), \(C\), \(Î©\) and function \(f\) with modulus of uniform continuity \(Ï‰(x)â‰”Î“x\) of \(d(f|_{UÂ°})\) and approximation parameter \(Î¼â‰”\frac{Îµ}{2}\).
\end{theorem}

%DONE: (compare~\cref{rem:ch03-differential-lipschitz})
%CANCELLED: proof modulus of continuity stuff?
%DONE: fix statement, Ï‰
\begin{proof}
  First notice, that since
  \[n_2'(Îµ)=\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“' \right\rceil^n,1\right\}\]
  we have
  \[ \lim_{Îµâ†’0}n_2'(Îµ)Îµ=(\operatorname{diam}_âˆ(Î©)Î“')^n. \]
  We further have (c.f.~\cref{rem:ch03-differential-lipschitz})
  \begin{align*}
   n_2(Îµ)&= \max\left(1,\left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(\sqrt{\frac{Îµ}{2Î“}},\frac{Ï}{2})} \right\rceil\right)(n+1)+3
  \end{align*}
  such that we get
  \[ \lim_{Îµâ†’0}n_2(Îµ)\sqrt{Îµ}=\sqrt{\frac{nÎ“}{2}}\operatorname{diam}_âˆ(C)(n+1). \]
  We therefore have
  \[ \lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}\frac{1}{\sqrt{Îµ}}=\lim_{Îµâ†’0}\frac{n_2(Îµ)Îµ}{n_2'(Îµ)\sqrt{Îµ}}=\frac{\sqrt{\frac{nÎ“}{2}}\operatorname{diam}_âˆ(C)(n+1)}{\operatorname{diam}_âˆ(Î©)Î“'}âˆˆâ„ \]
  and hence conclude
  \[\lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}=0.\]
\end{proof}

\begin{figure}[!htbp]
  \centering
  \input{src/figures/ch03-alternative-construction.tex}
  \caption{Replacing the first hidden layer by non-recurrent structure}
  \label{fig:ch03-alternative-construction}
\end{figure}

To conclude this section, we shall give one last remark. While we used \rdtlifsnn for our construction in~\cref{thm:approx-snn}, we do think that it is not too hard to remove the interdependencies inside of the layers to obtain a \dtlifsnn by replicating layers as often as needed and connecting where necessary.
For example we can remove the interdependency of \(a_1\) on itself in the construction of~\cref{thm:approx-snn} by using \(u_0=-T_2\) and \(b=1\) to obtain the â€œalarm clockâ€ behavior.
Further its connections to the other neurons in the first layer can be removed by adding an intermediate layer with \(n\) neurons between the first and second hidden layer that just forward the input from their respective neuron in the first layer \footnote{see also~\cref{ex:ch02-forward-spikes}} and also take in an input from \(a_1\), compare~\cref{fig:ch03-alternative-construction}.
However, compared to our construction, this would significantly increase the network complexity.

%DONE: a1

%CANCELLED: betrachtung durch kombinatorik/wahrscheinlichkeit?

%DONE: there are also constructions for \dtlifsnn that use far less neurons, general structure likeâ€¦

% CANCELLED: encoding with fewer layers for on finite segments continuously differentiable functions


% DONE: wendeltreppe
% DONE: flush nach KTr (high-level erklÃ¤rung verhalten)
% DONE: vergleich d.t.lif-nn, r.constr, auch ohne d.t.lifsnn mÃ¶glich?
% DONE: g lin approx. graphic (von sinus)
