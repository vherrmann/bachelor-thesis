\section{Structure of computations in \rdtlifsnns}
\label{ch:struct}

When working with neural networks a fundamental question is how well they are able to approximate functions. Towards that end the following theorem was proved in~\cite{nguyen2025timespikeunderstandingrepresentational}.

\begin{theorem}\label{thm:approx-snn-constant}
  Let \(f\) be a continuous function on a compact set \(Î©âŠ‚â„^{n_0}\). For all \(Îµ>0\), there exists a d.t. LIF-SNN \(Î¦\) with direct encoding, membrane potential output, \(L=2\) and \(T=1\) such that
  \[ \norm{(R(Î¦)-f)|_{Î©}}_{âˆ}â‰¤Îµ\]
  Moreover, if \(f\) is \(Î“\)-Lipschitz, then \(Î¦\) can be chosen with width parameter \(n=(n_1,n_2)\) given by
  \begin{align*}
   n_1 &=\left(\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil,1\right\}+1\right)n_0, \\
   n_2 &=\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“ \right\rceil^{n_0},1\right\}.
  \end{align*}
\end{theorem}

\begin{figure}[h!]
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \input{src/figures/ch03-approx-id-dtlifsnn.tex}
    \caption{A \dtlifsnn approximating the identity}
    \label{fig:id-approx-by-dtlifsnn}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.55\textwidth}
    \centering
    \input{src/figures/ch03-approx-sinus-dtlifsnn.tex}
    \caption{\Dtlifsnns approximating a sinus wave}
    \label{fig:sin-approx-by-dtlifsnn}
  \end{subfigure}
\end{figure}

The proof of~\cref{thm:approx-snn-constant} first shows that a continuous function can be arbitrarily approximated by step functions, in particular by step functions constant on hypercubes in \(Î©\).
Then a \dtlifsnn is constructed by using the first layer to partition the input space along hyperplanes into cubes and the second layer to assign values to the hypercubes.

While quite simple, this construction does not use the unique feature of \dtlifsnns/\rdtlifsnns, the ability of neurons to accumulate state over time. It therefore needs quite a lot more neurons than actually needed for many functions with (almost) linear segments, like a sinus wave. E.g. in~\cref{fig:sin-approx-by-dtlifsnn} a neuron is needed for every constant region of the graphs in the first and second layer each.

We will now show a more efficient construction for \rdtlifsnn using the fact that \rdtlifsnn can quite efficiently approximate linear segments. The general intuition behind it is to use piece-wise linear functions to approximate continuously differentiable functions and then construct a \rdtlifsnn approximating the piece-wise linear function by discretizing the input dimensions into spike trains in the first layer that are consumed by groups of neurons in the second layer, one group for each almost linear segment.

To state our theorem we first need to define the notions of â€œmodulus of uniform continuityâ€ and â€œgeneralized inverse of a modulus of uniform continuityâ€ and proof some simple properties:

%TODO: quote def?
\begin{definition}
  Let \(M,N\) be metric spaces. A modulus of uniform continuity of a uniformly continuous function \(f:Mâ†’N\) is a function \(Ï‰:[0,âˆ]â†’[0,âˆ]\), such that it vanishes at \(0\), i.e. \(\lim_{xâ†’0}Ï‰(x)=0\), and
  \[ âˆ€_{x,yâˆˆM}d_N(f(x),f(y))â‰¤Ï‰(d_M(x,y)). \]
  The generalized inverse of \(Ï‰\) is defined as
  \[Ï‰^{â€ }(s)â‰”\inf\{tâˆˆ[0,âˆ]\mid Ï‰(t)>s\}.\]
\end{definition}

\begin{lemma}\label{lem:inverse-cont-mod}
  Let \(Ï‰:[0,âˆ]â†’[0,âˆ]\) be a modulus of uniform continuity of a uniformly continuous function \(f:Mâ†’N\), where \(M,N\) are metric spaces.

  We have the following properties
  \begin{enumerate}
  \item \(âˆ€_{x,yâˆˆM,sâˆˆ[0,âˆ]}d_M(x,y)â‰¤Ï‰^{â€ }(s)â‡’d_N(f(x),f(y))â‰¤s\).
  \item \(âˆ€_{sâˆˆ[0,âˆ]}s=0â‡”w^{â€ }(s)=0\).
  \end{enumerate}
\end{lemma}

\begin{proof}\phantom{}

  \begin{enumerate}
  \item Let \(x,yâˆˆM\) and \(sâˆˆ[0,âˆ]\) be given such that \(d_M(x,y)â‰¤Ï‰^{â€ }(s)\). By definition of \(Ï‰^{â€ }\), this means \(Ï‰(d_M(x,y))â‰¤s\). Since \(Ï‰\) is a modulus of uniform continuity of \(f\), we have \(d_N(f(x),f(y))â‰¤Ï‰(d_M(x,y))\) and therefore overall \(d_N(f(x),f(y))â‰¤s\).
  \item Since \(Ï‰\) is a modulus of uniform continuity, it is by definition continuous at \(0\). Let us choose an arbitrary sequence \((t_n)_{nâˆˆâ„•}\) with \(t_nâ†’0\). Then \(Ï‰(t_n)â†’0\) and therefore \(Ï‰^{â€ }(0)â‰¤\inf_{nâˆˆâ„•}t_n=0\).

  Is on the other hand \(Ï‰^{â€ }(s)=0\), then there is a sequence \((t_n)_{nâˆˆâ„•}\) with \(t_nâ†’0\) and therefore \(Ï‰(t_n)â†’0\). By definition of \(Ï‰^{â€ }(s)\), we have \(Ï‰(t_n)>s\), so we get \(s=0\).
  \end{enumerate}
\end{proof}


% CANCELLED: In the end, we will see that we can in fact approximate any continuous function like this, by using mollification to extract an continuously differentiable approximation of the function.

% DONE: motivation (previous theorem + previous lower bound example), insatisfactory since d.t. SNN. have somewhat linear structure

% CANCELLED: extend to continuously differentiable functions apart from lebesgue-zero sets (where they are still continuous?)
% DONE: differentiable, but defined on compact subset of euclidean space???
%
% DONE: use generalized inverse of the modulus of continuity

Let us now state our theorem:

% CANCELLED: demand differentiability only for Î©Â°
\begin{theorem}\label{thm:approx-snn}
  Let \(fâˆˆğ’^0(C,â„^m)\) be defined on a half-open cube \(Câ‰ âˆ…\), such that \(f\mid_{CÂ°}âˆˆğ’^1(CÂ°,â„^m)\) is continuously differentiable with bounded differential, i.e. \(\norm{d(f|_{CÂ°})}_{âˆ,2}<âˆ\). For all \(Îµ,Î¼,Î½>0\), \(Îµ=Î¼+Î½\), there exists a \rdtlifsnn \(Î¦\) with \(L=2\) and
  \begin{align*}
   T   &= (K(Î¼)+1)T_r(Î½)+2 \\
   n_1 &= n+1 \\
   n_2 &= K(Î¼)^n(n+1)+3
  \end{align*}
  such that
  \[ \norm{R(Î¦)|_C-f}_{âˆ,2}â‰¤Îµ.\]
  Where we use 
  \begin{align*}
   T_r&â‰”T_r(Î½)â‰”\max\left(2,\left\lceil \sqrt{n}\frac{\operatorname{diam}_{âˆ}(C)}{K}\frac{\norm{d(f|_{CÂ°})}_{âˆ,2}}{Î½} \right\rceil\right), \\
   K&â‰”K(Î¼)â‰” \min_{\substack{Î¾,Î¸>0\\Î¾Î¸=Î¼}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸)} \right\rceil\right\}.
  \end{align*} %TODO: if Ï‰^{â€ }=âˆ, then K=0?
  Here \(Ï‰^{â€ }\) is the generalized inverse of a modulus of uniform continuity with regard to \(\norm{Â·}_2\) of the total derivative \(d(f|_{CÂ°})\). \(d(f|_{CÂ°})\) is uniformly continuous since it is bounded.
  Since \(Î¾â‰ 0\), we have \(Ï‰^{â€ }(Î¾)>0\) by~\cref{lem:inverse-cont-mod}. Further there are \(Î¾,Î¸\) such that the minimum in the definition of \(K^n\) is obtained, since we the minimum is taken over the set of natural numbers, so \(K\) is well-defined. Moreover \(T_r\) is well-defined, since \(Câ‰ 0\) and therefore \(\operatorname{diam}_{âˆ}(C)â‰ 0\) and \(Kâ‰ 0\).
\end{theorem}

\begin{remark}
  In our construction \(Î¼\) and \(Î½\) determine whether to optimize the number of neurons or the number of time-steps. As we see later, \(K^n\) corresponds to the number of subcubes we will split \(C\) into such that \(f\) is almost linear on each of them. From the definition it is clear that \(K\) and therefore the number of neurons only depend on \(f\) through \(Ï‰^{â€ }(Î¾)\), which is essentially a measure of how strongly the slope of \(f\) is changing and therefore into how many subcubes we need to split \(C\) to get sufficiently almost affine linear regions of \(f\).

  We will further see that \(T_r\) corresponds to the number of constant intervals with which we approximate \(f\) on the almost affine linear regions. It is therefore to be expected that \(T_r\) depends on the width \(\frac{\operatorname{diam}_{âˆ}(C)}{K}\) of the subcubes and the maximal slope \(\norm{d(f|_{CÂ°})}_{âˆ,2}\).
\end{remark}
\begin{figure}[!ht]
  \centering
  \input{src/figures/ch03-spiral-staircase.tex}
  \caption{A spiral staircase function}
  \label{fig:ch03-spiral-staircase}
\end{figure}

\begin{remark}
  \cref{thm:approx-snn} cannot be generalized to arbitrary input spaces like compact sets: Consider a spiral staircase function like shown in~\cref{fig:ch03-spiral-staircase} defined on a compact set \(Î©â‰”\{(r\cos(Ï†),r\sin(Ï†))\mid 0.5â‰¤râ‰¤1,\ \abs{Ï†-Ï€}â‰¥0.2\}\). Such a function is clearly continuous and continuously differentiable on its interior \(Î©Â°\), such that in particular the differential is bounded, \(\norm{d(f|_{Î©Â°})}_{âˆ,2}<âˆ\).

  For our construction of the SNN we now need an encompassing half-open rectangle of \(Î©\), let e.g. \(C=[0,2)^2\). Now in~\cref{lem:approx-by-lin} we will partition \(C\) into sub-cubes, such that \(f\) is almost linear on those  w%TODO
  
  We need the half-open cube \(C\) with \(Î©âŠ‚CâŠ‚U\) to correctly choose \(T_r\) and \(K\): Suppose we regard the function \(f:((0,1) âˆª (2,3))â†’â„\), \(xâ†¦Ï‡_{[0,1]}(x)\). This function is certainly continuously differentiable, in particular, all derivatives are zero. In our construction of the \rdtlifsnn we use a cube in the input space as our canvas,
  %TODO
  Suppose, \(K\) would depend on the modulus of uniform continuity of \(df\) on \(U\) and \(T_r\) on \(\norm{df|_U}_{âˆ,2}\), without the a half-open cube \(C\) with \(Î©âŠ‚CâŠ‚U\)
\end{remark}

\begin{remark}
If \(fâˆˆğ’^1(U,â„^m)\) is given with \(âˆ…â‰ UâŠ‚â„^n\) and a compact subset \(Î©âŠ‚â„^n\), we can extend \(f|_{Î©}\) to a function \(dfâˆˆğ’^1(â„^n,â„^m)\): There is a partition of one \(Ï†_1,Ï†_2âˆˆğ’^{âˆ}(â„^n)\) subordinate to \(U\) and \(â„^nâˆ–Î©\). We get \(Ï†_1|_{Î©}=1\) and therefore \((Ï†_1f)|_{Î©}=f|_{Î©}\). \(f'â‰”Ï†_1f\) is further clearly \(ğ’^1(U,â„^m)\).
% \item We would like to only demand differentiability on \(Î©Â°\), â€¦ %TODO

%TODO â†“
  % We would like to only demand differentiability on \(Î©Â°\), but the theorem would not be true anymore. Imagine the function \(f(x)=Ï‡_{[0,1]}\) with \(Î©=[0,1]âˆª[2,3]\). Clearly, \(f|_{[0,1]âˆª[2,3]}\) is continuous and \(f|_{(0,1)âˆª(2,3)}âˆˆğ’^1(â„)\). But on the other hand a half-open cube around \(Î©\) at least needs to include \((1,2)\) and would need to have
\end{remark}


\begin{remark}%TODO
If \(fâˆˆğ’^1(U,â„^m)\) is given with \(âˆ…â‰ UâŠ‚â„^n\) and a compact \(Î©âŠ‚â„^n\), but with no half-open cube \(C\) such that \(Î©âŠ‚\overline{C}âŠ‚U\), we can extend \(f|_{Î©}\) to a function \(dfâˆˆğ’^1(â„^n,â„^m)\): There is a partition of one \(Ï†_1,Ï†_2âˆˆğ’^{âˆ}(â„^n)\) subordinate to \(U\) and \(â„^nâˆ–Î©\). We get \(Ï†_1|_{Î©}=1\) and therefore \((Ï†_1f)|_{Î©}=f|_{Î©}\). \(f'â‰”Ï†_1f\) is further clearly \(ğ’^1(U,â„^m)\).
% \item We would like to only demand differentiability on \(Î©Â°\), â€¦ %TODO

%TODO â†“
  % We would like to only demand differentiability on \(Î©Â°\), but the theorem would not be true anymore. Imagine the function \(f(x)=Ï‡_{[0,1]}\) with \(Î©=[0,1]âˆª[2,3]\). Clearly, \(f|_{[0,1]âˆª[2,3]}\) is continuous and \(f|_{(0,1)âˆª(2,3)}âˆˆğ’^1(â„)\). But on the other hand a half-open cube around \(Î©\) at least needs to include \((1,2)\) and would need to have
\end{remark}


%TODO: what about \dtlifsnn?

%TODO: remove?, minimal
\begin{comment}
%TODO
\begin{lemma}\label{lem:smallest-cube}
  Let \(Î©âŠ‚â„^n\) be compact. Then there exists a half-open cube \(C\) with width \(\operatorname{diam}_âˆ(Î©)\) such that \(Î©âŠ‚\overline{C}\).
\end{lemma}

%TODO: we later use âŸ¦y,zâ¦† instead of âŸ¦a,bâ¦†; unify notation
\begin{proof}[\proofofref{lem:smallest-cube}]
  We can first define \(xâ‰”(\inf_{xâˆˆÎ©} x_i)_{iâˆˆ[n]}âˆˆâ„^n\) since \(Î©\) is compact. We further define \(yâ‰”x+\operatorname{diam}_âˆ(Î©)Â·ğŸ™_n\). We now have \(Câ‰”âŸ¦x,yâ¦†\). Suppose now a point \(zâˆˆÎ©âˆ–\overline{C}\) exists. By definition of \(x\), we have \(xâ‰¤z\). By definition of \(C\) we further get \(z\nleq y\), and therefore \(y_i<z_i\) for an \(iâˆˆ[n]\) by definition of \(C\). But this means \(\norm{x-z}_âˆ>\operatorname{diam}_âˆ(Î©)\).
\end{proof}
\end{comment}

\begin{comment}
\begin{lemma}\label{lem:uniform-cont-â‰¤}
  Let \(M\) be a normed vector space and \(N\) be a metric space. Let \(f:Mâ†’N\) be uniformly continuous with modulus \(Î´\). We then have for every \(Îµ>0\):
  \[ âˆ€_{x,yâˆˆM}(\norm{x-y}_Mâ‰¤Î´(Îµ)â‡’d_N(x,y)â‰¤Îµ). \]
\end{lemma}

\begin{proof}
  Let \(Îµ>0\) with modulus \(Î´(Îµ)\), as well as \(xâˆˆM\) be given. Since \(f\) is uniformly continuous with modulus \(Î´(Îµ)\), we have \(f(B_{Î´}(x))âŠ‚B_{Îµ}(f(x))\). Since \(f\) is in particular continuous, we get
  \[ f(\overline{B}_{Î´}(x))=f(\overline{B_{Î´}(x)})âŠ‚\overline{f(B_{Î´}(x))}âŠ‚\overline{B}_{Îµ}(f(x)) \]
  We need to use the fact that \(M\) is a normed vector space in the first equality to get \(\overline{B}_{Î´}(x)=\overline{B_{Î´}(x)}\).
\end{proof}
\end{comment}

% DONE: differentiable, but defined on compact subset of euclidean space???
% TODO: quote formulation?
% DONE: instead of fixing \(Îµ^{t}\), get supremum of
% TODO: why Kâ‰ 0?
% CANCELLED: only demand differentiability on CÂ°

We first proof that continuous differentiable functions with uniform continuous differential can be efficiently approximated by piece-wise linear function. Compare e.g.~\cref{fig:sin-approx-sinus-linear} to~\cref{fig:sin-approx-by-dtlifsnn}

\begin{figure}[h!]
  \centering
  \begin{minipage}{0.55\textwidth}
    \centering
    \input{src/figures/ch03-approx-sinus-linear.tex}
    \caption{Piece-wise affine linear functions approximating the sinus}
  \end{minipage}
  \label{fig:sin-approx-sinus-linear}
\end{figure}


\begin{lemma}\label{lem:approx-by-lin}
  Let \(fâˆˆğ’^0(C,â„^m)\) be a continuous function defined on half-open cube \(C\), such that \(f\mid_{CÂ°}âˆˆğ’^1(CÂ°,â„^m)\) is continuously differentiable and \(d(f|_{CÂ°})\) is uniformly continuous. Let further \(K(Î¼)\) be defined as in~\cref{thm:approx-snn}.

  For every \(Î¼>0\) we can compose \(C\) into \(K^nâ‰”K(Î¼)^n\) half-open subcubes \((C^{(j)})_{jâˆˆ[K^n]}\) such that affine linear functions \(g^{(j)}:C^{(j)}â†’â„^m\) exist with  \(\norm{d(g^{(j)})}_{âˆ,2}â‰¤\norm{df}_{âˆ,2}\) and \(\norm{f-g}_{âˆ,2}<Î¼\), where \(gâ‰”\sum_{i=1}^mg^{(j)}Ï‡_{C^{(j)}}\).
\end{lemma}

% TODO: remark about why we are using 2-norm

% DONE: fix issue with C^(j) s not completely fitting in open \sqrt{2}Î´(Î¼) balls due to closed part of C^(j)
% CANCELLED: F might not be defined at c^(j), since \(Î©â‰ \overline{C}\)

\begin{proof}[\proofofref{lem:approx-by-lin}]
  Let \(Î¼>0\) be given. Let \(Ï‰\) be a modulus of uniform continuity of \(df\mid_{CÂ°}\).
  We will now partition \(C\) in \(K^n\) half-open subcubes with \(K\) defined as in~\cref{thm:approx-snn}.
  Let \(Î¾,Î˜\) be given, such that the minium is attained. Then the subcubes have width \(wâ‰”\frac{\operatorname{diam}_âˆ(C)}{K}â‰¤\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾),Î¸)\).

  Let us further define \(g^{(j)}:C^{(j)}â†’â„^m\) by \(g^{(j)}(x)â‰”f(c^{(j)})+df_{c^{(j)}}(x-c^{(j)})\) where \(c^{(j)}\) is the center of \(C^{(j)}\), so in particular for all  \(xâˆˆC^{(j)}\)
  \begin{equation}\label{eq:4}
  \norm{x-c^{(j)}}_2=\sqrt{\sum_{i=1}^n\abs{(x-c^{(j)})_i}^2}â‰¤\frac{w}{2}\sqrt{n}â‰¤\min(Ï‰^{â€ }(Î¾),Î¸).
  \end{equation}
  Now by definition of \(g^{(j)}\) we already have \(\norm{d(g^{(j)})}_{âˆ,2}=\norm{df_{c^{(j)}}}â‰¤\norm{df}_{âˆ,2}\).

  It suffices now to show \(\norm{f|_{C^{(j)}}-g^{(j)}}_âˆ<Î¼\). Let \(xâˆˆC^{(j)}\) and \(h(t)â‰”f(t(x-c^{(j)})+c^{(j)})\). We then have
  \[h'(t)=(df_{(x-c^{(j)})t+c^{(j)}}âˆ˜d(tâ†¦t(x-c^{(j)})+c^{(j)})_t)(1)=df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)}).\]
  We obtain by the fundamental theorem of calculus:
  \begin{align*}
    \norm{f(x)-g^{(j)}(x)}_2 &= \norm{f(x)-f(c^{(j)})-df_{c^{(j)}}(x-c^{(j)})}_2 \\
          &= \norm{h(1)-h(0)-df_{c^{(j)}}(x-c^{(j)})}_2 \\
          &= \norm{\int_0^1df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)})dt-df_{c^{(j)}}(x-c^{(j)})}_2
  \end{align*}
  Due to the generalized Minkowski-Inequality we can move the norm inside the integral:
  %MAYBE: ref for gen. Minkowski-Ineq?
  \begin{align*}
    &â‰¤ \int_0^1\norm{df_{(x-c^{(j)})t+c^{(j)}}(x-c^{(j)})-df_{c^{(j)}}(x-c^{(j)})}_2dt \\
    &= \int_0^1\norm{(df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}})(x-c^{(j)})}_2dt \\
    &â‰¤ \int_0^1\norm{df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}}}_2\norm{x-c^{(j)}}_2dt \\
    &â‰¤ \int_0^1Î¾\norm{x-c^{(j)}}_2dt \\
    &= Î¾\norm{x-c^{(j)}}_2 \\
    &â‰¤ Î¾Î¸ \\
    &= Î¼
  \end{align*}
  In the fourth step we use \(\norm{df_{(x-c^{(j)})t+c^{(j)}}-df_{c^{(j)}}}â‰¤Î¾\), which holds due to \(âˆ€_{tâˆˆ[0,1]}(x-c^{(j)})t+c^{(j)}âˆˆC^{(j)}\),~\eqref{eq:4} and~\cref{lem:inverse-cont-mod}.
  %CANCELLED: proof for cont. extension
\end{proof}
%DONE change notation of c_i to sth clearly not meant component

\begin{proof}[\proofofref{thm:approx-snn}.]
  Let there be \(Îµ,Î¼,Î½>0\) with \(Îµ=Î¼+Î½\).
  %DONE: clarify notation âŸ¦y,zâ¦† (line or proper subspace)
  By~\cref{lem:approx-by-lin} we have a composition \(K^n\) of \(C\) into half-open subcubes \((C^{(j)})_{i=1..K^n}\) and linear functions \(g^{(j)}:C^{(j)}â†’â„^m\), such that \(\norm{d(g^{(j)})}_{âˆ,2}â‰¤\norm{df}_{âˆ,2}\) and \(\norm{f-g}_âˆ<Î¼\) for \(gâ‰”\sum_{i=1}^mg^{(j)}Ï‡_{C^{(j)}}\). %CANCELLED: cont ext?

  We will now define a \rdtlifsnn \(Î¦\) with direct input encoding and membrane-potential outputs such that \(\norm{R(Î¦)|_C-g}_âˆ<Î½\).

  Let us first set the basic parameters \(i^{[l]}(0)=0\), \(Î±^{[l]}=0\) and \(Î²^{[l]}=Ï‘^{[l]}=1\) for all layers.

  \begin{figure}[h!]
    \centering
    \input{src/figures/ch03-whole-network}
    \caption{Structure of the whole network}
    \label{fig:ch03-whole-network}
  \end{figure}

  \begin{figure}[h!]
    \centering
    \input{src/figures/ch03-group-network}
    \caption{Structure of the network, focused on the \(j\)-th group of the second layer}
    \label{fig:ch03-group-network}
  \end{figure}

  The intuitive idea for the construction of the network is the following: We have five phases. We define
  \begin{gather*}
    T_1â‰”\rangeI{1}{KT_r},\qquad T_2â‰”\{KT_r\}, \qquad T_3â‰”\{KT_r+1\},\\
    T_4â‰”\{KT_r+2\},\qquad T_5â‰”\rangeI{KT_r+3}{T}.
  \end{gather*}
  For ease of notation, will also use \(T_2,T_3,T_4\) as if they were numbers.

  The first layer is only active during the first phase, \(T_1\). It is composed of \(n+1\) neurons, where the first \(n\) neuron convert the input vector regarding its position in \(C\) into spike trains. The last neuron, the â€œalarm clockâ€, shuts down the first layer after \(T_1\) ends.

  The second layer only accumulates state without spike during \(T_1âˆ–T_2\). Then during \(T_2âˆªT_3âˆªT_4\) it is decided in which region \(C^{(j)}\) the input \(x\) is located. Further during \(T_4âˆªT_5\) the location in \(C^{(j)}\) is encoded through spikes.

  For each region \(C^{(j)}\) we have \(n+1\)-neurons in the second layer. Each of the first \(n\) neurons encodes a commponent of the linear part of \(g^{(j)}\). They are also used to inform the \(n+1\)-th neuron of the group if the \(x\) has at least as big as the base point of \(C^{(j)}\). The \(n+1\)-th deactivates all other neurons of regions with smaller base point and encodes the constant part of \(g^{(j)}\). The last \(3\) neurons act as â€œclock neuronsâ€ enabling and disabling the other ones.

  % In the second layer we have \(m+1\)-neurons for each affine linear region \(C^{(j)}\). Each of the first \(m\) neurons encodes the linear part of a component of the output vector. The last, additional neuron acts as an activator to the specific region by recursively enabling the other neurons of this region and disabling all regions with smaller base point (that would otherwise be enabled).
  
  %TODO: properly label/name and ref graphics
  \input{src/figures/ch03-approx-first-layer-timeline}
  \begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
      \centering
      \begin{timeline}
        \SpikeOutAt{$(T2)$}
        \SpikeOutAt{$(T3)$}
        \SpikeOutAt{$(T4)$}
        \SpikeOutAt{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt{$(T)$}
      \end{timeline}
      
      \caption{$a_1$}
      \label{fig:timeline-a1}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[][$x_i$]{$(O)+(0.3,0)$}
        \SpikeOut[dashed]{$(O)+(0.6,0)$}{}
        \dotsBtSp{$(O)+(0.8,0)$}{$(T2)+(-0.8,0)$}
        % TODO:
        \SpikeIn[][$x_i$]{$(T2)+(-0.6,0)$}
        \SpikeOut[dashed]{$(T2)+(-0.3,0)$}

        \SpikeIn[][$x_i$]{$(T3)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T3)+(-1.3,0)$}

        \SpikeIn[][$x_i$]{$(T4)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T4)+(-1.3,0)$}

        \SpikeIn[][$x_i$]{$(T4+1)+(-1.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T4+1)+(-1.3,0)$}

        \dotsBtSp{$(T4+1)-(1.1,0)$}{$(T)+(-1,0)$}

        \SpikeIn[][$x_i$]{$(T)+(-0.8,0)$}
        \SpikeIn[][$s_{a_1}^{[1]}$]{$(T)+(-0.3,0)$}
      \end{timeline}
      
      \caption{$i$}
      \label{fig:timeline-i}
    \end{subfigure}
    \caption{Timelines of first layer neurons}
    \label{fig:timeline-first-layer}
  \end{figure}
  

  \begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T2-1)$}
      \end{timeline}
      
      \caption{$c_1$}
      \label{fig:timeline-c_1}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T2)$}
      \end{timeline}
      
      \caption{$c_2$}
      \label{fig:timeline-c2}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeOutAt{$(T4)$}
        \SpikeOutAt{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt{$(T)$}
      \end{timeline}
      \caption{$a_2$}
      \label{fig:timeline-a2}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[double][$\sum_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}$][]{$(T3)-(1.7,0)$}
        \SpikeOut[dashed][][$x^{C^{(j)}}{â‰¤}x$]{$(T3)-(0.3,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$][]{$(T4)-(1.5,0)$}
        \SpikeOut[dashed][][$x{âˆˆ}C^{(j)}$]{$(T4)-(0.2,0)$}

        \SpikeIn[][$s_{a_2}^{[1]}$]{$(T4+1)+(-1.7,0)$}

        \dotsBtSp{$(T4+1)-(1.5,0)$}{$(T)+(-0.5,0)$}

        \SpikeIn[][$s_{a_2}^{[1]}$]{$(T)+(-0.3,0)$}
      \end{timeline}
      \caption{$Ï‰_j$}
      \label{fig:timeline-ommega}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering

      \begin{timeline}
        \SpikeIn[][$x_i$]{$(O)+(0.3,0)$}
        \dotsBtSp[12]{$(O)+(0.5,0)$}{$(T2)+(-2,0)$}
        \SpikeIn[][$x_i$]{$(T2)+(-1.8,0)$}
        \SpikeIn[][$s^{[2]}_{c_1}$]{$(T2)+(-1.3,0)$}
        \SpikeIn[][$s^{[2]}_{c_2}$]{$(T3)+(-1.7,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$]{$(T4)+(-1.7,0)$}
        \SpikeIn[double][$r_s(q^{(j)})$]{$(T4+1)+(-1.7,0)$}

        \SpikeOut[dashed][][$x^{C^{(j)}}_i{â‰¤}x_i$]{$(T2)+(-0.2,0)$}

        \SpikeOutAt[dashed]{$(T4+1)$}
        \dotsBtSp{$(T4+1)-(0.1,0)$}{$(T)+(-0.5,0)$}
        \SpikeOutAt[dashed]{$(T)$}
        % \draw [decorate,decoration={brace,amplitude=6pt}] ($(T4+1)+(-0.4,1.6)$) -- ($(T)+(-0.2,1.6)$) node[midway,above=6pt]{\small If \(xâˆˆC_j\), };
      \end{timeline}
      \caption{$Î¹_j(i)$}
      \label{fig:timeline-iota_i}
    \end{subfigure}
    \caption{Timelines of second layer neurons}
    \label{fig:timeline-second-layer}
  \end{figure}

  To obtain the normalized location of a value in \(C\) we will often us
  \[ o_i(z)=\frac{z_i-x_i^C}{y_i^C-x_i^C} \]
  with \(iâˆˆ[n_1]\) in the following. 

  \begin{enumerate}
  \item \textbf{First layer}:
  We define the \textbf{\(i\)-th neuron} of the \(n\) neurons of the first layer by parameters
  \begin{equation}
    w=\frac{1}{y^C_i-x^C_i}e_i, \quad b=-\frac{x^C_i}{y^C_i-x^C_i}
    , \quad v=-e_{a_1},\quad u_0=0, \quad i_0=0.\tag{\(i\)}
  \end{equation}
  The \textbf{â€œalarm neuronâ€} of the first layer, with index \(a_1â‰”n+1\), is defined by:
  \begin{equation}
    w=0, \quad b=\frac{1}{T_2}, \quad v=e_{a_1}, \quad u_0=0, \quad i_0=0.\tag{\(a_1\)}
  \end{equation}

  % CANCELLED: check that proof works for xâˆˆ\(\overline{C}âˆ–C\)

  % We further define the \(n+1\)-th neuron of the first layer by \(w=0\), \(b=\frac{1}{K-1}\), \(v=\)
  % By~\cref{lem:non-recursive-defs} we get

  % DONE: proof correct behaviour

  %DONE: a_2 is used wrongly
  %DONE: add timeline
  \item \textbf{Second layer}:
  Let us now construct the second layer in the following way: For each of the \(K^n\) subcubes in \(C\) we define \(n+1\) neurons like so: Let \(C^{(j)}=âŸ¦x^{C^{(j)}},y^{C^{(j)}}â¦†\) be one such subcube with position \(q^{(j)}âˆˆ([K-1]_0)^{n_1}\) in \(C\), i.e.
  \[âˆ€_{iâˆˆ[n_1]}q^{(j)}_i=Ko_i(x^{C^{(j)}}).\]
  We will write \(Î¹_j(i)â‰”j(n+1)+i\) to index the first \(n\) neurons in the layer and \(Ï‰_jâ‰”(j+1)(n+1)\) to index the last neuron of each group.

  The \textbf{\(i\)-th neuron} of the first \(n\) neurons (of the \(j\)-th group), with index \(Î¹_j(i)\) in the second layer, has the parameters
  \begin{equation}
  \begin{gathered}
    w=e_i, \quad b=0, \quad v=T(e_{c_1}-2e_{c_2}+r(q^{(j)})), \\
    u_0=-q^{(j)}_iT_r-T+1, \quad i_0=0.
  \end{gathered}\tag{\(Î¹_j(i)\)}
  \end{equation}
  where â€œthe switchâ€ is
  \[ r(q)â‰”e_{Ï‰_{j(q)}}-\sum_{\substack{q'âˆˆ([K-1]_0)^{n_1} \\ q<q'}}e_{Ï‰_{j(q')}}. \]
  with the index \(j(q)\) of the subcube at position \(q\). We further define the applied variant
  \[ r_s(q;t)â‰”âŸ¨r(q),s^{[2]}(t)âŸ©=s^{[2]}_{Ï‰_{j(q)}}(t)-\sum_{\substack{q'âˆˆ([K-1]_0)^{n_1} \\ q<q'}}s^{[2]}_{Ï‰_{j(q')}}(t). \]
  The \textbf{final neuron} of the group, with index \(Ï‰_j\) in its layer, has the parameters
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=0, \quad v=\frac{1}{n}\sum_{i=1}^ne_{Î¹_j(i)}-2e_{a_2}+r(q^{(j)}), \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(Ï‰_j\)}
  \end{equation}
  We also define the two \textbf{â€œclock neuronsâ€}, with index \(c_1â‰”(n+1)K^n+1\) and \(c_2â‰”(n+1)K^n+2\) with parameters:
  %TODO: T_2=1?
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=b_{c_i}, \quad v=-(T-1)e_{c_i}, \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(c_1,c_2\)}
  \end{equation}
  where \(b_{c_1}=\frac{1}{T_2-1}\) and \(b_{c_2}=\frac{1}{T_2}\).
  We further define the \textbf{â€œalarm neuronâ€}, with index \(a_2â‰”(j+1)K^n(Î¼)+3\), by
  \begin{equation}
  \begin{gathered}
    w=0, \quad b=\frac{1}{T_4}, \quad v=e_{a_2}, \quad u_0=0, \quad i_0=0.
  \end{gathered}\tag{\(a_2\)}
  \end{equation}
  %DONE: use c_{-1}, c_0 and c_{1} and a (for alarm) for the first layer neuron.
  \item Output decoder:
    We further define the parameters of the output decoder by \(a_t=0\), for \(tâ‰¤T_3\) and otherwise \(a_t=1\). We further set \(b^{[L+1]}=0\) and
    \[ (W^{[L+1]})_{k,Î¹_j(i)}=d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{1}{T_r}e_i) \] %TODO: remove parantheses?
    for \(kâˆˆ[m]\), \(jâˆˆK^n\) and \(iâˆˆ[n]\). Here \(d(g^{(j)})\) is not only the total derivative, but also the linear part of \(g^{(j)}\), i.e. \(âˆ€_{xâˆˆC^{(j)}}g^{(j)}(x)=d(g^{(j)})(x-x^{C^{(j)}})+g(x^{C^{(j)}})\). We further set
    \[ (W^{[L+1]})_{k,Ï‰_j}=g_k(x^{C^{(j)}}) \]
    for \(kâˆˆ[m]\) and \(jâˆˆK^n\). We finally define \(W^{[L+1]}_{k,c_i}=0\) for \(iâˆˆ\{2..4\}\).
    %TODO: indices in correct order?
    %TODO: make easier to read
  \end{enumerate}

  We will now proof that this construction indeed approximates \(g\) well enough. It will be helpful to consider the following, by choice of \(i^{[l]},Î±^{[l]},Î²^{[l]},Ï‘^{[l]}\), simplified equations:
  \begin{align*}
    i^{[l]}(t) & = W^{[l]}s^{[l-1]}(t)+V^{[l]}s^{[l]}(t-1) \\
    p^{[l]}(t) & = u^{[l]}(t-1)+i^{[l]}(t)+b^{[l]} \\
    s^{[l]}(t) & = H(p^{[l]}(t)-\oneV{n_l}) \\
    u^{[l]}(t) & = p^{[l]}(t)-s^{[l]}(t)
  \end{align*}
  and in particular by~\cref{lem:non-recursive-defs}
  \begin{align*}
   p^{[l]}(t) &= u^{[l]}(0)+\sum_{k=1}^t\left(i^{[l]}(t)+b^{[l]}\right)-\sum_{k=1}^{t-1}s^{[l]}(k).
  \end{align*}

  Let now \(s^{[0]}(t)=xâˆˆC\). We will proof \(\norm{R(Î¦)(x)-g(x)}_{âˆ,2}â‰¤Î½\) in steps, by first characterizing the behavior of the first layer:

  \begin{enumerate}
  \item Characterization of the â€œalarm neuronâ€ \(a_1\):\\
  Let us first regard the neuron in the first layer: By choice of parameters we get:
  \begin{equation*}
   p^{[1]}_{a_1}(t) = \frac{t}{T_2}+\sum_{k=1}^ts^{[1]}_{a_1}(k-1)-\sum_{k=1}^{t-1}s^{[1]}_{a_1}(k) = \frac{t}{T_2}
  \end{equation*}
  So \(s^{[1]}_{a_1}(t)=1â‡”tâ‰¥T_2\).
  \item Characterization of \(i\)-th neuron, \(iâˆˆ[n]\):\\
  We have
  \begin{align*}
   i^{[1]}_i(t)+b^{[1]}_i=\frac{x_i-x^C_i}{y^C_i-x^C_i}-s^{[1]}_{a_1}(t-1)=o_i(x)-s^{[1]}_{a_1}(t-1).
  \end{align*}
  So in particular
  \begin{align*}
   0â‰¤i^{[1]}_i(t)+b^{[1]}_i=o_i(x)â‰¤1
  \end{align*}
  for \(tâˆˆT_1\), since \(x^C_iâ‰¤x_i<y_i^C\).
  Because further \(0â‰¤u^{[1]}_i(0)=0<1\) we can use~\cref{lem:sum-spikes-slowly-through-time} with \(t_0â‰”0\) and \(t_{Ï‰}â‰”T_2\) to obtain
  \begin{equation*}
  \left\lfloor T_2o_i(x) \right\rfloor =\left\lfloor u^{[1]}_i(0)+\sum_{t=1}^{T_2}(i^{[1]}_i(t)+b^{[1]}_i) \right\rfloor = \sum_{t=1}^{T_2}s^{[1]}_i(t).
  \end{equation*}
  
  We further have
  \begin{equation*}
   i^{[1]}_i(t)+b^{[1]}_i=o_i(x)-s^{[1]}_{a_1}(t-1)=o_i(x)-1<0
  \end{equation*}
  for \(t>T_2\), so we get
  \[p_i^{[1]}(t)=u_i^{[1]}(T_2)+\sum_{k=T_3}^t\left(i^{[1]}_i(k)+b^{[1]}_i\right)-\sum_{k=T_3}^{t-1}s^{[1]}(k) <1\]
  and therefore \(s_i^{[1]}(t)=0\) for \(t>T_2\).

  % So in particular
  % \begin{equation}\label{eq:3}
  % \left\lfloor T_2\frac{x_i-x^C_i}{y^C_i-x^C_i} \right\rfloor = \sum_{t=1}^{T_2}s^{[1]}_i(t) = \sum_{t=1}^Ts^{[1]}_i(t)
  % \end{equation}

  % The equation clearly also holds for \(x_i=y_i\).
  \end{enumerate}
  
  \noindent We will now continue with characterizing the second layer. We start with â€œclock neuronsâ€ and the â€œalarmâ€ neuron:
  
  \begin{enumerate}
  \item Characterization of the â€œclock neuronsâ€: \\
  In contrast to the alarm neuron of the first layer, the two â€œclockâ€ neurons only fire once:
  \begin{equation*}
   p^{[2]}_{c_i}(t) = tb_{c_i}-(T-1)\sum_{k=1}^ts^{[2]}_{c_i}(k-1)-\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k)=tb_{c_i}-T\sum_{k=1}^{t-1}s^{[2]}_{c_i}(k)
  \end{equation*}
  %DONE: proof t<3(T_2-1)
  Let us first consider \(c_1\): We clearly have \(p^{[2]}_{c_1}(t)<1\) for \(t<T_2-1\), but \(p^{[2]}_{c_1}(T_2-1)=1\).
  Since by definition \(Kâ‰¥1\) and \(T_râ‰¥2\), so \(T_2=KT_râ‰¥2\), we have \(tâ‰¤Tâ‰¤T(T_2-1)\). Therefore \(p^{[2]}_{c_1}(t)<1\) for \(t>T_2-1\) due to \(s^{[2]}_{c_1}(T_2-1)=1\). So \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_1}(t)=Ï‡_{\{T_2-1\}}(t)\).

  We similarly obtain \(âˆ€_{tâˆˆ[T]}s^{[2]}_{c_2}(t)=Ï‡_{T_2}(t)\).
  \item Characterization of the â€œalarm neuronâ€:\\
  Just like for \(a_1\), we also get \(s^{[1]}_{a_2}(t)=1â‡”tâ‰¥T_4\).
  \end{enumerate}

  \noindent We now proof the behavior of the remaining neurons in the second layer step by step throughout the phases.
  \begin{enumerate}
  \item Phase 1 \\
  We will show \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(t)=0\) and \(s^{[2]}_{Ï‰_j}(t)=0\) for all \(jâˆˆ[K^n]\) and \(tâˆˆ[T_2-1]_0\) by induction over \(t\). Let \(t=0\). We then get \(s^{[2]}_{Î¹_j(i)}(0)=s^{[2]}_{Ï‰_j}(0)=0\) by definition.
  Let further \(1â‰¤tâ‰¤T_2-1\). First notice that by induction hypothesis, we have \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(t')=0\) and \(r_s(q^{(j)};t')=0\) for \(t'<t\). It follows that
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(t')+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(t')+T(s^{[2]}_{c_1}(t'-1)-2s^{[2]}_{c_2}(t'-1)+r_s(q^{(j)};t'-1))=s^{[1]}_i(t'), \\
   i^{[2]}_{Ï‰_j}(t')+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(t'-1)-2s^{[2]}_{a_2}(t'-1)+r_s(q^{(j)};t'-1)=0.
  \end{align*}
  for all \(t'â‰¤t\). Thus, we further get
  \begin{align*}%TODO: t' anstatt k
   p^{[2]}_{Î¹_j(i)}(t) &= -q^{(j)}_iT_r-T+1+\sum_{k=1}^ts^{[1]}_i(k)-\sum_{k=1}^{t-1}s^{[2]}_{Î¹_j(i)}(k)= -q^{(j)}_iT_r-T+1+\sum_{k=1}^ts^{[1]}_i(k),\\
   p^{[2]}_{Ï‰_j}(t) &= \sum_{k=1}^t0-\sum_{k=1}^{t-1}s^{[2]}_{Ï‰_j}(k) = 0.
  \end{align*}
   Since \(\sum_{k=1}^ts^{[1]}_i(k)â‰¤T_2-1<T-1\) and \(s^{[2]}_{c_1}(t)=Ï‡_{\{T_2-1\}}(t)\), we in particular get \(p^{[2]}_{Î¹_j(i)}(t)â‰¤0\). So we have prooven \(âˆ€_{iâˆˆ[n]}s^{[2]}_{Î¹_j(i)}(t)=0\), \(s^{[2]}_{Ï‰_j}(t)=0\).
  \item Phase 2 \\
  Just as in phase 1, we have
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_2)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2-1)-2s^{[2]}_{a_2}(T_2-1)+r_s(q^{(j)};T_2-1)=0.
  \end{align*}
  So we also get \(p^{[2]}_{Ï‰_j}(T_2)=0\) and \(s^{[2]}_{Ï‰_j}(T_2)=0\). It is different for \(Î¹_j(i)\) due to the dependence on \(c_1\). Let \(jâˆˆ[K^n]\) and \(iâˆˆ[n]\) be given. The neuron with index \(Î¹_j(i)\) fires exactly then at \(T_2\), if \(x_iâ‰¥x^{C^{(j)}}_i\):
  First notice that
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_2)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_2)+T(s^{[2]}_{c_1}(T_2-1)-2s^{[2]}_{c_2}(T_2-1)+r_s(q^{(j)};T_2-1))=s^{[1]}_i(T_2)+T,
  \end{align*}
  thus, we conclude that
  \begin{align*}
   p^{[2]}_{Î¹_j(i)}(T_2) &= p^{[2]}_{Î¹_j(i)}(T_2-1) + i^{[2]}_{Î¹_j(i)}(T_2)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_2-1) \\
                         &= -q^{(j)}_iT_r+1+\sum_{k=1}^{T_2}s^{[1]}_i(k) \\
                         &= -q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor
  \end{align*}
  holds using the characterization of layer 1. Therefore we have \(s^{[2]}_{Î¹_j(i)}(T_2)=1\) exactly if \(q^{(j)}_iT_râ‰¤T_2o_i(x)\), which is equivalent to \(\frac{q^{(j)}_i}{K}(y^C_i-x^C_i)+x_i^Câ‰¤x_i\) by definition of \(o_i\). Further \(\frac{q^{(j)}_i}{K}(y^C_i-x^C_i)+x_i^C\) is equal to \(x^{C^{(j)}}\) by definition of \(q^{(j)}\). So \(s^{[2]}_{Î¹_j(i)}(T_2)=1\) holds exactly if \(x^{C^{(j)}}_iâ‰¤x_i\).
  \item Phase 3 \\
  The â€œActivator neuronâ€ \(Ï‰_j\) fires at \(T_3\) if and only if \(x^{C^{(j)}}â‰¤x\): First notice
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_3)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2)-2s^{[2]}_{a_2}(T_2)+r_s(q^{(j)};T_2)=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2).
  \end{align*}
  from which we derive
  \begin{align*}
   p^{[2]}_{Ï‰_j}(T_3) &= p^{[2]}_{Ï‰_j}(T_2) + i^{[2]}_{Ï‰_j}(T_3)+b^{[2]}_{Ï‰_j} - s^{[2]}_{Ï‰_j}(T_2) = \frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_2).
  \end{align*}
  So \(0â‰¤p^{[2]}_{Ï‰_j}(T_3)â‰¤1\) and we get \(p^{[2]}_{Ï‰_j}(T_3)=1\), as well as \(s^{[2]}_{Ï‰_j}(T_3)=1\) exactly if \(âˆ€_{iâˆˆ[n_1]}x^{C^{(j)}}_iâ‰¤x_i\), so if \(x^{C^{(j)}}â‰¤x\).

  Let further \(iâˆˆ[n_1]\). We get
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_3)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_3)+T(s^{[2]}_{c_1}(T_2)-2s^{[2]}_{c_2}(T_2)+r_s(q^{(j)};T_2))=-2T
  \end{align*}
  and therefore
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(T_3) &= p^{[2]}_{Î¹_j(i)}(T_2)+i^{[2]}_{Î¹_j(i)}(T_3)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_2) \\
                          &= -q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor-2T - s^{[2]}_{Î¹_j(i)}(T_2).
  \end{align*}
  So \(p^{[2]}_{Î¹_j(i)}(T_3)â‰¤-T\) and \(s^{[2]}_{Î¹_j(i)}(T_3)=0\), since \(\left\lfloor T_2o_i(x) \right\rfloorâ‰¤T_2â‰¤T-1\).
  \item Phase 4 \\
  Let \(iâˆˆ[n]\). The neuron \(Î¹_j(i)\) stays inactive at \(T_4\), since
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_4)+T(s^{[2]}_{c_1}(T_3)-2s^{[2]}_{c_2}(T_3)+r_s(q^{(j)};T_3))=Tr_s(q^{(j)};T_3)
  \end{align*}
  and hence
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(T_4) &= p^{[2]}_{Î¹_j(i)}(T_3)+i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_3) \\
                        &= p^{[2]}_{Î¹_j(i)}(T_3)+Tr_s(q^{(j)};T_3).
  \end{align*}
  Since \(p^{[2]}_{Î¹_j(i)}(T_3)â‰¤-T\), we conclude \(p^{[2]}_{Î¹_j(i)}(T_4)â‰¤0\) and \(s^{[2]}_{Î¹_j(i)}(T_4)=0\).

  Further the â€œactivator neuronâ€ \(Ï‰_j\) fires at \(T_4\) exactly if \(xâˆˆC^{(j)}\): First notice
  \begin{align*}
   i^{[2]}_{Ï‰_j}(T_4)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(T_3)-2s^{[2]}_{a_2}(T_3)+r_s(q^{(j)};T_3)=r_s(q^{(j)};T_3).
  \end{align*}
  So we get
  \begin{align*}
    p^{[2]}_{Ï‰_j}(T_4) &= p^{[2]}_{Ï‰_j}(T_3)+i^{[2]}_{Î¹_j(i)}(T_4)+b^{[2]}_{Î¹_j(i)} - s^{[2]}_{Î¹_j(i)}(T_3) \\
                       &= p^{[2]}_{Ï‰_j}(T_3)+r_s(q^{(j)};T_3) - s^{[2]}_{Î¹_j(i)}(T_3). \\
  \end{align*}
  By definition of \(q\), \(q^{(j)}_i<q^{(j')}_i\) holds exactly if \(x^{C^{(j)}}_i<x^{C^{(j')}}_i\) holds for all \(iâˆˆ[n_1]\) and \(j,j'âˆˆ[K^n]\), so \(âˆ€_{j,j'âˆˆ[K^n]}q^{(j)}<q^{(j')}â‡”x^{C^{(j)}}<x^{C^{(j')}}\).
  We further have shown \(âˆ€_{j'âˆˆK^n}s^{[2]}_{Ï‰_{j'}}(T_3)=1â‡”x^{C^{(j')}}â‰¤x\) before.
  \begin{align*}
  r_s(q^{(j)};T_3)&=s^{[2]}_{Ï‰_j}(T_3)-\sum_{\substack{q'âˆˆ([K-1]_0)^{n_1} \\ q^{(j)}<q'}}s^{[2]}_{Ï‰_{j(q')}}(T_3) \\
                  &=s^{[2]}_{Ï‰_j}(T_3)-\sum_{\substack{j'âˆˆ[K^n] \\ x^{C^{(j)}}<x^{C^{(j')}}â‰¤x}}1
  \end{align*}%TODO: n_1 vs n

  Now if \(xâˆˆC^{(j)}\), i.e. \(x^{C^{(j)}}â‰¤x<y^{C^{(j)}}\), then \(s^{[2]}_{Ï‰_j}(T_3)=1\) and if there was a \(s^{[2]}_{Ï‰_{j'}}(T_3)=1\) with \(x^{C^{(j)}}<x^{C^{(j')}}â‰¤x\), we would further get \(x^{C^{(j)}}<x^{C^{(j(q'))}}<y^{C^{(j)}}\) due to \(x<y^{C^{(j)}}\). But this contradicts the construction of the subcubes \((C^{(j)})_{jâˆˆ[K^n]}\).
  
  We have also shown \(p^{[2]}_{Ï‰_j}(T_3)=1\) and \(s^{[2]}_{Ï‰_j}(T_3)=1\) for this case, so we can conclude \(p^{[2]}_{Ï‰_j}(T_4)=1\) as well as \(s^{[2]}_{Ï‰_j}(T_4)=1\).

  Now suppose \(xâˆ‰C^{(j)}\). Since we assumed \(xâˆˆC\) and constructed the subcubes \((C^{(j)})_{jâˆˆ[K^n]}\) as a partition of \(C\), we have \(xâˆˆC^{(j')}\), \(jâ‰ j'\). Now if \(x^{C^{(j)}}<x^{C^{(j')}}\), then \(r_s(q^{(j)};T_3)â‰¤0\). We further have \(p^{[2]}_{Ï‰_j}(T_3)=1\) and \(s^{[2]}_{Ï‰_j}(T_3)=1\) in this case and therefore \(p^{[2]}_{Ï‰_j}(T_4)â‰¤0\) and \(s^{[2]}_{Ï‰_j}(T_4)=0\).

  Is on the other hand \(x^{C^{(j)}}â‰®x^{C^{(j')}}\), then there is a component \(iâˆˆ[n_1]\), such that \(x^{C^{(j')}}_i<x^{C^{(j)}}_i\) and therefore \(x_i<y^{C^{(j')}}_iâ‰¤x^{C^{(j)}}_i\). This implies \(x^{C^{(j)}}\nleq x\) and we also get \(r_s(q^{(j)};T_3)â‰¤0\). We further have have \(p^{[2]}_{Ï‰_j}(T_3)<1\) and \(s^{[2]}_{Ï‰_j}(T_3)=0\) in this case and therefore \(p^{[2]}_{Ï‰_j}(T_4)<1\) and \(s^{[2]}_{Ï‰_j}(T_4)=0\).

  To summarize, we \(s^{[2]}_{Ï‰_j}(T_4)=1\) exactly if \(xâˆˆC^{(j)}\) just as we claimed, as well as \(p^{[2]}_{Ï‰_j}(T_4)â‰¤1\) in general.
  %DONE: more rigorous?
  \item Phase 5 \\
  The â€œactivator neuronâ€ \(Ï‰_j\) is inactive during \(T_5\), since for \(t>T_4\)
  \begin{align*}
   i^{[2]}_{Ï‰_j}(t)+b^{[2]}_{Ï‰_j}&=\frac{1}{n}\sum_{i=1}^ns^{[2]}_{Î¹_j(i)}(t-1)-2s^{[2]}_{a_2}(t-1)+r_s(q^{(j)};t-1)â‰¤0.
  \end{align*}
  and
  \begin{align*}
     p^{[2]}_{Ï‰_j}(t) &= u^{[2]}_{Ï‰_j}(T_4)+\sum_{k=T_4+1}^{t}\left(i^{[2]}_{Ï‰_j}(k)+b^{[2]}_{Ï‰_j}\right)-\sum_{k=T_4+1}^{t-1}s^{[2]}_{Ï‰_j}(k) \\
                      &â‰¤ u^{[2]}_{Ï‰_j}(T_4). \\
  \end{align*}
  Further \(u^{[2]}_{Ï‰_j}(T_4)<1\), since \(p^{[2]}_{Ï‰_j}(T_4)â‰¤1<2\). So \(âˆ€_{t>T_4}s^{[2]}_{Ï‰_j}(t)=0\). In particular the â€œswitchâ€ \(r_s(q^{(j)};t)=0\) is off for all \(jâˆˆ[K^n]\) and \(t>T_4\).
  
  Let \(iâˆˆ[n]\). During \(T_5\) the neuron \(Î¹_j(i)\) captures the position of \(x\) in \(C^{(j)}\) regarding the \(i\)-th dimension if \(xâˆˆC^{(j)}\) and stays inactive otherwise.

  Let us first assume \(xâˆ‰C^{(j)}\). We have previously shown \(s^{[2]}_{Ï‰_j}(T_4)=0\) and just now \(âˆ€_{t>T_4}s^{[2]}_{Ï‰_j}(t)=0\). So \(âˆ€_{tâ‰¥T_4}r_s(q^{(j)};t)=0\) and therefore for all \(t>T_4\)
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(t)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(t)+T(s^{[2]}_{c_1}(t-1)-2s^{[2]}_{c_2}(t-1)+r_s(q^{(j)};t-1))â‰¤0
  \end{align*}
  as well as
  \begin{align*}
    p^{[2]}_{Î¹_j(i)}(t) &= p^{[2]}_{Î¹_j(i)}(T_4)+\sum_{k=T_4+1}^t\left(i^{[2]}_{Î¹_j(i)}(k)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(k-1)\right) \\
                        &= p^{[2]}_{Î¹_j(i)}(T_4)+\sum_{k=T_4+1}^t\left(i^{[2]}_{Î¹_j(i)}(k)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(k-1)\right) \\
                      &â‰¤p^{[2]}_{Î¹_j(i)}(T_4) \\
                      &â‰¤0. \\
  \end{align*}
  So in particular \(âˆ€_{t>T_4}s^{[2]}_{Î¹_j(i)}(t)=0\).

  Suppose now \(xâˆˆC^{(j)}\).


  Let \(j\) be given with \(xâˆˆC^{(j)}\). As we have seen before, we have \(r_s(q^{(j)};T_4)=1 \) and \(âˆ€_{t>T_4}r_s(q^{(j)};t)=0\).
  We therefore get
  \begin{align*}
   i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}&=s^{[1]}_i(T_4+1)+T(s^{[2]}_{c_1}(T_4)-2s^{[2]}_{c_2}(T_4)+r_s(q^{(j)};T_4)) = T
  \end{align*}
  and for all \(t>T_4+1\)
  \begin{align*}
    i_{Î¹_j(i)}^{[2]}(t)+b^{[2]}_{Î¹_j(i)}=&s^{[1]}_i(t)+T(s^{[2]}_{c_1}(t-1)-2s^{[2]}_{c_2}(t-1)+r_s(q^{(j)};t-1))=0.
  \end{align*}
    
  Using previous results and in particular \(s^{[2]}_{Î¹_j(i)}(T_2)=1â‡”x^{C^{(j)}}_iâ‰¤x_i\), we obtain
  \begin{align*}
    p_{Î¹_j(i)}^{[2]}(T_4+1)&=p_{i_j(i)}^{[2]}(T_4)+i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}-s^{[2]}_{Î¹_j(i)}(T_4) \\
                           &=p^{[2]}_{Î¹_j(i)}(T_3)+Tr_s(q^{(j)};T_3)+T \\
                           &=-q^{(j)}_iT_r+1+\left\lfloor T_2o_i(x) \right\rfloor-2T - s^{[2]}_{Î¹_j(i)}(T_2)+2T \\
                           &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor.
  \end{align*}
  We now have \(KT_ro_i(x^{C^{(j)}})â‰¤KT_ro_i(x) \), since \(x^{C^{(j)}}_iâ‰¤x_i\), so \(p_{Î¹_j(i)}^{[2]}(T_4+1)â‰¥0\).

  We further have 
  \[ o_i(y^{C^{(j)}})-o_i(x^{C^{(j)}})=\frac{y_i^{C^{(j)}}-x_i^{C^{(j)}}}{y_i^C-x_i^C}=\frac{1}{K}. \]
  and \(o_i(x)<o_i(y^{C^{(j)}})=o_i(x^{C^{(j)}})+\frac{1}{K}\) for all \(xâˆˆC^{(j)}\). We can deduce
  \begin{align*}
    p_{Î¹_j(i)}^{[2]}(T_4+1) &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor \\
    &â‰¤-KT_ro_i(x^{C^{(j)}})+KT_ro_i(y^{C^{(j)}}) \\
    &â‰¤-KT_ro_i(x^{C^{(j)}})+T_r+KT_ro_i(x^{C^{(j)}}) \\
    &â‰¤T_r.
  \end{align*}
  So we have shown
  \[ 0â‰¤p_{Î¹_j(i)}^{[2]}(T_4+1)=u_{i_j(i)}^{[2]}(T_4)+i^{[2]}_{Î¹_j(i)}(T_4+1)+b^{[2]}_{Î¹_j(i)}â‰¤T_r=T-(T_4+1). \]
  By~\autoref{lem:pos-input-pos-res-pos} we now get \(u^{[2]}_j(T_4+1)â‰¥0\). We can therefore use~\autoref{lem:sum-spikes-decaying} with \(t_0â‰”T_4+1\), \(t_mâ‰”T_4+1\) and \(t_Ï‰â‰”T\), such that we obtain
  \begin{align*}
    \sum_{t=T_4+1}^Ts^{[2]}_{Î¹_{j(i)}}(t)&=\left\lfloor u^{[2]}_{Î¹_{j(i)}}(T_4)+i^{[2]}_{Î¹_{j(i)}}(T_4+1)+b^{[2]}_{Î¹_{j(i)}} \right\rfloor \\
                                         &=-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor.
  \end{align*}

  \end{enumerate}

  \noindent We have now reached the final step, where we will show that the spikes actually approximate \(g\). Let us consolidate our results. For \(j\) with \(xâˆˆC^{(j)}\) we have
  \begin{align*}
    &\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t)=\sum_{t=T_4+1}^Ts^{[2]}_{Î¹_{j(i)}}(t)= -KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor, \\
    &\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)=s^{[2]}_{Ï‰_j}(T_4)=1.
  \end{align*}
  And therefore for all \(kâˆˆ[m]\) %TODO: split into parts
  \begin{align*}
   W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t) &= g_k(x^{C^{(j)}}), \\
   W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t) &= \left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x)\right\rfloor\right)\left(d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{1}{T_r}e_i)\right).
  \end{align*}
          %CANCELLED: write from which steps
          %TODO: make dependence on _i explicit
          %DONE: introduce notation _k for functions
  Let now \(j\) be such that \(xâˆ‰C^{(j)}\), we have shown
  \begin{align*}
    &\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t)=0, \\
    &\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)=0.
  \end{align*}
  So this group of neurons does not contribute to the output of the network:
  \begin{align*}
   W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t) &= 0, \\
   W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t) &= 0.
  \end{align*}
  Finally note that by choice of \(W^{[L+1]}\), the neurons \(c_1,c_2,a_2\) don't contribute to the output as well. For any \(jâˆˆ\{c_1,c_2,a_2\}\)
  \[ W^{[L+1]}_{k,j}\sum_{t=T_4}^Ts^{[2]}_j(t)=0. \]
          %TODO: introduce W_k,j?
          %CANCELLED: g_k wiederspricht hier vorheriger notation g_j
          %DONE: notation e_i
  Let now \(jâˆˆ[K^n]\) be such that \(xâˆˆC_j\) again. We can consequently have
  \[ R(Î¦)_k(x)=\sum_{t=T_4}^T(W^{[L+1]}s^{[2]}(t))_k= W^{[L+1]}_{k,Ï‰_j}\sum_{t=T_4}^Ts^{[2]}_{Ï‰_j}(t)+W^{[L+1]}_{k,Î¹_{j(i)}}\sum_{t=T_4}^Ts^{[2]}_{Î¹_{j(i)}}(t). \]
  Which is further equal to
  \begin{align*}
    & g_k(x^{C^{(j)}})+\sum_{iâˆˆ[n_1]}\left(d(g^{(j)})_k((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{1}{T_r}e_i)\right)\left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor\right) \\
    &= g_k(\underset{â‰•x'}{\underbrace{x^{C^{(j)}}+\sum_{iâˆˆ[n_1]}\tfrac{y^{C^{(j)}}_i-x^{C^{(j)}}_i}{T_r}\left(-KT_ro_i(x^{C^{(j)}})+\left\lfloor KT_ro_i(x) \right\rfloor\right)e_i}})
  \end{align*}
  Now by assumption, we have
  \[T_r=\sqrt{n_1}\frac{y^C_i-x^C_i}{K}\frac{\norm{df}_{âˆ,2}}{2Î½}â‰¥\sqrt{n_1}(y^{C^{(j)}}_i-x^{C^{(j)}}_i)\frac{\norm{d(g^{(j)})}_{âˆ,2}}{2Î½} \]
  for any \(iâˆˆ[n_1]\) and hence
  \[ Î¾_iâ‰”\frac{1}{T_r}\left(\left\lfloor KT_ro_i(x)\right\rfloor-KT_ro_i(x)\right)â‰¤\frac{1}{\sqrt{n_1}(y^{C^{(j)}}_i-x^{C^{(j)}}_i)}\frac{2Î½}{\norm{d(g^{(j)})}_{âˆ,2}}. \]
  We now get the following inequality by definition of \(Î¾_i\) and \((y^{C^(j)}_i-x^{C^(j)}_i)K = (y^{C}_i-x^{C}_i)\)
  \begin{align*}
    \norm{x'-x}_2^2 &=\sum_{iâˆˆ[n_1]}\left(x'-x_i\right)^2 \\
    &=\sum_{iâˆˆ[n_1]}\left((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\left(\tfrac{1}{T_r}\left\lfloor KT_ro_i(x)\right\rfloor- Ko_i(x^{C^{(j)}})\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n_1]}\left((y^{C^{(j)}}_i-x^{C^{(j)}}_i)\left(Î¾_i+K\frac{x_i-x^{C^{(j)}}_i}{y^C_i-x^C_i}\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n_1]}\left(\left(Î¾_i(y^{C^{(j)}}_i-x^{C^{(j)}}_i)+(x_i-x^{C^{(j)}}_i)\right)-(x_i-x^{C^{(j)}}_i)\right)^2 \\
    &=\sum_{iâˆˆ[n_1]}Î¾_i^2(y^{C^{(j)}}_i-x^{C^{(j)}}_i)^2 \\
    &â‰¤\frac{Î½^2}{\norm{d(g^{(j)})}_{âˆ,2}^2}.
  \end{align*}
  So we can conclude
  \[ \norm{g(x)-R(Î¦)_k(x)}_2=\norm{g(x)-g(x')}_2=\norm{d(g^{(j)})(x-x')}_2 â‰¤\norm{d(g^{(j)})}_{âˆ,2}\norm{x-x'}_2 â‰¤Î½ \]

  %CANCELLED: ensure usage of \left( \right)
  %DONE: give KT_r a shorter name
  %DONE: show how the inputs change in the different phases at the beginning of a phase paragraph

  %DONE: proof correct behaviour

  %CANCELLED: show construction that has binary state (flip-flop; )

  % DONE: why Kâ‰ 0
  % CANCELLED: check on â€off by one errorsâ€œ
\end{proof}

%CANCELLED (maybe): mollification

%CANCELLED: remark about SNN construction. By adding more neurons to first layer ((K-1)n to be exact) you can half the required time steps.
% CANCELLED: steelman comparison by arguing that L can be replaced by sth like 1/Î´(Îµ)

Sadly the size of the network in this construction is not always smaller than the one from~\cref{thm:approx-snn-constant}. A concrete counter example is a sinus wave with high frequency and small ampitude, like \(f(x)â‰”\frac{\sin(nx)}{n}\) with \(nâˆˆâ„•\) on \(C=[0,2Ï€)\), \(Î©=[0,2Ï€]\). Since \(f'(x)=\cos(nx)\) and \(\norm{f'|_{(0,2Ï€)}}_{âˆ}=1\), \(Î“â‰”1\) is the optimal Lipschitz-constant for \(f\).
At the same time, since \(f''(x)=n\sin(nx)\) and \(\norm{f''|_{(0,2Ï€)}}_{âˆ}=n\), the biggest possible modulus of uniform continuity on \(CÂ°\) we can give for \(f'\) is \(Î´(Îµ)â‰”\frac{Îµ}{n}\). So we get
\[ \max_{\substack{Î¾,Î¸>0\\Î¾Î¸=Îµ}}\min(Î´(Î¾),Î¸)=\max_{\substack{Î¾>0}}\min(\frac{Î¾}{n},\frac{Îµ}{Î¾})=\sqrt{Îµn} \]
So we get \(K(Îµ)â‰”\lfloor \frac{Ï€}{\sqrt{nÎµ}} \rfloor\). We therefore get for the layer sizes:
\begin{alignat*}{2}
   & \cref{thm:approx-snn-constant} \qquad &\qquad  &\cref{thm:approx-snn} \\
  &n_1 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil+1 \qquad &\qquad &n_1 = 2 \\
  &n_2 =\left\lceil \tfrac{2Ï€}{Îµ} \right\rceil \qquad &\qquad &n_2 = 2\lfloor \tfrac{Ï€}{\sqrt{nÎµ}} \rfloor+3
\end{alignat*}
While the first and second layer of~\cref{thm:approx-snn} are clearly arbitrarily smaller than the first layer of the other construction for small \(Îµ\), the second layer of~\cref{thm:approx-snn} is arbitrarily bad for \(nâ†’âˆ\) compared to the second layer of~\cref{thm:approx-snn-constant}.

On the other hand, even for large \(nâˆˆâ„•\), the second layer of~\cref{thm:approx-snn} is arbitrarily more efficient for \(Îµâ†’0\), since the size of the second layer of~\cref{thm:approx-snn} only grows proportionally to \(\tfrac{1}{\sqrt{Îµ}}\) and not \(\tfrac{1}{Îµ}\).
%TODO: image

%TODO: burdon of Î“ is placed on T in our theorem

%DONE: introduce notation C^0, C^1
We generalize this observation with the following theorem.
\begin{theorem} %DONE: switch to C
  Let \(Câ‰ âˆ…\) be a cube in \(â„^n\), \(Î©âŠ‚\overline{C}\) a compact subset with \(\operatorname{diam}_{âˆ}(Î©)â‰ 0\) and \(fâˆˆğ’^0(\overline{C},â„)\) a continuous function that is \(Î“'\)-Lipschitz, such that \(f|_{CÂ°}âˆˆğ’^1(CÂ°,â„^m)\) is continuously differentiable and \(d(f|_{CÂ°})\) is \(Î“\)-Lipschitz.

  We get
  \[ \lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}=0. \]
  Where \(n_2'(Îµ)\) is \(n_2(Îµ)\) as defined in~\cref{thm:approx-snn-constant} for \(Î©\), \(f|_{Î©}\) with Lipschitz-constant \(Î“'\) and \(Îµ\); and where \(n_2(Îµ)\) is as defined in~\cref{thm:approx-snn} for \(C\), \(f|_C\) with modulus of continuity \(Ï‰(x)â‰”Î“x\) of \(d(f|_{CÂ°})\) and \(Î¼â‰”\frac{Îµ}{2}\).
\end{theorem}

%CANCELLED: proof modulus of continuity stuff?
%DONE: fix statement, Ï‰
\begin{proof}
  First notice, that since
  \[n_2'(Îµ)=\max\left\{\left\lceil \frac{\operatorname{diam}_âˆ(Î©)}{Îµ}Î“' \right\rceil^{n_0},1\right\}\]
  we get
  \[ \lim_{Îµâ†’0}n_2'(Îµ)Îµ=\operatorname{diam}_âˆ(Î©)Î“'. \]
  We further have
  \begin{align*}
   n_2(Îµ)&=\min_{\substack{Î¾,Î¸>0\\Î¾Î¸=\frac{Îµ}{2}}}\left\{\left\lceil \frac{\operatorname{diam}_âˆ(C)}{\frac{2}{\sqrt{n}}\min(Ï‰^{â€ }(Î¾) ,Î¸)} \right\rceil\right\}(n+1)+3.
  \end{align*}
  Let now \((Î¾_{Îµ})_{Îµ>0}\) and \((Î¸_{Îµ})_{Îµ>0}\) be given such that \(âˆ€_{Îµ>0}Î¾_{Îµ}Î¸_{Îµ}=\frac{Îµ}{2}\) and the minimum in \(n_2(Îµ)\) is obtained, i.e.
  \[\min(Ï‰^{â€ }(Î¾_{Îµ}) ,Î¸_{Îµ})=\max_{\substack{Î¾,Î¸>0\\Î¾Î¸=\frac{Îµ}{2}}}\min(Ï‰^{â€ }(Î¾) ,Î¸).\]
  Note moreover that \(Ï‰^{â€ }(s)=\inf\{tâˆˆ[0,âˆ]\mid Î“t>s\}=\frac{s}{Î“}\) for \(Î“â‰ 0\) and \(Ï‰^{â€ }(s)=âˆ\) otherwise, since \(Ï‰(x)=Î“x\).
  We now get
  \begin{align*}
   \min(Ï‰^{â€ }(Î¾_{Îµ}) ,Î¸_{Îµ}) = \min\left(\frac{Î¾_{Îµ}}{Î“} ,\frac{Îµ}{Î¾_{Îµ}2}\right) = \sqrt{\frac{Î“Îµ}{2}}
  \end{align*}
  and therefore
  \begin{align*}%TODO: put this into remark after theorem? cor?
   n_2(Îµ)&=\left\lceil \sqrt{\frac{n}{2Î“Îµ}}\operatorname{diam}_âˆ(C) \right\rceil(n+1)+3
  \end{align*}
  for \(Î“â‰ 0\). Is \(Î“=0\), then \(\min(Ï‰^{â€ }(Î¾_{Îµ}),Î¸_{Îµ})=âˆ\) (TODO)
  With this simplified definition, we get
  \[ \lim_{Îµâ†’0}n_2(Îµ)\sqrt{Îµ}=\sqrt{\frac{n}{2Î“}}\operatorname{diam}_âˆ(C)(n+1). \]
  We can conclude
  \[ \lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}\frac{1}{\sqrt{Îµ}}=\lim_{Îµâ†’0}\frac{n_2(Îµ)Îµ}{n_2'(Îµ)\sqrt{Îµ}}=\frac{\sqrt{\frac{n}{2Î“}}\operatorname{diam}_âˆ(C)(n+1)}{\operatorname{diam}_âˆ(Î©)Î“'}âˆˆâ„ \]
  So the proof is finished, since we can deduce
  \[\lim_{Îµâ†’0}\frac{n_2(Îµ)}{n_2'(Îµ)}=0.\]
\end{proof}

%CANCELLED: betrachtung durch kombinatorik/wahrscheinlichkeit?

%TODO: there are also constructions for \dtlifsnn that use far less neurons, general structure likeâ€¦

% CANCELLED: encoding with fewer layers for on finite segments continuously differentiable functions


% DONE: wendeltreppe
% TODO: flush nach KTr (high-level erklÃ¤rung verhalten)
% TODO: vergleich d.t.lif-nn, r.constr, auch ohne d.t.lifsnn mÃ¶glich?
% DONE: g lin approx. graphic (von sinus)
